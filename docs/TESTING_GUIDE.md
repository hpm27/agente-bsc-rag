# Guia de Testes End-to-End - Agente BSC RAG

## üìã Vis√£o Geral

Este guia detalha como executar e interpretar os **testes End-to-End (E2E)** do sistema Agente BSC RAG. Os testes validam o sistema completo funcionando de ponta a ponta, desde a ingest√£o de dados at√© a gera√ß√£o de respostas.

**√öltima atualiza√ß√£o**: 14/10/2025

---

## üéØ Objetivos dos Testes E2E

Os testes E2E validam:

1. **Fluxo Completo**: Ingest√£o ‚Üí Query ‚Üí Retrieval ‚Üí Agentes ‚Üí Synthesis ‚Üí Judge ‚Üí Resposta
2. **Performance**: Lat√™ncia (P50/P95/P99), cache de embeddings, paraleliza√ß√£o
3. **Qualidade**: Precis√£o de respostas, aprova√ß√£o do Judge, scores de relev√¢ncia
4. **Otimiza√ß√µes**: Cache 949x speedup, busca multil√≠ngue +106% precis√£o, paraleliza√ß√£o 3.34x
5. **Prontid√£o**: Qdrant UP, dataset indexado, API keys configuradas

---

## üì¶ Estrutura de Testes

### Arquivos

```
tests/integration/
‚îú‚îÄ‚îÄ test_e2e.py           # Suite principal de testes (550+ linhas)
‚îî‚îÄ‚îÄ test_queries.json     # Dataset de queries de teste (20 queries)
```

### Classes de Teste

| Classe | Testes | Foco |
|--------|--------|------|
| `TestE2EWorkflow` | 7 | Fluxo b√°sico do workflow |
| `TestQueryScenarios` | 4 | Queries espec√≠ficas por perspectiva |
| `TestPerformanceOptimizations` | 4 | Cache, paraleliza√ß√£o, multil√≠ngue |
| `TestJudgeValidation` | 2 | Valida√ß√£o do Judge Agent |
| `TestMetrics` | 2 | Lat√™ncia P50/P95/P99, approval rate |
| `TestSystemReadiness` | 3 | Prerequisites (Qdrant, dataset, API keys) |
| **TOTAL** | **22 testes** | **Cobertura completa** |

---

## üöÄ Pr√©-requisitos

### 1. Sistema em Execu√ß√£o

Antes de executar os testes, certifique-se de que:

#### a) Docker Compose est√° rodando

```powershell
# Iniciar containers (Qdrant, Weaviate, Redis)
docker-compose up -d

# Verificar status
docker-compose ps
```

**Esperado**:
```
NAME                    STATUS
qdrant                  Up
weaviate                Up
redis                   Up
```

#### b) Dataset BSC est√° indexado

```powershell
# Indexar documentos BSC (se ainda n√£o foi feito)
python scripts/build_knowledge_base.py
```

**Esperado**:
- Processamento de 5 livros BSC
- 7.965 chunks contextualizados indexados no Qdrant
- Tempo: ~10-15 minutos (primeira vez) ou ~10 segundos (com cache)

#### c) Vari√°veis de ambiente configuradas

Verificar arquivo `.env` na raiz do projeto:

```ini
# API Keys (OBRIGAT√ìRIAS)
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
COHERE_API_KEY=...

# LLM Model
DEFAULT_LLM_MODEL=claude-sonnet-4-5-20250929

# Embedding Cache (RECOMENDADO)
EMBEDDING_CACHE_ENABLED=true
EMBEDDING_CACHE_DIR=.cache/embeddings

# Qdrant
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=bsc_rag
```

### 2. Ambiente Python

```powershell
# Ativar ambiente virtual
.\venv\Scripts\Activate.ps1

# Instalar depend√™ncias (se necess√°rio)
pip install -r requirements.txt
```

---

## ‚ñ∂Ô∏è Executando Testes

### Executar Todos os Testes

```powershell
# Op√ß√£o 1: Usando pytest diretamente
pytest tests/integration/test_e2e.py -v

# Op√ß√£o 2: Com output detalhado
pytest tests/integration/test_e2e.py -v -s

# Op√ß√£o 3: Com relat√≥rio HTML
pytest tests/integration/test_e2e.py -v --html=report.html --self-contained-html
```

### Executar Testes Espec√≠ficos

#### Por Classe

```powershell
# Apenas testes de performance
pytest tests/integration/test_e2e.py::TestPerformanceOptimizations -v

# Apenas testes do Judge
pytest tests/integration/test_e2e.py::TestJudgeValidation -v

# Apenas testes de m√©tricas
pytest tests/integration/test_e2e.py::TestMetrics -v

# Apenas testes de prontid√£o (r√°pidos)
pytest tests/integration/test_e2e.py::TestSystemReadiness -v
```

#### Por Teste Individual

```powershell
# Teste de cache de embeddings
pytest tests/integration/test_e2e.py::TestPerformanceOptimizations::test_embedding_cache_speedup -v

# Teste de lat√™ncia P50/P95/P99
pytest tests/integration/test_e2e.py::TestMetrics::test_latency_percentiles -v

# Teste de busca multil√≠ngue
pytest tests/integration/test_e2e.py::TestPerformanceOptimizations::test_multilingual_search_pt_br_query -v
```

#### Por Marker (se configurado)

```powershell
# Apenas testes r√°pidos
pytest tests/integration/test_e2e.py -m "not slow" -v

# Apenas testes lentos
pytest tests/integration/test_e2e.py -m "slow" -v
```

---

## üìä Interpretando Resultados

### Output Esperado

```
========================= test session starts =========================
collected 22 items

tests/integration/test_e2e.py::TestE2EWorkflow::test_simple_factual_query PASSED     [ 4%]
tests/integration/test_e2e.py::TestE2EWorkflow::test_conceptual_query PASSED        [ 9%]
tests/integration/test_e2e.py::TestE2EWorkflow::test_comparative_query PASSED       [13%]
tests/integration/test_e2e.py::TestE2EWorkflow::test_complex_query PASSED           [18%]
tests/integration/test_e2e.py::TestE2EWorkflow::test_workflow_latency PASSED        [22%]
tests/integration/test_e2e.py::TestE2EWorkflow::test_refinement_process PASSED      [27%]
tests/integration/test_e2e.py::TestE2EWorkflow::test_multiple_perspectives PASSED   [31%]
...
tests/integration/test_e2e.py::TestSystemReadiness::test_qdrant_connection PASSED   [95%]
tests/integration/test_e2e.py::TestSystemReadiness::test_dataset_indexed PASSED     [100%]

========================= 22 passed in 180.50s =========================
```

### M√©tricas Importantes

#### 1. Lat√™ncia (P50/P95/P99)

```
[TEST METRICS LATENCY] Mean: 12.5s, P50: 11.2s, P95: 18.7s, P99: 22.3s
```

**Targets MVP**:
- P50 < 20s ‚úÖ
- P95 < 30s ‚úÖ
- P99 < 40s (aceit√°vel)

#### 2. Cache de Embeddings

```
[TEST CACHE SPEEDUP] Sem cache: 3.721s, Com cache: 0.004s, Speedup: 930.3x
```

**Targets**:
- Speedup >= 10x (esperado: 100-1000x) ‚úÖ
- Cache hit rate >= 80% ‚úÖ

#### 3. Judge Approval Rate

```
[TEST METRICS JUDGE] Approval Rate: 83.3%, Avg Score: 0.82
```

**Targets**:
- Approval rate >= 70% ‚úÖ
- Avg score >= 0.7 ‚úÖ

#### 4. Busca Multil√≠ngue

```
[TEST MULTILINGUAL] Query PT-BR recuperou 10 docs, 8 com score >0.7
```

**Target**:
- Pelo menos 50% dos docs com score >0.7 ‚úÖ

#### 5. Paraleliza√ß√£o de Agentes

```
[TEST PARALLEL] 4 perspectivas executadas em 14.2s
```

**Target**:
- <20s para 3+ perspectivas (vs ~30-40s sequencial) ‚úÖ

---

## ‚ö†Ô∏è Troubleshooting

### Problema 1: Qdrant n√£o est√° rodando

**Erro**:
```
E   pytest.fail(f"Qdrant n√£o est√° rodando ou inacess√≠vel: {e}")
```

**Solu√ß√£o**:
```powershell
# Verificar status do Docker
docker-compose ps

# Reiniciar containers
docker-compose restart

# Se necess√°rio, rebuild
docker-compose down
docker-compose up -d
```

---

### Problema 2: Dataset n√£o indexado

**Erro**:
```
E   AssertionError: Dataset muito pequeno: 0 chunks (esperado >=1000)
```

**Solu√ß√£o**:
```powershell
# Indexar dataset BSC
python scripts/build_knowledge_base.py

# Verificar indexa√ß√£o
python -c "
from qdrant_client import QdrantClient
client = QdrantClient(host='localhost', port=6333)
info = client.get_collection('bsc_rag')
print(f'Chunks indexados: {info.points_count}')
"
```

**Esperado**: `Chunks indexados: 7965` (ou similar)

---

### Problema 3: API Keys inv√°lidas

**Erro**:
```
E   AssertionError: OPENAI_API_KEY n√£o configurada no .env
```

**Solu√ß√£o**:
1. Abrir `.env` na raiz do projeto
2. Configurar API keys v√°lidas:

```ini
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-api03-...
COHERE_API_KEY=...
```

3. Verificar configura√ß√£o:

```powershell
python scripts/valida_env.py
```

---

### Problema 4: Testes lentos ou timeout

**Sintoma**: Testes demoram >5 minutos ou falham com timeout

**Causas poss√≠veis**:
1. Rate limiting da API (muitas chamadas simult√¢neas)
2. Cache de embeddings desativado
3. Lat√™ncia de rede alta

**Solu√ß√µes**:

#### a) Ativar cache de embeddings

```ini
# .env
EMBEDDING_CACHE_ENABLED=true
EMBEDDING_CACHE_DIR=.cache/embeddings
```

#### b) Executar testes em grupos menores

```powershell
# Primeiro, testes r√°pidos (prerequisites)
pytest tests/integration/test_e2e.py::TestSystemReadiness -v

# Depois, testes b√°sicos
pytest tests/integration/test_e2e.py::TestE2EWorkflow -v

# Por √∫ltimo, testes de m√©tricas (lentos)
pytest tests/integration/test_e2e.py::TestMetrics -v
```

#### c) Ajustar timeout do pytest

```powershell
pytest tests/integration/test_e2e.py -v --timeout=300
```

---

### Problema 5: Testes falham com erros de import

**Erro**:
```
ModuleNotFoundError: No module named 'src'
```

**Solu√ß√£o**:
```powershell
# Garantir que est√° no diret√≥rio raiz
cd D:\Users\OneDrive - engelar.eng.br\Documentos\Hugo\ENGELAR\agente-bsc-rag

# Ativar ambiente virtual
.\venv\Scripts\Activate.ps1

# Executar testes
pytest tests/integration/test_e2e.py -v
```

---

### Problema 6: Judge est√° rejeitando muitas respostas

**Sintoma**: Approval rate <70%

**Diagn√≥stico**:
```powershell
# Executar teste espec√≠fico com output detalhado
pytest tests/integration/test_e2e.py::TestMetrics::test_judge_approval_rate -v -s
```

**An√°lise**:
- Verificar logs para feedback do Judge
- Identificar padr√µes de rejei√ß√£o (completude, fundamenta√ß√£o, fontes)

**Poss√≠veis causas**:
1. Prompts dos agentes precisam de refinamento
2. Retrieval n√£o est√° recuperando docs suficientes
3. Threshold do Judge muito r√≠gido

**Solu√ß√£o**:
- Revisar prompts em `src/prompts/specialist_prompts.py`
- Ajustar par√¢metros de retrieval (top_k, score_threshold)
- Revisar crit√©rios do Judge em `src/prompts/judge_prompt.py`

---

## üìà M√©tricas de Sucesso MVP

### Targets Alcan√ßados (14/10/2025)

| M√©trica | Target MVP | Atual | Status |
|---------|------------|-------|--------|
| **Lat√™ncia P50** | <20s | ~12s | ‚úÖ |
| **Lat√™ncia P95** | <30s | ~19s | ‚úÖ |
| **Cache Speedup** | >10x | 949x | ‚úÖ |
| **Cache Hit Rate** | >80% | 87.5% | ‚úÖ |
| **Judge Approval** | >70% | ~83% | ‚úÖ |
| **Judge Avg Score** | >0.7 | ~0.82 | ‚úÖ |
| **Multilingual Precision** | >50% docs score >0.7 | 80% | ‚úÖ |
| **Paraleliza√ß√£o** | <20s para 3+ persp. | ~14s | ‚úÖ |

**Conclus√£o**: Sistema MVP **100% dentro dos targets** üéâ

---

## üîß Customizando Testes

### Adicionar Nova Query de Teste

Editar `tests/integration/test_queries.json`:

```json
{
  "factual_queries": [
    {
      "id": "f005",
      "query": "Sua nova query aqui",
      "expected_perspectives": ["financial"],
      "complexity": "simple"
    }
  ]
}
```

### Criar Novo Teste

Editar `tests/integration/test_e2e.py`:

```python
@pytest.mark.asyncio
async def test_meu_caso_especifico(self, workflow):
    """Descri√ß√£o do teste."""
    query = "Minha query de teste"
    
    result = await workflow.run(query, session_id="test-custom")
    
    # Suas assertions aqui
    assert result is not None
    assert "response" in result
```

### Ajustar Thresholds

Se m√©tricas estiverem muito r√≠gidas/flex√≠veis, ajustar em `test_e2e.py`:

```python
# Lat√™ncia
assert p95 < 30  # Ajustar para 40 se necess√°rio

# Judge approval rate
assert approval_rate >= 70  # Ajustar para 60 se necess√°rio

# Cache speedup
assert speedup >= 10  # Ajustar para 5 se infra for mais lenta
```

---

## üö¶ CI/CD Integration

### GitHub Actions (Exemplo)

```yaml
name: E2E Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      qdrant:
        image: qdrant/qdrant:latest
        ports:
          - 6333:6333
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Index dataset
        run: |
          python scripts/build_knowledge_base.py
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
      
      - name: Run E2E tests
        run: |
          pytest tests/integration/test_e2e.py -v --junitxml=report.xml
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: pytest-results
          path: report.xml
```

---

## üìö Refer√™ncias

- **Pytest Documentation**: https://docs.pytest.org/
- **Pytest Asyncio**: https://github.com/pytest-dev/pytest-asyncio
- **Testing Best Practices**: https://docs.python-guide.org/writing/tests/
- **LangGraph Testing**: https://python.langchain.com/docs/langgraph/how-tos/test
- **RAG Evaluation**: https://docs.ragas.io/

---

## üìù Changelog

### 14/10/2025 - v1.0 (Inicial)
- Suite completa de 22 testes E2E
- Cobertura: workflow, performance, Judge, m√©tricas, prerequisites
- Testes de otimiza√ß√µes: cache 949x, multil√≠ngue +106%, paraleliza√ß√£o 3.34x
- Documenta√ß√£o completa com troubleshooting

---

**√öltima atualiza√ß√£o**: 14/10/2025  
**Vers√£o**: 1.0  
**Autor**: Agente BSC RAG Team

