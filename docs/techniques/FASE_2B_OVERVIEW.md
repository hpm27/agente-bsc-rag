# Fase 2B - T√©cnicas RAG Avan√ßadas: Self-RAG vs CRAG

**Data:** 2025-10-14  
**Status:** üìã Planejamento Completo  
**Decis√£o:** Aguardando Benchmark Fase 2A

---

## üìã VIS√ÉO GERAL

Fase 2B introduz duas t√©cnicas RAG avan√ßadas complementares:

1. **Self-RAG** - Self-reflection para reduzir alucina√ß√µes
2. **CRAG** - Corrective retrieval para melhorar qualidade

---

## üîç SELF-RAG - Self-Reflective RAG

### O Que √â?

Sistema RAG que **auto-avalia** suas pr√≥prias decis√µes e outputs usando LLM como "cr√≠tico interno".

### Como Funciona?

```
Query ‚Üí [Precisa retrieval?] 
        ‚îú‚îÄ SIM ‚Üí Retrieve ‚Üí [Docs relevantes?]
        ‚îÇ                   ‚îú‚îÄ SIM ‚Üí Keep
        ‚îÇ                   ‚îî‚îÄ N√ÉO ‚Üí Discard
        ‚îÇ        ‚Üí Generate ‚Üí [Suportado por docs?]
        ‚îÇ                     ‚îú‚îÄ SIM ‚Üí Return
        ‚îÇ                     ‚îî‚îÄ N√ÉO ‚Üí Re-generate
        ‚îî‚îÄ N√ÉO ‚Üí Generate direto
```

### Componentes

1. **Retrieval Decision** - LLM decide SE query precisa de retrieval
2. **Document Grading** - Filtra docs irrelevantes
3. **Answer Grading** - Verifica suporte factual
4. **Hallucination Check** - Detecta informa√ß√£o inventada

### Reflection Tokens

Tokens especiais que guiam o comportamento:

- `[Retrieve]` / `[No Retrieve]`
- `[Relevant]` / `[Irrelevant]`
- `[Supported]` / `[Partially]` / `[Not Supported]`
- `[Useful]` / `[Not Useful]`

### Quando Usar?

‚úÖ **BOM para:**
- Alta necessidade de acur√°cia factual
- Hallucination rate > 10%
- Queries complexas com m√∫ltiplas partes
- Dom√≠nios onde inventar informa√ß√£o √© cr√≠tico (m√©dico, legal, financeiro)

‚ùå **RUIM para:**
- Lat√™ncia cr√≠tica (<10s requirement)
- Faithfulness j√° > 0.90
- Or√ßamento muito limitado
- Queries simples diretas

### ROI Esperado

| M√©trica | Melhoria | Trade-off |
|---------|----------|-----------|
| Hallucination Rate | -40 a -50% | - |
| Faithfulness | +10 a +15% | - |
| Judge Approval | +8 a +12% | - |
| Lat√™ncia | - | +20 a +30% |
| Custo | - | +30 a +40% |

**Complexidade:** ‚≠ê‚≠ê‚≠ê‚≠ê M√©dia-Alta  
**Tempo Implementa√ß√£o:** 3-4 dias (13-16h)

---

## üîß CRAG - Corrective RAG

### O Que √â?

Sistema RAG que **auto-corrige** retrieval ruim via query reformulation e web search fallback.

### Como Funciona?

```
Query ‚Üí Retrieve ‚Üí [Qualidade?]
                   ‚îú‚îÄ CORRECT (>0.7) ‚Üí Usar docs internos
                   ‚îú‚îÄ AMBIGUOUS (0.3-0.7) ‚Üí Docs + Web search
                   ‚îî‚îÄ INCORRECT (<0.3) ‚Üí Rewrite query ‚Üí Web search
                   ‚Üí Generate
```

### Componentes

1. **Retrieval Grading** - Avalia qualidade (score 0-1)
2. **Query Rewriting** - Reformula queries amb√≠guas
3. **Knowledge Refinement** - Decomp√µe docs em strips √∫teis
4. **Web Search Fallback** - Busca externa quando falha
5. **Fusion** - Combina fontes m√∫ltiplas (RRF)

### Grading Thresholds

- **Correct** (confidence > 0.7): Docs altamente relevantes ‚Üí Usar
- **Ambiguous** (0.3 < conf ‚â§ 0.7): Incerto ‚Üí Combinar docs + web
- **Incorrect** (conf ‚â§ 0.3): Docs irrelevantes ‚Üí Web search only

### Quando Usar?

‚úÖ **BOM para:**
- Retrieval frequentemente falha (<70% precision)
- Queries amb√≠guas ou mal formuladas
- Dataset incompleto para dom√≠nio
- Necessidade de informa√ß√£o externa (web)

‚ùå **RUIM para:**
- Retrieval j√° excelente (>80% precision)
- Dataset completo e bem curado
- Lat√™ncia muito cr√≠tica
- Sem acesso a web search (air-gapped)

### ROI Esperado

| M√©trica | Melhoria | Trade-off |
|---------|----------|-----------|
| Context Precision | +15 a +25% | - |
| Retrieval Quality | 0.65 ‚Üí 0.80 | - |
| Accuracy (queries corrigidas) | +15% | - |
| Lat√™ncia | - | +30 a +40% |
| Custo | - | +40 a +50% |

**Complexidade:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Alta  
**Tempo Implementa√ß√£o:** 4-5 dias (18-21h)

---

## üÜö COMPARA√á√ÉO DIRETA

### Self-RAG vs CRAG

| Aspecto | Self-RAG | CRAG |
|---------|----------|------|
| **Objetivo** | Reduzir alucina√ß√µes | Melhorar retrieval ruim |
| **Approach** | Self-reflection em generation | Corre√ß√£o de retrieval |
| **Quando Aplicar** | Gera√ß√£o √© problema | Retrieval √© problema |
| **Complexidade** | M√©dia-Alta | Alta |
| **Lat√™ncia Adicional** | +20-30% | +30-40% |
| **Custo Adicional** | +30-40% | +40-50% |
| **ROI** | Alto (SE alucina√ß√µes >10%) | Alto (SE precision <70%) |
| **Web Search** | N√£o precisa | **Precisa** (Tavily/Bing) |
| **LLM Calls** | +2-3 calls/query | +3-4 calls/query |

### Quando Usar Ambas?

‚úÖ **Combina√ß√£o Ideal:**
- Self-RAG: Post-processing de generation
- CRAG: Pre-processing de retrieval

**Workflow Combinado:**
```
Query ‚Üí CRAG (corrige retrieval) 
      ‚Üí Agents (com docs corrigidos)
      ‚Üí Self-RAG (valida resposta)
      ‚Üí Final Answer
```

**Trade-off:** Lat√™ncia +50-70%, Custo +70-90%, mas qualidade m√°xima.

---

## üìä MATRIZ DE DECIS√ÉO

Use esta matriz para decidir qual t√©cnica implementar:

| Seu Problema | T√©cnica Recomendada | Prioridade |
|--------------|---------------------|------------|
| Hallucination rate > 15% | Self-RAG | üî• ALTA |
| Context Precision < 0.60 | CRAG | üî• ALTA |
| Ambos problemas | Self-RAG + CRAG | üî• ALTA |
| Hallucination 10-15% | Self-RAG | M√âDIA |
| Precision 0.60-0.70 | CRAG | M√âDIA |
| Hallucination < 10% | ‚ùå Pular Self-RAG | BAIXA |
| Precision > 0.80 | ‚ùå Pular CRAG | BAIXA |
| Ambas m√©tricas boas | ‚ùå Pular Fase 2B | BAIXA |

---

## üéì LI√á√ïES DE PLANEJAMENTO

### 1. Brightdata Research

**Descoberta:** Implementa√ß√µes modernas 2025 (LangGraph, Meilisearch, DataCamp) simplificam muito vs paper original.

**Economia:** ~40% tempo de desenvolvimento (usa c√≥digo validado)

### 2. Decis√£o Data-Driven

**Princ√≠pio:** N√ÉO implementar antes de validar necessidade com benchmark.

**Benef√≠cio:** Evita over-engineering, foco em ROI real.

### 3. Modularidade

**Design:** Self-RAG e CRAG como m√≥dulos independentes com feature flags.

**Benef√≠cio:** A/B testing f√°cil, rollback r√°pido, customiza√ß√£o por cliente.

---

## üìö REFER√äNCIAS

### Self-RAG

**Papers:**
- [Self-RAG: Learning to Retrieve, Generate, and Critique](https://arxiv.org/abs/2310.11511) (Oct 2023)

**Tutorials:**
- [LangGraph Self-RAG Tutorial](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/) (Oficial)
- [Analytics Vidhya - Self-RAG 2025](https://www.analyticsvidhya.com/blog/2025/01/self-rag/)
- [Medium - Reflective Agentic RAG](https://nayakpplaban.medium.com/build-a-reflective-agentic-rag-workflow-using-langgraph)

**GitHub:**
- [AkariAsai/self-rag](https://github.com/AkariAsai/self-rag)

### CRAG

**Papers:**
- [Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884) (Jan 2024)

**Tutorials:**
- [Meilisearch - CRAG Workflow](https://www.meilisearch.com/blog/corrective-rag) (Sep 2025)
- [DataCamp - CRAG Implementation](https://www.datacamp.com/tutorial/corrective-rag-crag) (Sep 2024)
- [LangGraph CRAG Tutorial](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/)

**GitHub:**
- [HuskyInSalt/CRAG](https://github.com/HuskyInSalt/CRAG)

---

## üîú PR√ìXIMO PASSO

**AGUARDAR:** Benchmark Fase 2A completar  
**ANALISAR:** Faithfulness, Context Precision, Judge Approval  
**DECIDIR:** Implementar Fase 2B baseado em m√©tricas objetivas

---

**√öltima Atualiza√ß√£o:** 2025-10-14  
**Status:** ‚úÖ PLANEJAMENTO COMPLETO

