# Judge Agent Context-Aware

**Data:** Novembro 2025  
**Vers√£o:** 1.0  
**Status:** ‚úÖ Implementado

---

## üìã Vis√£o Geral

O Judge Agent foi modificado para avaliar respostas de forma **context-aware**, ajustando crit√©rios de avalia√ß√£o baseado no tipo de conte√∫do sendo avaliado (RAG, DIAGN√ìSTICO, FERRAMENTAS).

**Problema Resolvido:**
- Na fase DIAGN√ìSTICO, recomenda√ß√µes s√£o baseadas APENAS no perfil do cliente (sem retrieval)
- Judge penalizava diagn√≥sticos por falta de fontes (`has_sources=False`)
- Fontes de literatura BSC s√≥ vir√£o em fases posteriores (Ferramentas Consultivas com retrieval)

**Solu√ß√£o Implementada:**
- Par√¢metro `evaluation_context` adicionado ao Judge.evaluate()
- Prompt context-aware que ajusta crit√©rios baseado no contexto
- Compatibilidade retroativa mantida (default 'RAG')

---

## üéØ Contextos de Avalia√ß√£o

### 1. **RAG** (Retrieval-Augmented Generation)

**Quando usar:** Respostas geradas com retrieval em documentos BSC indexados

**Crit√©rios aplicados:**
- ‚úÖ **Cita√ß√£o de fontes ESPERADA** (Fonte, P√°gina)
- ‚úÖ **Fundamenta√ß√£o em docs ESPERADA** (grounded)
- ‚úÖ **Detec√ß√£o de alucina√ß√µes ATIVA**
- ‚úÖ **Penaliza√ß√£o por falta de fontes** (se quality_score < 0.85)

**Exemplo de uso:**
```python
from src.agents.judge_agent import JudgeAgent

judge = JudgeAgent()

judgment = judge.evaluate(
    original_query="O que √© BSC?",
    agent_response="BSC √© um sistema... (Fonte: Kaplan & Norton, 1996)",
    retrieved_documents="[Documentos BSC recuperados...]",
    agent_name="RAG Agent",
    evaluation_context="RAG"  # ‚Üê Fontes ESPERADAS
)
```

**Crit√©rio de aprova√ß√£o:**
```
approved: quality_score >= 0.7 E (is_grounded=True OU quality_score >= 0.85)
needs_improvement: 0.5 <= quality_score < 0.7 OU (0.7 <= quality_score < 0.85 E is_grounded=False)
rejected: quality_score < 0.5 OU (is_grounded=False E has_sources=False E quality_score < 0.7)
```

---

### 2. **DIAGNOSTIC** (Diagn√≥stico sem Retrieval)

**Quando usar:** Diagn√≥sticos BSC baseados no perfil do cliente (sem retrieval em docs)

**Crit√©rios aplicados:**
- ‚úÖ **Qualidade da an√°lise** (coer√™ncia, profundidade)
- ‚úÖ **Relev√¢ncia ao perfil do cliente**
- ‚úÖ **Viabilidade das recomenda√ß√µes**
- ‚ùå **Cita√ß√£o de fontes N√ÉO esperada** (n√£o penaliza)
- ‚ùå **Fundamenta√ß√£o em docs N√ÉO esperada** (n√£o penaliza)

**Exemplo de uso:**
```python
judgment = judge.evaluate(
    original_query="Diagn√≥stico BSC para TechCorp",
    agent_response="""
        EXECUTIVE SUMMARY:
        Empresa TechCorp apresenta s√≥lido desempenho financeiro mas 
        enfrenta desafios em reten√ß√£o de clientes (churn 15%).
        
        RECOMENDA√á√ïES:
        1. [HIGH] Implementar Customer Success estruturado
        2. [MEDIUM] Melhorar onboarding de clientes
    """,
    retrieved_documents="[Perfil cliente: TechCorp, setor tecnologia, 500 funcion√°rios...]",
    agent_name="Diagnostic Agent",
    evaluation_context="DIAGNOSTIC"  # ‚Üê Fontes N√ÉO esperadas
)
```

**Crit√©rio de aprova√ß√£o (simplificado):**
```
approved: quality_score >= 0.7
needs_improvement: 0.5 <= quality_score < 0.7
rejected: quality_score < 0.5
```

**Foco da avalia√ß√£o:**
- An√°lise est√° alinhada com perfil do cliente?
- Recomenda√ß√µes s√£o l√≥gicas e vi√°veis?
- Diagn√≥stico aborda aspectos relevantes?

---

### 3. **TOOLS** (Ferramentas Consultivas - Futuro)

**Quando usar:** Sa√≠das de ferramentas consultivas (SWOT, Five Whys, Issue Tree, etc.)

**Status:** Planejado para implementa√ß√£o futura

**Crit√©rios esperados:**
- ‚úÖ **Estrutura adequada da ferramenta** (ex: matriz SWOT 2x2)
- ‚úÖ **Cita√ß√£o de fontes ESPERADA** (retrieval em docs BSC)
- ‚úÖ **Relev√¢ncia ao contexto do cliente**
- ‚úÖ **Profundidade da an√°lise**

---

## üîß API Reference

### `JudgeAgent.evaluate()`

```python
def evaluate(
    self,
    original_query: str,
    agent_response: str,
    retrieved_documents: str,
    agent_name: str = "Unknown Agent",
    evaluation_context: str = "RAG"  # ‚Üê NOVO PAR√ÇMETRO
) -> JudgmentResult:
    """
    Avalia resposta de um agente especialista (context-aware).
    
    Args:
        original_query: Pergunta original do usu√°rio
        agent_response: Resposta fornecida pelo agente
        retrieved_documents: Documentos recuperados e usados
        agent_name: Nome do agente que forneceu a resposta
        evaluation_context: Contexto da avalia√ß√£o:
            - 'RAG': Respostas com retrieval (fontes esperadas) [DEFAULT]
            - 'DIAGNOSTIC': Diagn√≥stico sem retrieval (fontes n√£o esperadas)
            - 'TOOLS': Ferramentas consultivas (futuro)
    
    Returns:
        JudgmentResult com avalia√ß√£o completa
    """
```

### `JudgeAgent.evaluate_multiple()`

```python
def evaluate_multiple(
    self,
    original_query: str,
    agent_responses: List[Dict[str, Any]],
    retrieved_documents: str,
    evaluation_context: str = "RAG"  # ‚Üê NOVO PAR√ÇMETRO
) -> List[Dict[str, Any]]:
    """
    Avalia m√∫ltiplas respostas (de diferentes agentes).
    
    Args:
        original_query: Pergunta original
        agent_responses: Lista de respostas
        retrieved_documents: Documentos recuperados
        evaluation_context: Contexto da avalia√ß√£o ('RAG', 'DIAGNOSTIC', 'TOOLS')
    
    Returns:
        Lista de avalia√ß√µes ordenadas por quality_score
    """
```

---

## üìä Compara√ß√£o de Crit√©rios

| Crit√©rio | RAG | DIAGNOSTIC | TOOLS (futuro) |
|----------|-----|------------|----------------|
| **Cita√ß√£o de fontes** | ‚úÖ OBRIGAT√ìRIO | ‚ùå N√£o esperado | ‚úÖ OBRIGAT√ìRIO |
| **Fundamenta√ß√£o em docs** | ‚úÖ OBRIGAT√ìRIO | ‚ùå N√£o esperado | ‚úÖ OBRIGAT√ìRIO |
| **Detec√ß√£o alucina√ß√µes** | ‚úÖ ATIVA | ‚ùå N/A | ‚úÖ ATIVA |
| **Qualidade da an√°lise** | ‚úÖ Avaliada | ‚úÖ **FOCO PRINCIPAL** | ‚úÖ Avaliada |
| **Relev√¢ncia ao cliente** | ‚úÖ Avaliada | ‚úÖ **FOCO PRINCIPAL** | ‚úÖ Avaliada |
| **Penaliza√ß√£o por falta de fontes** | ‚úÖ SIM (se score < 0.85) | ‚ùå **N√ÉO** | ‚úÖ SIM |

---

## üéì Exemplos Pr√°ticos

### Exemplo 1: Diagn√≥stico BSC (context='DIAGNOSTIC')

```python
from src.agents.judge_agent import JudgeAgent

judge = JudgeAgent()

# Diagn√≥stico SEM fontes (an√°lise do perfil do cliente)
diagnostic = """
EXECUTIVE SUMMARY:
Empresa TechCorp (setor tecnologia, 500 funcion√°rios) apresenta:
- S√≥lido desempenho financeiro (EBITDA 35%, crescimento 25% a.a.)
- Desafios cr√≠ticos em reten√ß√£o de clientes (churn 15%)
- Processos de desenvolvimento lentos (18 meses ciclo)

GAPS IDENTIFICADOS:
- Perspectiva Clientes: NPS 45 (benchmark 70+)
- Perspectiva Processos: Time-to-market lento
- Perspectiva Aprendizado: Turnover engenheiros 22% (mercado 10%)

RECOMENDA√á√ïES PRIORIZADAS:
1. [HIGH] Implementar Customer Success estruturado (ROI: -30% churn em 6 meses)
2. [HIGH] Adotar metodologia Agile (ROI: -40% time-to-market em 12 meses)
3. [MEDIUM] Programa reten√ß√£o talentos (ROI: -50% turnover em 18 meses)
"""

judgment = judge.evaluate(
    original_query="Diagn√≥stico BSC para TechCorp",
    agent_response=diagnostic,
    retrieved_documents="[Perfil cliente: TechCorp, setor tecnologia, 500 funcion√°rios...]",
    agent_name="Diagnostic Agent",
    evaluation_context="DIAGNOSTIC"  # ‚Üê Fontes N√ÉO esperadas
)

print(f"Score: {judgment.quality_score:.2f}")  # Ex: 0.85
print(f"Verdict: {judgment.verdict}")  # Ex: 'approved'
print(f"Reasoning: {judgment.reasoning}")
# Ex: "Diagn√≥stico abrangente com an√°lise coerente das 4 perspectivas BSC.
#      Recomenda√ß√µes priorizadas com ROI estimado demonstram viabilidade."
```

**Resultado esperado:**
- ‚úÖ **Aprovado** (foco em qualidade da an√°lise, n√£o fontes)
- ‚úÖ Score alto (0.80-0.90) se an√°lise for coerente
- ‚ùå Sem penaliza√ß√£o por falta de fontes

---

### Exemplo 2: Resposta RAG (context='RAG')

```python
# Resposta RAG COM fontes
rag_response = """
O Balanced Scorecard (BSC) √© um sistema de gest√£o estrat√©gica desenvolvido por 
Robert Kaplan e David Norton em 1992 que traduz a vis√£o e estrat√©gia da organiza√ß√£o 
em objetivos mensur√°veis organizados em 4 perspectivas balanceadas:

1. **Perspectiva Financeira**: Objetivos financeiros tradicionais como crescimento 
   de receita, lucratividade, ROI (Fonte: Kaplan & Norton, 1996, p. 8-12)

2. **Perspectiva Clientes**: Satisfa√ß√£o, reten√ß√£o, aquisi√ß√£o e participa√ß√£o de 
   mercado (Fonte: Kaplan & Norton, 1996, p. 13-18)

3. **Perspectiva Processos Internos**: Processos cr√≠ticos que geram valor para 
   clientes e acionistas (Fonte: Kaplan & Norton, 1996, p. 19-24)

4. **Perspectiva Aprendizado e Crescimento**: Capacidades organizacionais, sistemas 
   e cultura necess√°rios para inova√ß√£o (Fonte: Kaplan & Norton, 1996, p. 25-30)
"""

judgment = judge.evaluate(
    original_query="O que √© Balanced Scorecard?",
    agent_response=rag_response,
    retrieved_documents="[Kaplan, R. S., & Norton, D. P. (1996). The Balanced Scorecard...]",
    agent_name="RAG Agent",
    evaluation_context="RAG"  # ‚Üê Fontes ESPERADAS (default)
)

print(f"Score: {judgment.quality_score:.2f}")  # Ex: 0.95
print(f"Verdict: {judgment.verdict}")  # Ex: 'approved'
print(f"has_sources: {judgment.has_sources}")  # True
print(f"is_grounded: {judgment.is_grounded}")  # True
```

**Resultado esperado:**
- ‚úÖ **Aprovado** (fontes citadas adequadamente)
- ‚úÖ Score alto (0.90-1.0) se bem fundamentado
- ‚úÖ `has_sources=True`, `is_grounded=True`

---

### Exemplo 3: Resposta RAG SEM fontes (penalizada)

```python
# Resposta RAG SEM fontes
rag_response_no_sources = """
O Balanced Scorecard √© um sistema de gest√£o que organiza objetivos em 4 perspectivas:
Financeira, Clientes, Processos Internos, e Aprendizado e Crescimento.
Ajuda empresas a medir performance e executar estrat√©gia de forma balanceada.
"""

judgment = judge.evaluate(
    original_query="O que √© Balanced Scorecard?",
    agent_response=rag_response_no_sources,
    retrieved_documents="[Kaplan & Norton, 1996...]",
    agent_name="RAG Agent",
    evaluation_context="RAG"  # ‚Üê Fontes ESPERADAS
)

print(f"Score: {judgment.quality_score:.2f}")  # Ex: 0.65
print(f"Verdict: {judgment.verdict}")  # Ex: 'needs_improvement' ou 'rejected'
print(f"has_sources: {judgment.has_sources}")  # False
print(f"Issues: {judgment.issues}")
# Ex: ["N√£o cita fontes apropriadamente", "Falta fundamenta√ß√£o nos documentos"]
```

**Resultado esperado:**
- ‚ö†Ô∏è **Needs improvement** ou **Rejected** (falta de fontes penaliza)
- ‚ö†Ô∏è Score m√©dio/baixo (0.50-0.70)
- ‚ùå `has_sources=False`, issues listam falta de fontes

---

## üîÑ Integra√ß√£o com Workflow

### Fluxo RAG Tradicional

```python
# src/graph/workflow.py - judge_evaluation()

def judge_evaluation(self, state: BSCState) -> dict[str, Any]:
    """Avalia resposta agregada (RAG tradicional)."""
    
    judgment = self.judge.evaluate(
        original_query=state.query,
        agent_response=state.aggregated_response,
        retrieved_documents="[Documentos recuperados pelos agentes]",
        agent_name="Synthesized Response",
        evaluation_context="RAG"  # ‚Üê Fontes ESPERADAS (default)
    )
    
    # ... processar judgment
```

### Fluxo Diagn√≥stico (DISCOVERY)

```python
# src/graph/consulting_orchestrator.py - coordinate_discovery()

async def coordinate_discovery(self, state: BSCState) -> dict[str, Any]:
    """Coordena diagn√≥stico BSC."""
    
    # Gerar diagn√≥stico (sem retrieval)
    complete_diagnostic = await diagnostic_agent.run_diagnostic(state)
    
    # Avaliar diagn√≥stico com Judge (context='DIAGNOSTIC')
    judgment = self.judge.evaluate(
        original_query="Diagn√≥stico BSC completo",
        agent_response=self._format_diagnostic_for_judge(complete_diagnostic),
        retrieved_documents=f"[Perfil cliente: {state.client_profile.company.name}...]",
        agent_name="Diagnostic Agent",
        evaluation_context="DIAGNOSTIC"  # ‚Üê Fontes N√ÉO esperadas
    )
    
    # ... processar judgment
```

---

## ‚úÖ Valida√ß√£o e Testes

### Testes Unit√°rios

Arquivo: `tests/test_judge_context_aware.py`

**Cobertura:**
- ‚úÖ Context 'DIAGNOSTIC' aceita diagn√≥sticos sem fontes
- ‚úÖ Context 'DIAGNOSTIC' foca em qualidade da an√°lise
- ‚úÖ Context 'RAG' exige fontes
- ‚úÖ Context 'RAG' aprova respostas com fontes
- ‚úÖ Mesmo conte√∫do avaliado diferente em contextos diferentes
- ‚úÖ Compatibilidade retroativa (default 'RAG')
- ‚úÖ evaluate_multiple propaga contexto
- ‚úÖ Smoke tests para todos contextos

**Executar testes:**
```bash
pytest tests/test_judge_context_aware.py -v
```

### Demonstra√ß√£o Interativa

Arquivo: `examples/judge_context_aware_demo.py`

**Demos inclu√≠das:**
1. Avalia√ß√£o RAG (fontes esperadas)
2. Avalia√ß√£o DIAGN√ìSTICO (fontes n√£o esperadas)
3. Compara√ß√£o lado a lado

**Executar demo:**
```bash
python examples/judge_context_aware_demo.py
```

---

## üìö Decis√µes de Design

### Por que Context-Aware ao inv√©s de Separate Prompts?

**Decis√£o:** Usar 1 m√©todo `evaluate()` com par√¢metro `evaluation_context`

**Alternativas consideradas:**
- ‚ùå **Separate prompts**: Criar `evaluate_diagnostic()`, `evaluate_rag()` ‚Üí Duplica√ß√£o de c√≥digo
- ‚ùå **Relaxar crit√©rio global**: Mudar Judge para sempre tolerar falta de fontes ‚Üí Reduz precis√£o RAG
- ‚ùå **Add retrieval ao diagn√≥stico**: Fazer DiagnosticAgent buscar docs ‚Üí Muda conceito, lat√™ncia +3-5s

**Trade-offs:**
| Solu√ß√£o | Complexidade | Precis√£o | Manuten√ß√£o | Flexibilidade |
|---------|--------------|----------|------------|---------------|
| **Context-Aware** ‚úÖ | Baixa | Alta | F√°cil | Alta |
| Separate prompts | M√©dia | Alta | Dif√≠cil (2x c√≥digo) | M√©dia |
| Relaxar crit√©rio | Baixa | **Reduz** | F√°cil | Baixa |
| Add retrieval | Muito alta | Alta | Dif√≠cil | Baixa |

**Benef√≠cios da solu√ß√£o escolhida:**
1. ‚úÖ **Baixa complexidade**: 1 par√¢metro + ajuste de prompt
2. ‚úÖ **Alta precis√£o**: Mant√©m rigor RAG, relaxa DIAGNOSTIC
3. ‚úÖ **Flexibilidade**: F√°cil adicionar novos contextos (TOOLS, etc.)
4. ‚úÖ **Compatibilidade**: Default 'RAG' mant√©m comportamento original
5. ‚úÖ **Manuten√ß√£o**: C√≥digo centralizado, sem duplica√ß√£o

---

## üéØ Casos de Uso

### Caso 1: Diagn√≥stico BSC Inicial

**Cen√°rio:** Cliente novo completa onboarding, DiagnosticAgent gera diagn√≥stico inicial

**Contexto:** `DIAGNOSTIC`

**Motivo:** Diagn√≥stico baseado APENAS em perfil do cliente (contexto, desafios, objetivos), sem retrieval em literatura BSC

**Expectativa:** Judge avalia QUALIDADE DA AN√ÅLISE e COER√äNCIA das recomenda√ß√µes, n√£o penaliza por falta de fontes

---

### Caso 2: Ferramenta SWOT

**Cen√°rio:** Cliente solicita an√°lise SWOT, ferramenta busca literatura BSC sobre SWOT + contexto do cliente

**Contexto:** `TOOLS` (futuro)

**Motivo:** Ferramenta faz retrieval em docs BSC, fontes s√£o ESPERADAS

**Expectativa:** Judge avalia estrutura SWOT + cita√ß√£o de fontes + relev√¢ncia ao cliente

---

### Caso 3: Query RAG Tradicional

**Cen√°rio:** Cliente pergunta "O que √© Balanced Scorecard?", agentes fazem retrieval

**Contexto:** `RAG` (default)

**Motivo:** Resposta gerada com retrieval, fontes s√£o ESPERADAS

**Expectativa:** Judge avalia cita√ß√£o de fontes + fundamenta√ß√£o + qualidade da resposta

---

## üîÆ Roadmap

### Fase 1: Implementado ‚úÖ

- ‚úÖ Judge context-aware com 'RAG' e 'DIAGNOSTIC'
- ‚úÖ Testes unit√°rios completos
- ‚úÖ Demonstra√ß√£o interativa
- ‚úÖ Documenta√ß√£o completa

### Fase 2: Pr√≥ximos Passos (Planejado)

- ‚è≥ **Context 'TOOLS'**: Adicionar suporte a ferramentas consultivas
- ‚è≥ **M√©tricas por contexto**: Tracking separado de approval rate por contexto
- ‚è≥ **Feedback loop**: Integrar avalia√ß√µes Judge em refinamento de diagn√≥sticos

### Fase 3: Futuro

- üîú **Context 'REFINEMENT'**: Avalia√ß√£o de diagn√≥sticos refinados (feedback cliente)
- üîú **Dynamic thresholds**: Ajustar thresholds de aprova√ß√£o por contexto via config
- üîú **Multi-language**: Suporte a avalia√ß√µes em m√∫ltiplos idiomas

---

## üìù Changelog

### v1.0 - Novembro 2025

**Adicionado:**
- Par√¢metro `evaluation_context` em `evaluate()` e `evaluate_multiple()`
- Prompt context-aware com instru√ß√µes espec√≠ficas por contexto
- Contextos 'RAG' e 'DIAGNOSTIC' implementados
- Testes unit√°rios (10+ testes, 100% cobertura)
- Demonstra√ß√£o interativa
- Documenta√ß√£o completa

**Modificado:**
- `JudgeAgent.__init__()`: Removido chain est√°tico (agora criado dinamicamente)
- `JudgeAgent._create_prompt()`: Aceita par√¢metro `evaluation_context`
- `JudgeAgent.evaluate()`: Cria chain dinamicamente por contexto

**Compatibilidade:**
- ‚úÖ Compatibilidade retroativa mantida (default 'RAG')
- ‚úÖ Comportamento original preservado para c√≥digo existente

---

## ü§ù Contribuindo

Ao adicionar novos contextos de avalia√ß√£o:

1. **Adicionar contexto em `_create_prompt()`:**
```python
elif evaluation_context == "NEW_CONTEXT":
    context_instructions = """..."""
    verdict_rules = """..."""
```

2. **Criar testes em `tests/test_judge_context_aware.py`:**
```python
def test_new_context_behavior(judge):
    judgment = judge.evaluate(..., evaluation_context="NEW_CONTEXT")
    assert judgment.verdict == "expected"
```

3. **Documentar em `docs/JUDGE_CONTEXT_AWARE.md`:**
- Adicionar se√ß√£o no "Contextos de Avalia√ß√£o"
- Adicionar exemplos pr√°ticos
- Atualizar tabela comparativa

---

## üìß Contato

**Problema identificado por:** Usu√°rio do sistema BSC RAG Agent  
**Solu√ß√£o implementada por:** AI Agent (Cursor)  
**Data:** Novembro 2025  
**Vers√£o:** 1.0

