"""
Script para construir a base de conhecimento a partir dos documentos BSC.

Pipeline completo:
1. Carrega PDFs
2. Chunk com TableAwareChunker
3. Contextual Retrieval (Anthropic) - adiciona contexto aos chunks
4. Embeddings (OpenAI text-embedding-3-large)
5. Armazena em Vector Store (Qdrant/Weaviate/Redis)
"""
import os
import sys
from pathlib import Path
from tqdm import tqdm
from loguru import logger
from typing import List, Dict, Any

# Adiciona o diret√≥rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from pypdf import PdfReader
from src.rag.vector_store_factory import create_vector_store
from src.rag.embeddings import EmbeddingManager
from src.rag.chunker import TableAwareChunker
from src.rag.contextual_chunker import ContextualChunker
from config.settings import settings


def load_pdf(file_path: str) -> dict:
    """Carrega um arquivo PDF."""
    try:
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        
        return {
            "content": text,
            "source": os.path.basename(file_path),
            "num_pages": len(reader.pages)
        }
    except Exception as e:
        logger.error(f"Erro ao carregar {file_path}: {e}")
        return None


def main():
    """Fun√ß√£o principal do pipeline de ingest√£o."""
    logger.info("=" * 70)
    logger.info("üöÄ Pipeline de Ingest√£o - Base de Conhecimento BSC")
    logger.info("=" * 70)
    
    # Inicializa componentes
    logger.info("\nüì¶ Inicializando componentes...")
    vector_store = create_vector_store()
    logger.info(f"   ‚úÖ Vector Store: {type(vector_store).__name__}")
    
    embedding_manager = EmbeddingManager()
    logger.info(f"   ‚úÖ Embeddings: {embedding_manager.provider} ({embedding_manager.model_name if embedding_manager.provider == 'openai' else 'fine-tuned'})")
    
    chunker = TableAwareChunker()
    logger.info(f"   ‚úÖ Chunker: TableAwareChunker")
    
    # Contextual Retrieval (opcional)
    contextual_chunker = None
    if settings.enable_contextual_retrieval and settings.anthropic_api_key:
        contextual_chunker = ContextualChunker()
        logger.info(f"   ‚úÖ Contextual Retrieval: Habilitado (Anthropic)")
    else:
        logger.info(f"   ‚ö†Ô∏è  Contextual Retrieval: Desabilitado")
    
    # Cria/recria √≠ndice
    logger.info(f"\n[SETUP] Criando √≠ndice '{settings.vector_store_index}'...")
    try:
        vector_store.create_index(
            index_name=settings.vector_store_index,
            force_recreate=True
        )
        logger.info("   [OK] √çndice criado com sucesso")
    except Exception as e:
        logger.error(f"   ‚ùå Erro ao criar √≠ndice: {e}")
        return
    
    # Carrega documentos
    literature_dir = Path(settings.literature_dir)
    pdf_files = list(literature_dir.glob("*.pdf"))
    md_files = list(literature_dir.glob("*.md"))
    all_files = pdf_files + md_files
    
    if not all_files:
        logger.warning(f"\n[WARN] Nenhum arquivo PDF ou MD encontrado em {literature_dir}")
        logger.info("[INFO] Por favor, adicione arquivos da literatura BSC neste diret√≥rio.")
        logger.info("   Exemplo: papers sobre Balanced Scorecard, livros, artigos, etc.")
        return
    
    logger.info(f"\n[INFO] Encontrados {len(pdf_files)} PDFs e {len(md_files)} Markdown")
    
    all_chunks: List[Dict[str, Any]] = []
    
    # Processa cada documento
    logger.info("\n[PROCESS] Processando documentos...")
    for doc_file in tqdm(all_files, desc="Documentos"):
        logger.info(f"   [DOC] {doc_file.name}")
        
        # Carrega documento (PDF ou MD)
        if doc_file.suffix == '.pdf':
            doc = load_pdf(str(doc_file))
        else:  # .md
            with open(doc_file, 'r', encoding='utf-8') as f:
                content = f.read()
                doc = {
                    "content": content,
                    "source": doc_file.name,
                    "num_pages": 1
                }
        
        if not doc:
            logger.error(f"      [ERRO] Erro ao carregar")
            continue
        
        # Divide em chunks (com ou sem Contextual Retrieval)
        if contextual_chunker:
            logger.info(f"      [CONTEXT] Aplicando Contextual Retrieval...")
            contextual_chunks = contextual_chunker.chunk_text(
                text=doc["content"],
                metadata={
                    "source": doc["source"],
                    "num_pages": doc["num_pages"]
                }
            )
            # Converte ContextualChunk para dict format
            chunks = [
                {
                    "content": c.contextual_content,  # Usa conte√∫do com contexto
                    **c.metadata
                }
                for c in contextual_chunks
            ]
        else:
            chunks = chunker.chunk_text(
                doc["content"],
                metadata={
                    "source": doc["source"],
                    "num_pages": doc["num_pages"]
                }
            )
        
        all_chunks.extend(chunks)
        logger.info(f"      [OK] {len(chunks)} chunks criados")
    
    logger.info(f"\nüìä Total de chunks: {len(all_chunks)}")
    
    if not all_chunks:
        logger.error("‚ùå Nenhum chunk foi criado. Verifique os documentos.")
        return
    
    # Gera embeddings
    logger.info("\nüß¨ Gerando embeddings...")
    texts = [chunk["content"] for chunk in all_chunks]
    embeddings = embedding_manager.embed_batch(texts, batch_size=32)
    logger.info(f"   ‚úÖ {len(embeddings)} embeddings gerados")
    
    # Adiciona ao vector store
    logger.info(f"\nüíæ Adicionando ao {type(vector_store).__name__}...")
    
    # Prepara documentos com IDs √∫nicos
    documents_to_add = []
    embeddings_list = []
    
    for i, (chunk, embedding) in enumerate(zip(all_chunks, embeddings)):
        # Monta doc_dict com metadata corretamente
        metadata = {k: v for k, v in chunk.items() if k != "content"}
        
        doc_dict = {
            "id": f"doc_{i}",
            "content": chunk["content"],
            "metadata": metadata
        }
        documents_to_add.append(doc_dict)
        
        # Converte embedding para lista se for numpy array
        if hasattr(embedding, 'tolist'):
            embeddings_list.append(embedding.tolist())
        elif isinstance(embedding, list):
            embeddings_list.append(embedding)
        else:
            # Se j√° for outro formato, converte para lista
            embeddings_list.append(list(embedding))
    
    try:
        # Adiciona em batches para evitar payload muito grande
        batch_size = 100  # Qdrant tem limite de 33MB por request
        total_docs = len(documents_to_add)
        
        logger.info(f"[DEBUG] Adicionando {total_docs} docs em batches de {batch_size}...")
        
        for i in range(0, total_docs, batch_size):
            batch_docs = documents_to_add[i:i+batch_size]
            batch_embeddings = embeddings_list[i:i+batch_size]
            
            vector_store.add_documents(
                documents=batch_docs,
                embeddings=batch_embeddings
            )
            logger.info(f"   [BATCH] {i+len(batch_docs)}/{total_docs} documentos adicionados ({int((i+len(batch_docs))/total_docs*100)}%)")
        
        logger.info("   ‚úÖ Todos os documentos adicionados com sucesso")
    except Exception as e:
        import traceback
        logger.error(f"   ‚ùå Erro ao adicionar documentos: {e}")
        logger.error(f"[DEBUG] Traceback completo:\n{traceback.format_exc()}")
        return
    
    # Estat√≠sticas finais
    logger.info("\n" + "=" * 70)
    logger.info("‚úÖ Base de Conhecimento Constru√≠da com Sucesso!")
    logger.info("=" * 70)
    
    try:
        stats = vector_store.get_stats()
        logger.info(f"üìä Estat√≠sticas:")
        logger.info(f"   ‚Ä¢ Documentos indexados: {stats.num_docs}")
        logger.info(f"   ‚Ä¢ Dimens√£o dos vetores: {stats.vector_dimension}")
        logger.info(f"   ‚Ä¢ Vector Store: {stats.store_type}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  N√£o foi poss√≠vel obter estat√≠sticas: {e}")
    
    # Teste r√°pido de retrieval
    logger.info("\nüß™ Executando teste r√°pido...")
    test_query = "O que √© Balanced Scorecard?"
    
    try:
        test_embedding = embedding_manager.embed_text(test_query)
        results = vector_store.vector_search(
            query_embedding=test_embedding,
            k=3
        )
        
        logger.info(f"   Query: '{test_query}'")
        logger.info(f"   Resultados: {len(results)}")
        
        if results:
            logger.info(f"\n   üìÑ Melhor resultado:")
            logger.info(f"      Fonte: {results[0].metadata.get('source', 'N/A')}")
            logger.info(f"      Score: {results[0].score:.4f}")
            logger.info(f"      Preview: {results[0].content[:150]}...")
        else:
            logger.warning("   ‚ö†Ô∏è  Nenhum resultado encontrado")
    except Exception as e:
        logger.error(f"   ‚ùå Erro no teste: {e}")
    
    logger.info("\n" + "=" * 70)
    logger.info("üéâ Pipeline de Ingest√£o Completo!")
    logger.info("=" * 70)
    logger.info("üí° Pr√≥ximos passos:")
    logger.info("   1. Testar retrieval com queries reais")
    logger.info("   2. Ajustar par√¢metros de chunking se necess√°rio")
    logger.info("   3. Avaliar qualidade dos resultados")
    logger.info("=" * 70)


if __name__ == "__main__":
    main()
