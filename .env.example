# ==============================================================================
# AGENTE BSC RAG - Configuracao Atualizada (2025)
# ==============================================================================

# ------------------------------------------------------------------------------
# APIs de IA (OBRIGATORIO)
# ------------------------------------------------------------------------------

# OpenAI API Key (OBRIGATORIO - usado para embeddings e pode ser usado para LLM)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Cohere API Key (OBRIGATORIO - usado para re-ranking)
COHERE_API_KEY=your_cohere_api_key_here

# Anthropic API Key (OPCIONAL - necessaria se usar modelos Claude)
# Se usar Claude como LLM principal, esta chave e OBRIGATORIA
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-sonnet-4-5-20250929

# ------------------------------------------------------------------------------
# Modelo LLM Padrao
# ------------------------------------------------------------------------------
# Pode ser 'gpt-*' (OpenAI) ou 'claude-*' (Anthropic)
# Sistema detecta automaticamente o provider pelo prefixo
DEFAULT_LLM_MODEL=claude-sonnet-4-5-20250929
# Alternativas:
# DEFAULT_LLM_MODEL=gpt-4-turbo-preview
# DEFAULT_LLM_MODEL=gpt-5-2025-08-07

# ------------------------------------------------------------------------------
# Vector Store Configuration
# ------------------------------------------------------------------------------
# Tipo de vector store: 'qdrant', 'weaviate' ou 'redis'
VECTOR_STORE_TYPE=qdrant
VECTOR_STORE_INDEX=bsc_documents

# Qdrant (recomendado para producao)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Weaviate (alternativa)
WEAVIATE_HOST=localhost
WEAVIATE_PORT=8080

# Redis (mantido para compatibilidade)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_INDEX_NAME=bsc_knowledge

# ------------------------------------------------------------------------------
# RAG Configuration
# ------------------------------------------------------------------------------
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K_RETRIEVAL=10
TOP_N_RERANK=5
HYBRID_SEARCH_WEIGHT_SEMANTIC=0.7
HYBRID_SEARCH_WEIGHT_BM25=0.3

# ------------------------------------------------------------------------------
# Contextual Retrieval (Anthropic ou OpenAI GPT-5)
# ------------------------------------------------------------------------------
ENABLE_CONTEXTUAL_RETRIEVAL=True
CONTEXTUAL_MODEL=claude-sonnet-4-5-20250929
CONTEXTUAL_CACHE_ENABLED=True
CONTEXTUAL_PROVIDER=openai
# Opcoes: 'openai' ou 'anthropic'

# GPT-5 Configuration (se usar GPT-5 para contextual retrieval)
GPT5_MODEL=gpt-5-2025-08-07
GPT5_MAX_COMPLETION_TOKENS=2048
GPT5_REASONING_EFFORT=minimal
# Opcoes: 'minimal', 'low', 'medium', 'high'

# ------------------------------------------------------------------------------
# Embedding Cache Configuration
# ------------------------------------------------------------------------------
EMBEDDING_CACHE_ENABLED=True
EMBEDDING_CACHE_DIR=.cache/embeddings
EMBEDDING_CACHE_TTL_DAYS=30
EMBEDDING_CACHE_MAX_SIZE_GB=5

# ------------------------------------------------------------------------------
# Query Decomposition (RAG Avancado - Fase 2A.1)
# ------------------------------------------------------------------------------
# Decompoe queries complexas BSC em sub-queries para melhorar recall (+30-40%)
ENABLE_QUERY_DECOMPOSITION=True
DECOMPOSITION_MIN_QUERY_LENGTH=30
DECOMPOSITION_SCORE_THRESHOLD=1
DECOMPOSITION_LLM=gpt-4o-mini

# ------------------------------------------------------------------------------
# Diversity Re-ranking (RAG Avançado - Fase 2A.2)
# ------------------------------------------------------------------------------
# MMR (Maximal Marginal Relevance) para balancear relevância vs diversidade
# Evita documentos repetidos/similares no top-k, garante variedade de fontes
ENABLE_DIVERSITY_RERANKING=True
DIVERSITY_LAMBDA=0.5              # 0.5 = balanceado, 1.0 = só relevância, 0.0 = só diversidade
DIVERSITY_THRESHOLD=0.8           # Similaridade máxima permitida entre docs (0-1)
METADATA_BOOST_ENABLED=True       # Boost docs de fontes/perspectivas diferentes
METADATA_SOURCE_BOOST=0.2         # +20% score para sources diferentes
METADATA_PERSPECTIVE_BOOST=0.15   # +15% score para perspectives BSC diferentes
ADAPTIVE_TOPN_ENABLED=True        # Ajustar top_n dinamicamente (query simples=5, complexa=15)

# Router Inteligente (RAG Avançado - Fase 2A.3)
# ------------------------------------------------------------------------------
# Query Router que classifica queries e escolhe estratégia de retrieval otimizada
# 4 estratégias: Direct (queries simples), Decomposition (complexas), Hybrid (conceituais), MultiHop (relacionais)
# Accuracy esperada: 85%+ | Latência overhead: <50ms (heurísticas), ~500ms (LLM fallback)
ENABLE_QUERY_ROUTER=True
ROUTER_USE_LLM_FALLBACK=True      # Usar LLM (GPT-4o-mini) para queries ambíguas (20% casos)
ROUTER_LLM_MODEL=gpt-4o-mini      # Modelo para LLM fallback (custo-efetivo)
ROUTER_CONFIDENCE_THRESHOLD=0.8   # Threshold para confiar em heurística vs LLM fallback
ROUTER_LOG_DECISIONS=True         # Logar todas decisões de routing para analytics
ROUTER_LOG_FILE=logs/routing_decisions.jsonl  # Arquivo de log (JSON Lines format)
SIMPLE_QUERY_MAX_WORDS=30         # Queries simples: <= 30 palavras
COMPLEX_QUERY_MIN_WORDS=30        # Queries complexas: > 30 palavras
RELATIONAL_KEYWORDS=relação,impacto,causa,efeito,depende,influencia,deriva  # Keywords relacionais
ENABLE_DIRECT_ANSWER_CACHE=True   # Cache para DirectAnswerStrategy (queries simples)
DIRECT_ANSWER_CACHE_TTL=3600      # TTL do cache em segundos (1 hora)

# ------------------------------------------------------------------------------
# Agent Configuration
# ------------------------------------------------------------------------------
MAX_ITERATIONS=10
TEMPERATURE=0.0
MAX_TOKENS=2000
AGENT_MAX_WORKERS=4
# Numero de agentes que podem processar em paralelo

# ------------------------------------------------------------------------------
# Embedding Fine-tuning (OPCIONAL)
# ------------------------------------------------------------------------------
USE_FINETUNED_EMBEDDINGS=False
FINETUNED_MODEL_PATH=./models/bsc-embeddings

# ------------------------------------------------------------------------------
# Human-in-the-loop
# ------------------------------------------------------------------------------
REQUIRE_APPROVAL_FOR_CRITICAL=True

# ------------------------------------------------------------------------------
# Monitoring
# ------------------------------------------------------------------------------
ENABLE_METRICS=True
METRICS_PORT=9090

# ------------------------------------------------------------------------------
# Application Configuration
# ------------------------------------------------------------------------------
APP_NAME=Agente BSC
APP_VERSION=1.0.0
DEBUG=False
LOG_LEVEL=INFO

# ------------------------------------------------------------------------------
# Paths
# ------------------------------------------------------------------------------
DATA_DIR=./data
LITERATURE_DIR=./data/bsc_literature
MODELS_DIR=./models
LOGS_DIR=./logs

# ==============================================================================
# INSTRUCOES DE USO
# ==============================================================================
#
# 1. CHAVES OBRIGATORIAS:
#    - OPENAI_API_KEY: Sempre necessaria (embeddings)
#    - COHERE_API_KEY: Sempre necessaria (re-ranking)
#    - ANTHROPIC_API_KEY: Necessaria apenas se DEFAULT_LLM_MODEL for 'claude-*'
#
# 2. ESCOLHA DO MODELO LLM:
#    - Para Claude: DEFAULT_LLM_MODEL=claude-sonnet-4-5-20250929
#      (requer ANTHROPIC_API_KEY)
#    - Para GPT-4: DEFAULT_LLM_MODEL=gpt-4-turbo-preview
#      (requer OPENAI_API_KEY)
#    - Para GPT-5: DEFAULT_LLM_MODEL=gpt-5-2025-08-07
#      (requer OPENAI_API_KEY)
#
# 3. VECTOR STORE:
#    - Qdrant (recomendado): docker-compose up -d
#    - Weaviate (alternativa): ajustar docker-compose.yml
#    - Redis (compatibilidade): usar Redis Stack
#
# 4. CONTEXTUAL RETRIEVAL:
#    - Se CONTEXTUAL_PROVIDER=openai, usar GPT-5
#    - Se CONTEXTUAL_PROVIDER=anthropic, usar Claude
#
# 5. CACHE DE EMBEDDINGS:
#    - Melhora drasticamente performance em queries repetidas
#    - Recomendado manter EMBEDDING_CACHE_ENABLED=True
#
# ==============================================================================
