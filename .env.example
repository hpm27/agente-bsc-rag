# ==============================================================================
# AGENTE BSC RAG - Configuracao Atualizada (2025)
# ==============================================================================

# ------------------------------------------------------------------------------
# APIs de IA (OBRIGATORIO)
# ------------------------------------------------------------------------------

# OpenAI API Key (OBRIGATORIO - usado para embeddings e pode ser usado para LLM)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Cohere API Key (OBRIGATORIO - usado para re-ranking)
COHERE_API_KEY=your_cohere_api_key_here

# Anthropic API Key (OPCIONAL - necessaria se usar modelos Claude)
# Se usar Claude como LLM principal, esta chave e OBRIGATORIA
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-sonnet-4-5-20250929

# ------------------------------------------------------------------------------
# Modelo LLM Padrao
# ------------------------------------------------------------------------------
# Pode ser 'gpt-*' (OpenAI) ou 'claude-*' (Anthropic)
# Sistema detecta automaticamente o provider pelo prefixo
DEFAULT_LLM_MODEL=claude-sonnet-4-5-20250929
# Alternativas:
# DEFAULT_LLM_MODEL=gpt-4-turbo-preview
# DEFAULT_LLM_MODEL=gpt-5-2025-08-07

# ------------------------------------------------------------------------------
# Vector Store Configuration
# ------------------------------------------------------------------------------
# Tipo de vector store: 'qdrant', 'weaviate' ou 'redis'
VECTOR_STORE_TYPE=qdrant
VECTOR_STORE_INDEX=bsc_documents

# Qdrant (recomendado para producao)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Weaviate (alternativa)
WEAVIATE_HOST=localhost
WEAVIATE_PORT=8080

# Redis (mantido para compatibilidade)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_INDEX_NAME=bsc_knowledge

# ------------------------------------------------------------------------------
# RAG Configuration
# ------------------------------------------------------------------------------
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K_RETRIEVAL=10
TOP_N_RERANK=5
HYBRID_SEARCH_WEIGHT_SEMANTIC=0.7
HYBRID_SEARCH_WEIGHT_BM25=0.3

# ------------------------------------------------------------------------------
# Contextual Retrieval (Anthropic ou OpenAI GPT-5)
# ------------------------------------------------------------------------------
ENABLE_CONTEXTUAL_RETRIEVAL=True
CONTEXTUAL_MODEL=claude-sonnet-4-5-20250929
CONTEXTUAL_CACHE_ENABLED=True
CONTEXTUAL_PROVIDER=openai
# Opcoes: 'openai' ou 'anthropic'

# GPT-5.1 Configuration (MIGRADO Nov 22, 2025 - contextual retrieval)
GPT5_MODEL=gpt-5.1
GPT5_MAX_COMPLETION_TOKENS=128000
# GPT-5.1 suporta até 128K tokens de output (mesmo que GPT-5)
# IMPORTANTE: gpt-5.1 (Thinking mode) APENAS aceita reasoning_effort='medium'!
# Para usar 'low' ou 'none', use gpt-5.1-chat-latest (Instant mode)
GPT5_REASONING_EFFORT=medium
# Opcoes GPT-5.1: 'none', 'low', 'medium', 'high' (GPT-5 'minimal' REMOVIDO!)

# Onboarding Agent LLM (GPT-5.1 family - MIGRADO Nov 22, 2025)
ONBOARDING_LLM_MODEL=gpt-5.1-chat-latest
# Opcoes:
# - gpt-5.1-chat-latest: GPT-5.1 Instant (2-3x mais rapido tarefas simples, $1.25 input/$10.00 output)
# - gpt-5.1: GPT-5.1 Thinking (reasoning profundo, mesmo pricing)
# - gpt-5-2025-08-07: GPT-5 legado (fallback se necessario)
# - gpt-5-mini-2025-08-07: Economico, $0.25 input/$2.00 output (2.5x/5x mais barato)

# Translation (Query PT<->EN)
TRANSLATION_LLM_MODEL=gpt-5-mini-2025-08-07
# Modelo para traducao de queries (tarefa simples, mini suficiente)

# Diagnostic Agent (Analise 4 perspectivas BSC - MIGRADO Nov 22, 2025)
DIAGNOSTIC_LLM_MODEL=gpt-5.1
# Opcoes:
# - gpt-5.1: GPT-5.1 Thinking (reasoning profundo, -50% tokens em tool calling, $1.25 input/$10.00 output)
# - gpt-5.1-chat-latest: GPT-5.1 Instant (mais rapido, mesmo pricing)
# - gpt-5-2025-08-07: GPT-5 legado (fallback se necessario)
# Modelo para analise diagnostica complexa (reasoning avancado necessario + 7 ferramentas consultivas)

# ------------------------------------------------------------------------------
# Embedding Cache Configuration
# ------------------------------------------------------------------------------
EMBEDDING_CACHE_ENABLED=True
EMBEDDING_CACHE_DIR=.cache/embeddings
EMBEDDING_CACHE_TTL_DAYS=30
EMBEDDING_CACHE_MAX_SIZE_GB=5

# ------------------------------------------------------------------------------
# Query Decomposition (RAG Avancado - Fase 2A.1)
# ------------------------------------------------------------------------------
# Decompoe queries complexas BSC em sub-queries para melhorar recall (+30-40%)
ENABLE_QUERY_DECOMPOSITION=True
DECOMPOSITION_MIN_QUERY_LENGTH=30
DECOMPOSITION_SCORE_THRESHOLD=1
DECOMPOSITION_LLM=gpt-5-mini-2025-08-07

# ------------------------------------------------------------------------------
# Diversity Re-ranking (RAG Avançado - Fase 2A.2)
# ------------------------------------------------------------------------------
# MMR (Maximal Marginal Relevance) para balancear relevância vs diversidade
# Evita documentos repetidos/similares no top-k, garante variedade de fontes
ENABLE_DIVERSITY_RERANKING=True
DIVERSITY_LAMBDA=0.5              # 0.5 = balanceado, 1.0 = só relevância, 0.0 = só diversidade
DIVERSITY_THRESHOLD=0.8           # Similaridade máxima permitida entre docs (0-1)
METADATA_BOOST_ENABLED=True       # Boost docs de fontes/perspectivas diferentes
METADATA_SOURCE_BOOST=0.2         # +20% score para sources diferentes
METADATA_PERSPECTIVE_BOOST=0.15   # +15% score para perspectives BSC diferentes
ADAPTIVE_TOPN_ENABLED=True        # Ajustar top_n dinamicamente (query simples=5, complexa=15)

# Router Inteligente (RAG Avançado - Fase 2A.3)
# ------------------------------------------------------------------------------
# Query Router que classifica queries e escolhe estratégia de retrieval otimizada
# 4 estratégias: Direct (queries simples), Decomposition (complexas), Hybrid (conceituais), MultiHop (relacionais)
# Accuracy esperada: 85%+ | Latência overhead: <50ms (heurísticas), ~500ms (LLM fallback)
ENABLE_QUERY_ROUTER=True
ROUTER_USE_LLM_FALLBACK=True      # Usar LLM (GPT-5 mini) para queries ambíguas (20% casos)
ROUTER_LLM_MODEL=gpt-5-mini-2025-08-07  # Modelo para LLM fallback (econômico)
ROUTER_CONFIDENCE_THRESHOLD=0.8   # Threshold para confiar em heurística vs LLM fallback
ROUTER_LOG_DECISIONS=True         # Logar todas decisões de routing para analytics
ROUTER_LOG_FILE=logs/routing_decisions.jsonl  # Arquivo de log (JSON Lines format)
SIMPLE_QUERY_MAX_WORDS=30         # Queries simples: <= 30 palavras
COMPLEX_QUERY_MIN_WORDS=30        # Queries complexas: > 30 palavras
RELATIONAL_KEYWORDS=relação,impacto,causa,efeito,depende,influencia,deriva  # Keywords relacionais
ENABLE_DIRECT_ANSWER_CACHE=True   # Cache para DirectAnswerStrategy (queries simples)
DIRECT_ANSWER_CACHE_TTL=3600      # TTL do cache em segundos (1 hora)

# ------------------------------------------------------------------------------
# Auto-Geração de Metadados (Fase 2 - Organização)
# ------------------------------------------------------------------------------
ENABLE_AUTO_METADATA_GENERATION=True  # Gerar metadados automaticamente para docs novos
SAVE_AUTO_METADATA=True               # Salvar metadados gerados no index.json (cache)
AUTO_METADATA_MODEL=gpt-5-mini-2025-08-07  # Modelo LLM para extração (econômico e rápido)
AUTO_METADATA_CONTENT_LIMIT=3000      # Palavras do documento para análise LLM

# ------------------------------------------------------------------------------
# Filtros por Perspectiva (Fase 2 - Integração)
# ------------------------------------------------------------------------------
ENABLE_PERSPECTIVE_FILTERS=True       # Filtrar retrieval por perspectiva BSC do agent

# ------------------------------------------------------------------------------
# Agent Configuration
# ------------------------------------------------------------------------------
MAX_ITERATIONS=10
TEMPERATURE=0.0
MAX_TOKENS=2000
AGENT_MAX_WORKERS=4
# Numero de agentes que podem processar em paralelo

# ------------------------------------------------------------------------------
# Embedding Fine-tuning (OPCIONAL)
# ------------------------------------------------------------------------------
USE_FINETUNED_EMBEDDINGS=False
FINETUNED_MODEL_PATH=./models/bsc-embeddings

# ------------------------------------------------------------------------------
# Human-in-the-loop
# ------------------------------------------------------------------------------
REQUIRE_APPROVAL_FOR_CRITICAL=True

# ------------------------------------------------------------------------------
# Monitoring
# ------------------------------------------------------------------------------
ENABLE_METRICS=True
METRICS_PORT=9090

# ------------------------------------------------------------------------------
# Application Configuration
# ------------------------------------------------------------------------------
APP_NAME=Agente BSC
APP_VERSION=1.0.0
DEBUG=False
LOG_LEVEL=INFO

# ------------------------------------------------------------------------------
# Paths
# ------------------------------------------------------------------------------
DATA_DIR=./data
LITERATURE_DIR=./data/bsc_literature
MODELS_DIR=./models
LOGS_DIR=./logs

# ------------------------------------------------------------------------------
# Memory Configuration (Mem0 Platform) - FASE 1.6
# ------------------------------------------------------------------------------
# OBRIGATORIO: Mem0 Platform API Key para memória persistente (ClientProfile)
# Obtenha em: https://app.mem0.ai/dashboard/api-keys
MEM0_API_KEY=m0-your-mem0-api-key-here

# Informações da organização e projeto Mem0 (OPCIONAL, auto-detectado)
MEM0_ORG_NAME=your-org-name
MEM0_ORG_ID=org_your_org_id_here
MEM0_PROJECT_ID=proj_your_project_id_here
MEM0_PROJECT_NAME=default-project

# Provider de memória (default: mem0, future: supabase, redis)
MEMORY_PROVIDER=mem0

# ==============================================================================
# INSTRUCOES DE USO
# ==============================================================================
#
# 1. CHAVES OBRIGATORIAS:
#    - OPENAI_API_KEY: Sempre necessaria (embeddings)
#    - COHERE_API_KEY: Sempre necessaria (re-ranking)
#    - MEM0_API_KEY: Sempre necessaria (memória persistente ClientProfile)
#    - ANTHROPIC_API_KEY: Necessaria apenas se DEFAULT_LLM_MODEL for 'claude-*'
#
# 2. ESCOLHA DO MODELO LLM:
#    - Para Claude: DEFAULT_LLM_MODEL=claude-sonnet-4-5-20250929
#      (requer ANTHROPIC_API_KEY)
#    - Para GPT-4: DEFAULT_LLM_MODEL=gpt-4-turbo-preview
#      (requer OPENAI_API_KEY)
#    - Para GPT-5: DEFAULT_LLM_MODEL=gpt-5-2025-08-07
#      (requer OPENAI_API_KEY)
#
# 3. VECTOR STORE:
#    - Qdrant (recomendado): docker-compose up -d
#    - Weaviate (alternativa): ajustar docker-compose.yml
#    - Redis (compatibilidade): usar Redis Stack
#
# 4. CONTEXTUAL RETRIEVAL:
#    - Se CONTEXTUAL_PROVIDER=openai, usar GPT-5
#    - Se CONTEXTUAL_PROVIDER=anthropic, usar Claude
#
# 5. CACHE DE EMBEDDINGS:
#    - Melhora drasticamente performance em queries repetidas
#    - Recomendado manter EMBEDDING_CACHE_ENABLED=True
#
# ==============================================================================
