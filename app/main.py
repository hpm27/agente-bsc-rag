"""
Interface Streamlit para o Agente BSC RAG.
"""
import streamlit as st
import asyncio
from datetime import datetime
from loguru import logger
import sys
from pathlib import Path

# Adicionar diret√≥rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.graph.workflow import create_bsc_workflow
from app.utils import (
    initialize_session_state,
    add_message,
    clear_chat_history,
    format_timestamp,
    format_latency,
    create_metrics_display,
    display_perspective_responses,
    display_judge_evaluation
)
from config.settings import settings


# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Agente BSC",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS customizado
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stAlert {
        margin-top: 1rem;
    }
</style>
""", unsafe_allow_html=True)


def main():
    """Fun√ß√£o principal da aplica√ß√£o."""
    
    # Inicializar estado
    initialize_session_state()
    
    # Header
    st.markdown('<div class="main-header">üìä Agente BSC</div>', unsafe_allow_html=True)
    st.markdown(
        '<div class="sub-header">Sistema RAG Multi-Agente para Balanced Scorecard</div>',
        unsafe_allow_html=True
    )
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√µes")
        
        # Informa√ß√µes do sistema
        st.subheader("Sistema")
        st.info(f"""
        **Vers√£o**: {settings.app_version}
        **Modelo**: {settings.openai_model}
        **Vector Store**: {settings.vector_store_type.upper()}
        **Sess√£o**: {st.session_state.session_id}
        """)
        
        # Configura√ß√µes de execu√ß√£o
        st.subheader("Execu√ß√£o")
        
        enable_perspectives = st.multiselect(
            "Perspectivas Ativas",
            options=["financial", "customer", "process", "learning"],
            default=["financial", "customer", "process", "learning"],
            format_func=lambda x: {
                "financial": "üí∞ Financeira",
                "customer": "üë• Cliente",
                "process": "‚öôÔ∏è Processos",
                "learning": "üìö Aprendizado"
            }[x]
        )
        
        show_details = st.checkbox("Mostrar Detalhes", value=True)
        show_sources = st.checkbox("Mostrar Fontes", value=True)
        show_judge = st.checkbox("Mostrar Avalia√ß√£o Judge", value=True)
        
        st.divider()
        
        # Estat√≠sticas
        st.subheader("üìà Estat√≠sticas")
        st.metric("Queries Realizadas", st.session_state.query_count)
        st.metric("Mensagens no Chat", len(st.session_state.messages))
        
        st.divider()
        
        # A√ß√µes
        if st.button("üóëÔ∏è Limpar Chat", use_container_width=True):
            clear_chat_history()
            st.rerun()
        
        if st.button("üîÑ Reiniciar Sess√£o", use_container_width=True):
            st.session_state.clear()
            st.rerun()
    
    # √Årea principal
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("üí¨ Chat")
        
        # Container para mensagens
        chat_container = st.container()
        
        with chat_container:
            # Exibir hist√≥rico de mensagens
            for message in st.session_state.messages:
                role = message["role"]
                content = message["content"]
                metadata = message.get("metadata", {})
                timestamp = message.get("timestamp")
                
                with st.chat_message(role):
                    st.markdown(content)
                    
                    if role == "assistant" and show_details and metadata:
                        # M√©tricas
                        if "judge_score" in metadata:
                            create_metrics_display(metadata)
                        
                        # Perspectivas
                        perspectives = metadata.get("perspectives", [])
                        if perspectives and show_sources:
                            st.markdown("---")
                            display_perspective_responses(perspectives)
                        
                        # Judge
                        judge_eval = metadata.get("judge_evaluation")
                        if judge_eval and show_judge:
                            st.markdown("---")
                            display_judge_evaluation(judge_eval)
                    
                    # Timestamp
                    if timestamp:
                        st.caption(f"üïê {format_timestamp(timestamp)}")
        
        # Input de query
        query = st.chat_input("Fa√ßa sua pergunta sobre Balanced Scorecard...")
        
        if query:
            # Adicionar mensagem do usu√°rio
            add_message("user", query)
            st.session_state.query_count += 1
            
            # Exibir mensagem do usu√°rio imediatamente
            with st.chat_message("user"):
                st.markdown(query)
                st.caption(f"üïê {format_timestamp()}")
            
            # Processar query
            with st.chat_message("assistant"):
                with st.spinner("Processando sua pergunta..."):
                    try:
                        # Executar workflow
                        start_time = datetime.now()
                        
                        # Criar workflow
                        workflow = create_bsc_workflow()
                        
                        # Executar
                        result = asyncio.run(workflow.run(
                            query=query,
                            session_id=st.session_state.session_id
                        ))
                        
                        end_time = datetime.now()
                        latency = (end_time - start_time).total_seconds()
                        
                        # Resposta
                        response = result.get("response", "Desculpe, n√£o consegui gerar uma resposta.")
                        metadata = result.get("metadata", {})
                        metadata["latency"] = latency
                        
                        # Adicionar metadados do resultado
                        metadata["judge_evaluation"] = result.get("judge_evaluation")
                        metadata["perspectives"] = result.get("perspectives", [])
                        
                        # Exibir resposta
                        st.markdown(response)
                        
                        # Exibir lat√™ncia
                        st.caption(f"‚ö° Processado em {format_latency(latency)}")
                        
                        # Exibir detalhes
                        if show_details:
                            # M√©tricas
                            if "judge_score" in metadata:
                                create_metrics_display(metadata)
                            
                            # Perspectivas
                            perspectives = metadata.get("perspectives", [])
                            if perspectives and show_sources:
                                st.markdown("---")
                                display_perspective_responses(perspectives)
                            
                            # Judge
                            judge_eval = metadata.get("judge_evaluation")
                            if judge_eval and show_judge:
                                st.markdown("---")
                                display_judge_evaluation(judge_eval)
                        
                        # Timestamp
                        st.caption(f"üïê {format_timestamp()}")
                        
                        # Adicionar mensagem do assistente ao hist√≥rico
                        add_message("assistant", response, metadata)
                        
                    except Exception as e:
                        logger.error(f"Erro ao processar query: {e}")
                        error_msg = f"‚ùå Erro ao processar sua pergunta: {str(e)}"
                        st.error(error_msg)
                        add_message("assistant", error_msg)
    
    with col2:
        st.subheader("üìö Exemplos de Queries")
        
        st.markdown("""
        **Queries Factuais:**
        - Quais s√£o os principais KPIs da perspectiva financeira?
        - Como medir a satisfa√ß√£o do cliente no BSC?
        - Que m√©tricas s√£o usadas na perspectiva de aprendizado?
        
        **Queries Conceituais:**
        - Como implementar BSC em uma empresa?
        - Qual a rela√ß√£o entre as perspectivas do BSC?
        - Por que o BSC √© importante para estrat√©gia?
        
        **Queries Comparativas:**
        - Qual a diferen√ßa entre KPIs financeiros e de clientes?
        - Como BSC se compara com outros frameworks?
        - Quais perspectivas impactam mais a lucratividade?
        
        **Queries Complexas:**
        - Como alinhar objetivos estrat√©gicos com m√©tricas BSC?
        - Quais s√£o as melhores pr√°ticas para implementa√ß√£o?
        - Como criar um mapa estrat√©gico BSC completo?
        """)
        
        st.divider()
        
        st.subheader("‚ÑπÔ∏è Sobre")
        st.markdown("""
        Este sistema utiliza:
        - **RAG (Retrieval-Augmented Generation)** com vector store moderno
        - **Multi-Agentes** especializados em cada perspectiva BSC
        - **LangGraph** para orquestra√ß√£o inteligente
        - **Judge Agent** para valida√ß√£o de qualidade
        - **Contextual Retrieval** para melhor precis√£o
        """)


if __name__ == "__main__":
    main()

