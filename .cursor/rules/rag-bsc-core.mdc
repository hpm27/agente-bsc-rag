---
alwaysApply: true
description: "Router central para projeto BSC RAG - Workflow obrigatÃ³rio, liÃ§Ãµes validadas do MVP, mapa de tÃ©cnicas RAG avanÃ§adas, e guia de navegaÃ§Ã£o. Sempre aplicado para garantir processo consistente durante toda a Fase 2 (6-8 semanas)."
version: "1.0"
last_updated: "2025-10-14"
---

# ğŸ§  BSC RAG - ROUTER CENTRAL

**Sistema de OrganizaÃ§Ã£o e NavegaÃ§Ã£o para ImplementaÃ§Ã£o de TÃ©cnicas RAG AvanÃ§adas**

Este router central Ã© **sempre aplicado** durante todo o desenvolvimento da Fase 2 (RAG AvanÃ§ado). Ele garante processo consistente, preserva contexto entre sessÃµes, e acelera decisÃµes tÃ©cnicas atravÃ©s de navegaÃ§Ã£o eficiente.

---

## ğŸ“‹ ÃNDICE NAVEGÃVEL

1. [ğŸš¨ Workflow ObrigatÃ³rio RAG](#workflow-obrigatÃ³rio-rag) - **7 steps** para implementar qualquer tÃ©cnica
2. [ğŸ“ LiÃ§Ãµes de ProduÃ§Ã£o MVP](#liÃ§Ãµes-de-produÃ§Ã£o-mvp) - **Top 5** descobertas validadas
3. [ğŸ—ºï¸ Mapa de TÃ©cnicas RAG](#mapa-de-tÃ©cnicas-rag) - **8 tÃ©cnicas** comparadas por ROI
4. [ğŸ¯ Guia por CenÃ¡rio](#guia-por-cenÃ¡rio-rag) - **4 cenÃ¡rios** prÃ¡ticos mapeados
5. [ğŸ“š LocalizaÃ§Ã£o da DocumentaÃ§Ã£o](#localizaÃ§Ã£o-da-documentaÃ§Ã£o) - NavegaÃ§Ã£o rÃ¡pida

---

## ğŸš¨ WORKFLOW OBRIGATÃ“RIO RAG

**ANTES de implementar QUALQUER tÃ©cnica RAG da Fase 2, siga estes 7 steps obrigatÃ³rios:**

### **Step 1: ğŸ§  Sequential Thinking**

**Objetivo:** Planejar arquitetura e identificar trade-offs ANTES de codificar.

**AÃ§Ãµes:**

- Usar ferramenta Sequential Thinking para raciocinar sobre a tÃ©cnica
- Identificar complexidade, tempo estimado, dependÃªncias
- Analisar trade-offs (latÃªncia, custo, complexidade vs benefÃ­cio)
- Validar se a tÃ©cnica Ã© necessÃ¡ria (evitar over-engineering)

**Exemplo:**

```
Thought 1: Query Decomposition - O que Ã©? Quebrar queries complexas em sub-queries.
Thought 2: Por quÃª precisamos? Queries BSC sÃ£o multi-perspectiva ("Como implementar BSC considerando finanÃ§as, clientes, processos?")
Thought 3: Complexidade? Baixa - usa LLM + RRF jÃ¡ implementado
Thought 4: ROI esperado? +30-50% answer quality (validado: Galileo AI, Epsilla)
Thought 5: DecisÃ£o? IMPLEMENTAR - alinhamento perfeito com use case BSC
```

---

### **Step 2: ğŸ¯ Discovery (RAG Techniques Catalog)**

**Objetivo:** Descobrir QUAL tÃ©cnica usar baseado em necessidade.

**AÃ§Ãµes:**

- Consultar seÃ§Ã£o [Mapa de TÃ©cnicas RAG](#mapa-de-tÃ©cnicas-rag) abaixo
- Comparar ROI, complexidade, tempo de implementaÃ§Ã£o
- Verificar quando usar vs quando NÃƒO usar
- Priorizar tÃ©cnicas de alto ROI e baixa complexidade (Quick Wins)

**Recursos:**

- `.cursor/rules/rag-techniques-catalog.mdc` â† Futuro (TIER 2)
- Tabela comparativa na seÃ§Ã£o 3 deste documento

---

### **Step 3: ğŸ—ºï¸ Navigation (Docs Index)**

**Objetivo:** Identificar documentaÃ§Ã£o relevante rapidamente.

**AÃ§Ãµes:**

- Consultar seÃ§Ã£o [LocalizaÃ§Ã£o da DocumentaÃ§Ã£o](#localizaÃ§Ã£o-da-documentaÃ§Ã£o)
- Navegar para `docs/techniques/[TECHNIQUE].md` se existir
- Buscar papers/artigos de referÃªncia (usar Brightdata se necessÃ¡rio)
- Verificar `docs/history/` para liÃ§Ãµes aprendidas relacionadas

**Recursos:**

- `docs/DOCS_INDEX.md` â† Futuro (TIER 3)
- Estrutura de diretÃ³rios na seÃ§Ã£o 5 deste documento

---

### **Step 4: ğŸ“š Knowledge Base EspecÃ­fica**

**Objetivo:** Estudar a tÃ©cnica em profundidade antes de implementar.

**AÃ§Ãµes:**

- Ler `docs/techniques/[TECHNIQUE].md` completamente
- Estudar papers/artigos de referÃªncia citados
- Analisar exemplos de cÃ³digo de implementaÃ§Ãµes validadas
- Entender mÃ©tricas de sucesso esperadas

**Fontes Principais (2025):**

- Meilisearch, AnalyticsVidhya, DataCamp, Thoughtworks
- Papers: Self-RAG, CRAG, GraphRAG, Multi-HyDE
- Benchmarks: Microsoft BenchmarkQED, Galileo AI, Epsilla

---

### **Step 5: ğŸ“˜ ImplementaÃ§Ã£o**

**Objetivo:** Codificar seguindo padrÃµes do projeto.

**AÃ§Ãµes:**

- Criar `src/rag/[module].py` seguindo templates validados
- Usar AsyncIO sempre que possÃ­vel (liÃ§Ã£o MVP #1)
- Adicionar type hints completos em todas funÃ§Ãµes
- Escrever docstrings em portuguÃªs brasileiro
- Seguir princÃ­pios SOLID e DRY
- Implementar feature flags em `.env` (ENABLE_[FEATURE]=true/false)

**PadrÃµes ObrigatÃ³rios:**

```python
# src/rag/query_decomposer.py - Exemplo de estrutura

from typing import List, Dict, Any
import asyncio
from langchain_core.language_models import BaseLLM
from src.config.settings import settings

class QueryDecomposer:
    """DecompÃµe queries BSC complexas em sub-queries independentes.
    
    Usa LLM (GPT-4o-mini) para decomposiÃ§Ã£o e Reciprocal Rank Fusion (RRF)
    para agregar resultados. Ideal para queries multi-perspectiva.
    
    MÃ©tricas esperadas:
    - Recall@10: +30-40% vs baseline
    - Precision@5: +25-35% vs baseline
    - LatÃªncia adicional: ~2s
    """
    
    def __init__(self, llm: BaseLLM):
        self.llm = llm
        self.enabled = settings.ENABLE_QUERY_DECOMPOSITION
        
    def should_decompose(self, query: str) -> bool:
        """Decide se query Ã© complexa o suficiente para decomposiÃ§Ã£o.
        
        HeurÃ­sticas:
        - Comprimento > 50 caracteres
        - ContÃ©m palavras de ligaÃ§Ã£o ("e", "tambÃ©m", "considerando")
        - Menciona mÃºltiplas perspectivas BSC
        """
        pass
    
    async def decompose(self, query: str) -> List[str]:
        """DecompÃµe query em 2-4 sub-queries independentes."""
        pass
```

---

### **Step 6: ğŸ§ª ValidaÃ§Ã£o**

**Objetivo:** Testar rigorosamente antes de considerar completo.

**AÃ§Ãµes ObrigatÃ³rias:**

- Criar `tests/test_[module].py` com **15+ testes unitÃ¡rios**
- Criar benchmark com **50 queries BSC** variadas (simples, complexas, relacionais)
- Medir mÃ©tricas: Recall@10, Precision@5, LatÃªncia P50/P95, Judge Approval
- Comparar com **baseline** (nÃ£o apenas 1 teste isolado!)
- Executar suite E2E completa (22 testes) para detectar regressÃµes
- **OBRIGATÃ“RIO**: Incluir **teste de regressÃ£o crÃ­tico** validando que funcionalidade existente NÃƒO quebrou ([[memory:9969868]] ponto 12)

**MÃ©tricas MÃ­nimas:**

```python
# tests/test_query_decomposer.py

def test_decompose_complex_bsc_query():
    """Query multi-perspectiva deve gerar 2-4 sub-queries."""
    decomposer = QueryDecomposer(llm=get_llm())
    query = "Como implementar BSC considerando perspectivas financeira, clientes e processos?"
    
    sub_queries = decomposer.decompose(query)
    
    assert len(sub_queries) >= 2
    assert len(sub_queries) <= 4
    assert "financeira" in " ".join(sub_queries).lower()
    assert "clientes" in " ".join(sub_queries).lower()

def test_should_not_decompose_simple_query():
    """Query simples NÃƒO deve ser decomposta."""
    decomposer = QueryDecomposer(llm=get_llm())
    query = "O que Ã© BSC?"
    
    assert decomposer.should_decompose(query) == False

# ... 13+ testes adicionais
```

**Benchmark Dataset:**

- Criar `tests/benchmark_queries.json` com 50 queries BSC
- Ground truth: documentos relevantes esperados
- Manual evaluation por 2 avaliadores independentes
- Validar melhoria vs baseline em mÃ©tricas objetivas

---

### **Step 7: ğŸ“Š DocumentaÃ§Ã£o**

**Objetivo:** Registrar implementaÃ§Ã£o e liÃ§Ãµes aprendidas.

**AÃ§Ãµes ObrigatÃ³rias:**

- Criar/Atualizar `docs/techniques/[TECHNIQUE].md` (300+ linhas)
- Adicionar entry em `.cursor/rules/rag-techniques-catalog.mdc` â† TIER 2
- Criar Recipe em `.cursor/rules/rag-recipes.mdc` se aplicÃ¡vel â† TIER 2
- Registrar liÃ§Ã£o aprendida em `docs/lessons/lesson-[technique]-[date].md` â† TIER 3
- Comparar ROI observado vs estimado

**Template de DocumentaÃ§Ã£o:**

```markdown
# [TECHNIQUE NAME] - DocumentaÃ§Ã£o TÃ©cnica

## ğŸ“‹ VisÃ£o Geral
[DescriÃ§Ã£o de 1 parÃ¡grafo]

## ğŸ¯ Casos de Uso BSC
[3-5 exemplos prÃ¡ticos]

## ğŸ”§ ImplementaÃ§Ã£o
[CÃ³digo completo com comentÃ¡rios]

## ğŸ“Š MÃ©tricas
| MÃ©trica | Target | Real | Status |
|---------|--------|------|--------|
| Recall@10 | 90% | 92% | âœ… |
| LatÃªncia | <7s | 6.2s | âœ… |

## ğŸ“ LiÃ§Ãµes Aprendidas
[O que funcionou, o que nÃ£o funcionou, aprendizados-chave]

## ğŸ”— ReferÃªncias
[Papers, artigos, cÃ³digo]
```

---

## ğŸ“ LIÃ‡Ã•ES DE PRODUÃ‡ÃƒO MVP

**Descobertas validadas que economizam 60+ segundos por query. Usar como base para Fase 2.**

---

### **LiÃ§Ã£o 1: âš¡ AsyncIO para Retrieval Paralelo (3.34x speedup)**

**Descoberta:** 4 agentes especialistas executando em paralelo com `asyncio.gather()`

**Impacto Medido:**

- **P50**: 70s â†’ 21s (**3.34x mais rÃ¡pido**)
- **P95**: 122s â†’ 37s (3.30x mais rÃ¡pido)
- **Mean**: 79.85s â†’ 23.92s (3.34x mais rÃ¡pido)

**CÃ³digo Validado:**

```python
import asyncio

async def parallel_retrieval(query: str):
    """Retrieval paralelo nas 4 perspectivas BSC."""
    tasks = [
        financial_agent.retrieve_async(query),
        customer_agent.retrieve_async(query),
        process_agent.retrieve_async(query),
        learning_agent.retrieve_async(query)
    ]
    results = await asyncio.gather(*tasks)
    return results

# Usage
results = asyncio.run(parallel_retrieval("Como implementar BSC?"))
```

**ROI:** 49 segundos economizados por query

**Fonte:** `docs/history/MULTILINGUAL_OPTIMIZATION_SUMMARY.md`

**Aplicar em:** Query Decomposition (retrieval paralelo de sub-queries), Self-RAG (mÃºltiplas iteraÃ§Ãµes)

---

### **LiÃ§Ã£o 2: âš¡ Cache de Embeddings (949x speedup)**

**Descoberta:** Reutilizar embeddings jÃ¡ computados em cache local

**Impacto Medido:**

- **Sem cache**: 1.17s por embedding
- **Com cache**: 0.00123s por embedding
- **Speedup**: **949x mais rÃ¡pido**
- **Dataset**: 7.965 chunks indexados

**ImplementaÃ§Ã£o:**

```python
# src/rag/embeddings.py
from functools import lru_cache
import hashlib

class CachedEmbeddings:
    def __init__(self, base_embeddings, cache_dir="data/contextual_cache"):
        self.base_embeddings = base_embeddings
        self.cache_dir = cache_dir
        
    def embed_query(self, text: str) -> List[float]:
        """Embed query com cache."""
        cache_key = hashlib.md5(text.encode()).hexdigest()
        cache_file = f"{self.cache_dir}/{cache_key}.npy"
        
        if os.path.exists(cache_file):
            return np.load(cache_file).tolist()  # 949x mais rÃ¡pido!
        
        embedding = self.base_embeddings.embed_query(text)
        np.save(cache_file, np.array(embedding))
        return embedding
```

**ROI:** 1.17s economizados por embedding (dataset completo: ~9.300s economizados)

**Fonte:** `docs/history/MULTILINGUAL_OPTIMIZATION_SUMMARY.md`

**Aplicar em:** Todas tÃ©cnicas RAG que usam embeddings

---

### **LiÃ§Ã£o 3: âš¡ Busca MultilÃ­ngue com RRF (+106% recall)**

**Descoberta:** Hybrid search PT + EN com Reciprocal Rank Fusion (RRF)

**Impacto Medido:**

- **Recall em queries PT**: +106% vs busca PT-only
- **Cobertura**: 100% da literatura BSC (originalmente em inglÃªs)
- **Precision**: Mantida (re-ranking Cohere elimina falsos positivos)

**CÃ³digo Validado:**

```python
def reciprocal_rank_fusion(
    results_list: List[List[Document]], 
    k: int = 60
) -> List[Document]:
    """Combina mÃºltiplas listas de resultados usando RRF.
    
    Formula: score(doc) = Î£ 1 / (k + rank(doc))
    """
    doc_scores = defaultdict(float)
    
    for results in results_list:
        for rank, doc in enumerate(results):
            doc_id = doc.metadata.get("id", doc.page_content[:50])
            doc_scores[doc_id] += 1.0 / (k + rank + 1)
    
    # Sort by score (highest first)
    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
    return [doc for doc_id, score in sorted_docs]

# Usage
query_pt = "Como implementar BSC?"
query_en = "How to implement BSC?"

results_pt = retriever.retrieve(query_pt, k=50)
results_en = retriever.retrieve(query_en, k=50)

fused_results = reciprocal_rank_fusion([results_pt, results_en])
```

**ROI:** Cobertura completa da literatura BSC (sem traduÃ§Ã£o do dataset)

**Fonte:** `docs/history/MULTILINGUAL_OPTIMIZATION_SUMMARY.md`

**Aplicar em:** Query Decomposition (RRF de sub-queries), CRAG (combinar retrieval original + corrigido)

---

### **LiÃ§Ã£o 4: âš¡ Contextual Retrieval (Anthropic 2024) - +35% recall**

**Descoberta:** Adicionar contexto do documento/capÃ­tulo nos chunks antes de gerar embeddings

**Impacto Medido:**

- **Recall**: +35% em benchmarks Anthropic
- **Qualidade dos chunks**: Chunks auto-explicativos (nÃ£o precisam do documento pai para fazer sentido)

**Exemplo:**

```
CHUNK ORIGINAL (sem contexto):
"As quatro perspectivas sÃ£o balanceadas para criar uma visÃ£o holÃ­stica."

CHUNK COM CONTEXTO (contextual):
"Este trecho do livro 'The Balanced Scorecard' de Kaplan & Norton (1996), 
CapÃ­tulo 2 sobre Framework BSC, explica: As quatro perspectivas sÃ£o 
balanceadas para criar uma visÃ£o holÃ­stica."
```

**ImplementaÃ§Ã£o:**

```python
# src/prompts/contextual_chunk_prompt.py

CONTEXTUAL_CHUNK_PROMPT = """VocÃª receberÃ¡ um chunk de texto e o contexto do documento completo.

Adicione contexto breve (1-2 sentenÃ§as) ANTES do chunk para tornÃ¡-lo auto-explicativo.

Documento: {document_metadata}
Chunk original: {chunk_content}

Chunk contextualizado:"""
```

**ROI:** Chunks 35% mais informativos (melhor retrieval)

**Fonte:** `docs/GPT5_CONTEXTUAL_RETRIEVAL.md`, `docs/history/IMPLEMENTATION_GPT5_CONTEXTUAL.md`

**Aplicar em:** Todas tÃ©cnicas RAG (base para qualidade do retrieval)

---

### **LiÃ§Ã£o 5: âš¡ Cohere Re-ranking (75% precision @ top-5)**

**Descoberta:** Re-rank top-50 hybrid search para top-10 com Cohere Rerank API

**Impacto Medido:**

- **Precision@5**: 75% (3 em 4 docs sÃ£o altamente relevantes)
- **Precision@10**: ~70%
- **EstratÃ©gia**: Retrieve 50 (recall alto) â†’ Re-rank to 10 (precision alta)

**CÃ³digo Validado:**

```python
# src/rag/reranker.py

from cohere import Client

class CohereReranker:
    def __init__(self, model="rerank-multilingual-v3.0", top_n=10):
        self.client = Client(api_key=settings.COHERE_API_KEY)
        self.model = model
        self.top_n = top_n
        
    def rerank(
        self, 
        query: str, 
        documents: List[Document], 
        top_n: int = None
    ) -> List[Document]:
        """Re-rank documentos por relevÃ¢ncia usando Cohere."""
        top_n = top_n or self.top_n
        
        # Cohere API call
        results = self.client.rerank(
            model=self.model,
            query=query,
            documents=[doc.page_content for doc in documents],
            top_n=top_n
        )
        
        # Sort by relevance score
        reranked_docs = [
            documents[result.index] 
            for result in results.results
        ]
        
        return reranked_docs

# Usage
retriever = BSCRetriever(search_type="hybrid", k=50)
reranker = CohereReranker(top_n=10)

query = "Como implementar BSC?"
docs = retriever.retrieve(query, k=50)  # Recall alto
reranked_docs = reranker.rerank(query, docs, top_n=10)  # Precision alta
```

**ROI:** Documentos 75% mais relevantes no top-5

**Fonte:** MVP implementation, `docs/TUTORIAL.md`

**Aplicar em:** Adaptive Re-ranking (diversity + metadata boost), CRAG (avaliar qualidade do retrieval)

---

**TOTAL ECONOMIA MVP:** ~60 segundos por query + cache hits ilimitados

---

## ğŸ“ LIÃ‡Ã•ES APRENDIDAS FASE 2A

**Descobertas validadas das 3 tÃ©cnicas implementadas (Query Decomposition, Adaptive Re-ranking, Router). Total: 10 dias â†’ 3 tÃ©cnicas prontas.**

---

### **LiÃ§Ã£o 6: ğŸ¯ Query Decomposition - GPT-4o-mini Ã© Suficiente**

**Descoberta:** GPT-4o-mini ($0.0001/1K tokens) tem qualidade equivalente ao GPT-4o ($0.01/1K tokens) para decomposiÃ§Ã£o de queries

**Impacto Medido:**
- **Custo**: 100x mais barato ($9.90/dia economizados em 1000 queries)
- **LatÃªncia**: Similar (~1.2s vs ~2s, +40% mais rÃ¡pido)
- **Qualidade**: Equivalente (sub-queries igualmente vÃ¡lidas)

**Top 3 Insights:**
1. âœ… **GPT-4o-mini para tarefas simples** (decomposiÃ§Ã£o, classificaÃ§Ã£o, extraÃ§Ã£o)
2. âœ… **Contexto nas sub-queries** (manter informaÃ§Ã£o da query original: "BSC para empresas de tecnologia" â†’ sub-query inclui "tecnologia")
3. âŒ **Regex sem word boundaries** causa falsos positivos (-8% accuracy)

**ROI:** $9.90/dia economizados + latÃªncia -40%

**Fonte:** `docs/lessons/lesson-query-decomposition-2025-10-14.md` (545 linhas)

**Aplicar em:** Self-RAG (reflection tokens), CRAG (query reformulation), todas tarefas LLM simples

---

### **LiÃ§Ã£o 7: ğŸ¨ Adaptive Re-ranking - TDD Acelera ImplementaÃ§Ã£o**

**Descoberta:** Test-Driven Development (TDD) reduziu bugs de 15 esperados para apenas 1 (float comparison)

**Impacto Medido:**
- **Bugs evitados**: 14 de 15 previstos (-93%)
- **Coverage**: 100% (38 testes, vs 85% target)
- **Tempo debug**: ~30 min (vs 2-3h estimado)

**Top 3 Insights:**
1. âœ… **TDD para cÃ³digo matemÃ¡tico** (similarity, MMR, normalize embeddings)
2. âœ… **np.allclose() para float comparisons** (evita erros de precisÃ£o)
3. âœ… **Mocking robusto** de embeddings (controle total de similarity scores)

**ROI:** 2-3h economizadas em debugging + 0 regressÃµes

**Fonte:** `docs/lessons/lesson-adaptive-reranking-2025-10-14.md` (626 linhas)

**Aplicar em:** Self-RAG (similarity de reflection tokens), Graph RAG (embeddings de entidades)

---

### **LiÃ§Ã£o 8: âš¡ Router Inteligente - ReutilizaÃ§Ã£o = AceleraÃ§Ã£o Exponencial**

**Descoberta:** Reutilizar cÃ³digo existente acelerou implementaÃ§Ã£o em **10x** (6h vs 5-7 dias estimado)

**Impacto Medido:**
- **Tempo**: 6h real vs 5-7 dias estimado (**10x mais rÃ¡pido**)
- **CÃ³digo reutilizado**: 70% (heurÃ­sticas de Query Decomp, strategies template)
- **Classifier accuracy**: 92% (+7pp vs 85% target)

**Top 3 Insights:**
1. âœ… **HÃ­brido heurÃ­stica+LLM** (90% queries resolvidas por regex rÃ¡pido, 10% por LLM)
2. âœ… **ThreadPoolExecutor** para paralelizar estratÃ©gias quando tipo incerto
3. âœ… **Logging estruturado** (JSONL) para analytics de routing decisions

**Curva de AceleraÃ§Ã£o Fase 2A:**
- 1Âª tÃ©cnica (Query Decomp): 4 dias (estabelece patterns)
- 2Âª tÃ©cnica (Adaptive Re-rank): 2 dias (reusa patterns)
- 3Âª tÃ©cnica (Router): **6 HORAS!** (reusa tudo)

**ROI:** 10x aceleraÃ§Ã£o + 92% accuracy + 70% reuso

**Fonte:** `docs/lessons/lesson-router-2025-10-14.md` (786 linhas)

**Aplicar em:** Self-RAG (decidir quando iterar), CRAG (decidir quando corrigir), meta-router para escolher tÃ©cnica

---

### **LiÃ§Ã£o 9: ğŸš« Top 5 AntipadrÃµes RAG a Evitar**

**Descoberta:** 5 armadilhas comuns identificadas que custam 2-8h debugging cada

**AntipadrÃµes CrÃ­ticos:**
1. âŒ **GPT-4o para tarefas simples** â†’ Usar GPT-4o-mini (100x mais barato)
2. âŒ **Regex sem word boundaries** â†’ Usar `\be\b` ao invÃ©s de `"e" in text` (-8% accuracy)
3. âŒ **Sub-queries sem contexto** â†’ Manter informaÃ§Ã£o da query original (precision +15%)
4. âŒ **Threshold muito alto** â†’ Validar coverage antes de aumentar (5% queries ficam sem decomposiÃ§Ã£o)
5. âŒ **Testes depois** â†’ TDD previne 93% bugs em cÃ³digo matemÃ¡tico

**ROI por AntipadrÃ£o Evitado:**
- AntipadrÃ£o #1: $9.90/dia economizados
- AntipadrÃ£o #2: +8% accuracy (30-40 queries/dia corrigidas)
- AntipadrÃ£o #3: +15% precision (45-60 queries/dia melhoradas)
- AntipadrÃ£o #4: +5% coverage (15-20 queries/dia nÃ£o ignoradas)
- AntipadrÃ£o #5: 2-3h economizadas em debugging

**Fonte:** `docs/lessons/antipadrÃµes-rag.md` (903 linhas, 32 antipadrÃµes catalogados)

**Checklist:** Revisar antes de implementar qualquer tÃ©cnica RAG

---

**TOTAL ECONOMIA FASE 2A:** 
- ğŸ’° $9.90/dia (custo LLM)
- âš¡ 10x aceleraÃ§Ã£o (reutilizaÃ§Ã£o)
- ğŸ› 2-3h/tÃ©cnica (TDD + antipadrÃµes)
- ğŸ“Š +15% precision, +8% accuracy, 92% routing

---

## ğŸ—ºï¸ MAPA DE TÃ‰CNICAS RAG

**Guia de decisÃ£o rÃ¡pida: Qual tÃ©cnica implementar baseado em necessidade?**

| Necessidade | TÃ©cnica RAG | Quando Usar | Complexidade | ROI | Prioridade |
|-------------|-------------|-------------|--------------|-----|------------|
| **Queries complexas multi-parte** | Query Decomposition | 2+ perguntas, palavras ligaÃ§Ã£o ("e", "tambÃ©m") | â­â­ Baixa | â­â­â­â­â­ | ğŸ”¥ **ALTA** |
| **Melhorar diversidade docs** | Adaptive Re-ranking | Evitar docs repetidos, mÃºltiplas fontes | â­â­ Baixa | â­â­â­â­ | **ALTA** |
| **Otimizar latÃªncia por tipo** | Router Inteligente | Workflows complexos, queries variadas | â­â­â­â­ MÃ©dia-Alta | â­â­â­â­â­ | **ALTA** |
| **Reduzir alucinaÃ§Ãµes** | Self-RAG | Alta acurÃ¡cia factual crÃ­tica | â­â­â­â­ MÃ©dia-Alta | â­â­â­â­ | **MÃ‰DIA** |
| **Retrieval frequentemente falha** | CRAG | Queries ambÃ­guas, dataset incompleto | â­â­â­ MÃ©dia | â­â­â­â­ | **MÃ‰DIA** |
| **Baixo recall (<70%)** | HyDE / Multi-HyDE | Queries abstratas, gap semÃ¢ntico | â­â­â­ Baixa-MÃ©dia | â­â­â­ | **BAIXA** |
| **RelaÃ§Ãµes complexas multi-hop** | Graph RAG | Dataset com entidades e relaÃ§Ãµes | â­â­â­â­â­ Muito Alta | â­â­â­â­â­* | **BAIXA*** |
| **Respostas precisam mais contexto** | Iterative Retrieval | Refinamento progressivo | â­â­â­ MÃ©dia | â­â­â­ | **BAIXA** |

**Notas:**

- **ROI**: Return on Investment (benefÃ­cio esperado vs esforÃ§o)
- **Prioridade**: EspecÃ­fica para nosso use case (literatura conceitual BSC)
- ***Graph RAG**: Baixa prioridade AGORA (dataset atual inadequado), ALTA se conseguirmos BSCs operacionais empresariais

**RecomendaÃ§Ã£o de ImplementaÃ§Ã£o (Fase 2):**

1. **Fase 2A - Quick Wins** (2-3 semanas): Query Decomposition â†’ Adaptive Re-ranking â†’ Router Inteligente
2. **Fase 2B - Advanced** (3-4 semanas): Self-RAG â†’ CRAG
3. **Fase 2C - Condicional**: Avaliar HyDE (SE recall < 70%) â†’ Avaliar Graph RAG (SE dataset mudar)

---

## ğŸ¯ GUIA POR CENÃRIO RAG

**4 cenÃ¡rios prÃ¡ticos mais comuns com soluÃ§Ã£o mapeada:**

---

### **CenÃ¡rio 1: Query BSC complexa multi-perspectiva** ğŸ”¥ **MAIS COMUM**

**Problema:**

```
Query: "Como implementar BSC considerando as 4 perspectivas e suas interconexÃµes?"
Resultado atual: Resposta parcial, foca em 1-2 perspectivas, nÃ£o aborda interconexÃµes
```

**Causa:** Query complexa tem mÃºltiplas partes que retrieval simples nÃ£o captura bem.

**SoluÃ§Ã£o:** Query Decomposition (TECH-001)

**Workflow:**

1. ğŸ§  **Sequential Thinking** â†’ Identificar sub-queries implÃ­citas
2. ğŸ¯ **Discovery** â†’ Consultar Mapa de TÃ©cnicas â†’ Query Decomposition (ROI â­â­â­â­â­)
3. ğŸ—ºï¸ **Navigation** â†’ Ler `docs/techniques/QUERY_DECOMPOSITION.md` â† Criar na Fase 2A.1
4. ğŸ“š **Knowledge Base** â†’ Estudar papers (Galileo AI, Epsilla, Microsoft BenchmarkQED)
5. ğŸ“˜ **ImplementaÃ§Ã£o** â†’ Criar `src/rag/query_decomposer.py`
6. ğŸ§ª **ValidaÃ§Ã£o** â†’ Testar com 15+ queries complexas BSC
7. ğŸ“Š **DocumentaÃ§Ã£o** â†’ Registrar liÃ§Ã£o aprendida

**MÃ©tricas Esperadas:**

- Recall@10: +30-40%
- Answer quality: +30-50%
- LatÃªncia adicional: ~2s (aceitÃ¡vel)

---

### **CenÃ¡rio 2: AlucinaÃ§Ãµes detectadas (>10% taxa)**

**Problema:**

```
Query: "Quais sÃ£o os KPIs financeiros recomendados por Kaplan & Norton?"
Resultado: Resposta menciona KPIs que NÃƒO estÃ£o nos livros (inventados pelo LLM)
```

**Causa:** LLM gera informaÃ§Ã£o sem validar se estÃ¡ nos documentos recuperados.

**SoluÃ§Ã£o:** Self-RAG (TECH-004)

**Workflow:**

1. ğŸ§  **Sequential Thinking** â†’ Avaliar necessidade (taxa alucinaÃ§Ã£o > 10%? â†’ SIM)
2. ğŸ¯ **Discovery** â†’ Consultar Mapa de TÃ©cnicas â†’ Self-RAG (ROI â­â­â­â­, -40-50% alucinaÃ§Ãµes)
3. ğŸ—ºï¸ **Navigation** â†’ Ler `docs/techniques/SELF_RAG.md` â† Criar na Fase 2B.1
4. ğŸ“š **Knowledge Base** â†’ Estudar paper original "Self-RAG: Learning to Retrieve, Generate, and Critique" (2023), tutorial DataCamp (Sep 2025)
5. ğŸ“˜ **ImplementaÃ§Ã£o** â†’ Criar `src/rag/self_rag.py` com reflection tokens
6. ğŸ§ª **ValidaÃ§Ã£o** â†’ Fact-checking manual em 50 queries, medir hallucination rate
7. ğŸ“Š **DocumentaÃ§Ã£o** â†’ Comparar taxa antes/depois, documentar trade-offs (latÃªncia +20-30%)

**MÃ©tricas Esperadas:**

- Hallucination rate: 15% â†’ <5% (-66%)
- Factual accuracy: +95%
- Trade-off: LatÃªncia +20-30%, Custo +30-40%

---

### **CenÃ¡rio 3: LatÃªncia alta (>60s P50)** âš¡

**Problema:**

```
Query: "O que Ã© BSC?"
Resultado: Resposta simples mas demora 70s (retrieval de 4 agentes + LLM generation)
```

**Causa:** Query simples nÃ£o precisa de retrieval complexo multi-agente.

**SoluÃ§Ã£o:** Router Inteligente (TECH-003) + AsyncIO (LiÃ§Ã£o MVP #1)

**Workflow:**

1. ğŸ§  **Sequential Thinking** â†’ Identificar gargalos (retrieval desnecessÃ¡rio em queries simples)
2. ğŸ¯ **Discovery** â†’ Consultar Mapa de TÃ©cnicas â†’ Router Inteligente (ROI â­â­â­â­â­)
3. ğŸ—ºï¸ **Navigation** â†’ Consultar `.cursor/rules/rag-recipes.mdc` â†’ RECIPE-002 (AsyncIO) â† Criar TIER 2
4. ğŸ“š **Knowledge Base** â†’ Ler `docs/history/MULTILINGUAL_OPTIMIZATION_SUMMARY.md` (AsyncIO 3.34x)
5. ğŸ“˜ **ImplementaÃ§Ã£o** â†’ Criar `src/rag/query_router.py` + strategies
6. ğŸ§ª **ValidaÃ§Ã£o** â†’ Benchmark latÃªncia antes/depois em 50 queries variadas
7. ğŸ“Š **DocumentaÃ§Ã£o** â†’ Medir latÃªncia P50/P95/Mean, comparar com baseline

**MÃ©tricas Esperadas:**

- Queries simples: 70s â†’ <10s (-85%)
- LatÃªncia mÃ©dia: -20% (routing compensado por estratÃ©gias otimizadas)
- Classifier accuracy: >85%

---

### **CenÃ¡rio 4: Retrieval de baixa qualidade (<70% precision)**

**Problema:**

```
Query: "diferenÃ§as BSC manufatura vs serviÃ§os"
Resultado: Top-5 docs nÃ£o mencionam manufatura nem serviÃ§os (retrieval falhou)
```

**Causa:** Query ambÃ­gua ou vocabulÃ¡rio diferente entre query e documentos.

**SoluÃ§Ã£o:** CRAG - Corrective RAG (TECH-005)

**Workflow:**

1. ğŸ§  **Sequential Thinking** â†’ Avaliar necessidade (precision < 70%? â†’ SIM)
2. ğŸ¯ **Discovery** â†’ Consultar Mapa de TÃ©cnicas â†’ CRAG (ROI â­â­â­â­)
3. ğŸ—ºï¸ **Navigation** â†’ Ler `docs/techniques/CRAG.md` â† Criar na Fase 2B.2
4. ğŸ“š **Knowledge Base** â†’ Estudar tutorial Meilisearch (Sep 2025), Thoughtworks (Apr 2025)
5. ğŸ“˜ **ImplementaÃ§Ã£o** â†’ Criar `src/rag/corrective_rag.py` com query reformulation
6. ğŸ§ª **ValidaÃ§Ã£o** â†’ Medir precision@5 antes/depois, contar correÃ§Ãµes trigadas
7. ğŸ“Š **DocumentaÃ§Ã£o** â†’ Documentar quando correÃ§Ã£o Ã© Ãºtil vs overhead desnecessÃ¡rio

**MÃ©tricas Esperadas:**

- Retrieval quality: 0.65 â†’ 0.80 (+23%)
- Correction triggered: 10-15% queries
- Accuracy em queries corrigidas: +15%

---

## ğŸ“š LOCALIZAÃ‡ÃƒO DA DOCUMENTAÃ‡ÃƒO

**NavegaÃ§Ã£o rÃ¡pida para toda documentaÃ§Ã£o do projeto BSC RAG:**

```
agente-bsc-rag/
â”œâ”€â”€ .cursor/rules/
â”‚   â”œâ”€â”€ rag-bsc-core.mdc              â† âœ… VOCÃŠ ESTÃ AQUI (router central)
â”‚   â”œâ”€â”€ rag-techniques-catalog.mdc    â† âœ… TIER 2 COMPLETO (catÃ¡logo 5 tÃ©cnicas)
â”‚   â”œâ”€â”€ rag-recipes.mdc               â† âœ… TIER 2 COMPLETO (3 recipes validados)
â”‚   â””â”€â”€ rag-lessons-learned.mdc       â† ğŸ”œ TIER 3 (liÃ§Ãµes + antipadrÃµes)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ DOCS_INDEX.md                 â† ğŸ”œ TIER 3 (Ã­ndice navegÃ¡vel completo)
â”‚   â”‚
â”‚   â”œâ”€â”€ techniques/                   â† âœ… TÃ©cnicas RAG detalhadas (Fase 2A completa)
â”‚   â”‚   â”œâ”€â”€ QUERY_DECOMPOSITION.md   â† âœ… Fase 2A.1 (400+ linhas)
â”‚   â”‚   â”œâ”€â”€ ADAPTIVE_RERANKING.md    â† âœ… Fase 2A.2 (500+ linhas)
â”‚   â”‚   â”œâ”€â”€ ROUTER.md                â† âœ… Fase 2A.3 (650+ linhas)
â”‚   â”‚   â”œâ”€â”€ SELF_RAG.md              â† ğŸ”œ Fase 2B.1 (Self-RAG)
â”‚   â”‚   â””â”€â”€ CRAG.md                  â† ğŸ”œ Fase 2B.2 (CRAG)
â”‚   â”‚
â”‚   â”œâ”€â”€ patterns/                     â† âœ… ConfiguraÃ§Ãµes validadas (criado TIER 1)
â”‚   â”‚   â”œâ”€â”€ HYBRID_SEARCH.md         â† ğŸ”œ TIER 2 (Pattern do MVP)
â”‚   â”‚   â”œâ”€â”€ COHERE_RERANK.md         â† ğŸ”œ TIER 2 (Pattern do MVP)
â”‚   â”‚   â”œâ”€â”€ ASYNCIO_PARALLEL.md      â† ğŸ”œ TIER 2 (Pattern do MVP)
â”‚   â”‚   â””â”€â”€ EMBEDDING_CACHE.md       â† ğŸ”œ TIER 2 (Pattern do MVP)
â”‚   â”‚
â”‚   â”œâ”€â”€ lessons/                      â† âœ… LiÃ§Ãµes aprendidas (criado TIER 1)
â”‚   â”‚   â”œâ”€â”€ lesson-query-decomposition-2025-10-XX.md  â† ğŸ”œ PÃ³s Fase 2A.1
â”‚   â”‚   â”œâ”€â”€ lesson-adaptive-reranking-2025-10-XX.md   â† ğŸ”œ PÃ³s Fase 2A.2
â”‚   â”‚   â””â”€â”€ lesson-router-2025-10-XX.md               â† ğŸ”œ PÃ³s Fase 2A.3
â”‚   â”‚
â”‚   â”œâ”€â”€ history/                      â† âœ… Progresso histÃ³rico (jÃ¡ existe)
â”‚   â”‚   â”œâ”€â”€ MVP_100_COMPLETO.md      â† âœ… MVP completo (14/10/2025)
â”‚   â”‚   â”œâ”€â”€ MULTILINGUAL_OPTIMIZATION_SUMMARY.md  â† âœ… OtimizaÃ§Ãµes massivas
â”‚   â”‚   â””â”€â”€ E2E_TESTS_IMPLEMENTATION_SUMMARY.md   â† âœ… E2E validado
â”‚   â”‚
â”‚   â””â”€â”€ [outros docs existentes]     â† âœ… TUTORIAL.md, ARCHITECTURE.md, etc
â”‚
â””â”€â”€ src/rag/                          â† âœ… ImplementaÃ§Ã£o (Fase 2A completa)
    â”œâ”€â”€ retriever.py                 â† âœ… BSCRetriever (MVP)
    â”œâ”€â”€ reranker.py                  â† âœ… CohereReranker (MVP)
    â”œâ”€â”€ hybrid_search.py             â† âœ… Hybrid search (MVP)
    â”œâ”€â”€ embeddings.py                â† âœ… Cached embeddings (MVP)
    â”‚
    â”œâ”€â”€ query_decomposer.py          â† âœ… Fase 2A.1 (270 linhas, 91% coverage)
    â”œâ”€â”€ query_router.py              â† âœ… Fase 2A.3 (280 linhas, 92% accuracy)
    â”œâ”€â”€ strategies.py                â† âœ… Fase 2A.3 (4 estratÃ©gias)
    â”œâ”€â”€ self_rag.py                  â† ğŸ”œ Fase 2B.1 (a criar)
    â””â”€â”€ corrective_rag.py            â† ğŸ”œ Fase 2B.2 (a criar)
```

**Legenda:**

- âœ… **Criado/Existe** - DisponÃ­vel agora
- ğŸ”œ **Futuro** - SerÃ¡ criado em fase especÃ­fica

---

## ğŸ“ CHANGELOG

### v1.0 - 2025-10-14 (VersÃ£o Inicial - TIER 1 Completo)

**Criado:**

- âœ… Router central always-applied
- âœ… Workflow obrigatÃ³rio de 7 steps
- âœ… Top 5 liÃ§Ãµes MVP com cÃ³digo e mÃ©tricas
- âœ… Mapa de 8 tÃ©cnicas RAG comparativas
- âœ… 4 cenÃ¡rios prÃ¡ticos mapeados
- âœ… Estrutura de documentaÃ§Ã£o organizada
- âœ… Pastas `docs/techniques/`, `docs/patterns/`, `docs/lessons/` criadas

**ROI Esperado TIER 1:**

- 5-10 min economizados por decisÃ£o tÃ©cnica (qual tÃ©cnica usar, qual doc consultar)
- Contexto preservado entre sessÃµes (6-8 semanas Fase 2)
- Processo consistente para todas as 8 tÃ©cnicas RAG

**PrÃ³ximo:**

- âœ… **Fase 2A (3 tÃ©cnicas)** - Query Decomposition, Adaptive Re-ranking, Router (COMPLETO)
- âœ… **TIER 2** - RAG Techniques Catalog + Recipes (COMPLETO)

---

### v1.1 - 2025-10-14 (TIER 2 Completo)

**Criado:**

- âœ… `.cursor/rules/rag-techniques-catalog.mdc` (1.200+ linhas)
  - Catalogadas 5 tÃ©cnicas (TECH-001 a TECH-005)
  - 3 tÃ©cnicas implementadas (Fase 2A) + 2 planejadas (Fase 2B)
  - Taxonomia por Categoria, Complexidade, ROI
  - Quick Reference Table e Ã­ndice navegÃ¡vel
  - Cross-references para docs/techniques/
  
- âœ… `.cursor/rules/rag-recipes.mdc` (800+ linhas)
  - 3 recipes validados (RECIPE-001 a RECIPE-003)
  - Hybrid Search + Re-ranking (MVP)
  - AsyncIO Parallel Retrieval (3.34x speedup)
  - Embedding Cache (949x speedup)
  - CÃ³digo essencial + troubleshooting

**ROI Validado TIER 2:**

- 15-20 min economizados por uso (discovery + implementaÃ§Ã£o)
- Conhecimento consolidado das 3 tÃ©cnicas Fase 2A
- PreparaÃ§Ã£o eficiente para Fase 2B (Self-RAG, CRAG)

**PrÃ³ximo:**

- âœ… **Benchmark Fase 2A** - Validar mÃ©tricas (COMPLETO)
- âœ… **TIER 3** - Docs Index + LiÃ§Ãµes Aprendidas (COMPLETO)
- ğŸ”œ **Fase 2B** - Self-RAG + CRAG

---

### v1.2 - 2025-10-15 (TIER 3 + Benchmark Completo)

**Criado:**

- âœ… `docs/DOCS_INDEX.md` (Ã­ndice navegÃ¡vel completo)
  - Tags A-Z para busca rÃ¡pida
  - Documentos por categoria (Techniques, Patterns, History)
  - Quick Search Matrix
  - Cross-references entre todos documentos
  
- âœ… `docs/lessons/` (liÃ§Ãµes aprendidas Fase 2A)
  - `lesson-query-decomposition-2025-10-14.md` (545 linhas)
  - `lesson-adaptive-reranking-2025-10-14.md` (550+ linhas)
  - `lesson-router-2025-10-14.md` (600+ linhas)
  - `antipadrÃµes-rag.md` (antipadrÃµes identificados)
  
- âœ… Benchmark Fase 2A Completo (50 queries Ã— 2 sistemas)
  - `tests/benchmark_fase2a/run_benchmark.py` (pipeline completo)
  - `tests/benchmark_fase2a/evaluate_existing_results.py` (avaliaÃ§Ã£o RAGAS)
  - `tests/benchmark_fase2a/analyze_results.py` (relatÃ³rio + visualizaÃ§Ãµes)
  - 3 grÃ¡ficos gerados (latency_boxplot, by_category, ragas_metrics)
  - RelatÃ³rio executivo markdown

**Resultados Benchmark Validados:**

- âœ… **LatÃªncia MÃ©dia**: +3.1% mais rÃ¡pido (128.7s â†’ 124.7s)
- âœ… **Answer Relevancy (RAGAS)**: +2.1% (0.889 â†’ 0.907)
- âœ… **Queries Simples**: +10.6% mais rÃ¡pido (64.6s â†’ 57.7s)
- âœ… **Queries Conceituais**: +8.5% mais rÃ¡pido (95.8s â†’ 87.7s)
- âœ… **Multi-Perspectiva**: +4.0% mais rÃ¡pido
- âš ï¸ **Faithfulness**: -0.6% (variaÃ§Ã£o mÃ­nima aceitÃ¡vel)

**ROI Validado TIER 3:**

- 20-30 min economizados por consulta de documentaÃ§Ã£o (Ã­ndice navegÃ¡vel)
- LiÃ§Ãµes aprendidas consolidadas (evitar repetir erros)
- AntipadrÃµes documentados (guia de boas prÃ¡ticas)
- MÃ©tricas Fase 2A validadas empiricamente (nÃ£o mais estimativas)

**PrÃ³ximo:**

- ğŸ”œ **Fase 2B** - Self-RAG + CRAG
- ğŸ”œ **ProduÃ§Ã£o** - Deploy com feature flags

---

### v1.3 - 2025-10-15 (LiÃ§Ãµes Fase 2A Integradas)

**Atualizado:**

- âœ… **SeÃ§Ã£o "LiÃ§Ãµes Aprendidas Fase 2A"** adicionada (108 linhas)
  - LiÃ§Ã£o 6: Query Decomposition - GPT-4o-mini Ã© Suficiente ($9.90/dia economizados)
  - LiÃ§Ã£o 7: Adaptive Re-ranking - TDD Acelera ImplementaÃ§Ã£o (-93% bugs)
  - LiÃ§Ã£o 8: Router Inteligente - ReutilizaÃ§Ã£o = AceleraÃ§Ã£o Exponencial (10x speedup)
  - LiÃ§Ã£o 9: Top 5 AntipadrÃµes RAG a Evitar (2-8h economizadas/antipadrÃ£o)
  - TOTAL ECONOMIA FASE 2A documentada ($9.90/dia + 10x aceleraÃ§Ã£o + 2-3h/tÃ©cnica)

**ROI Validado v1.3:**

- **IntegraÃ§Ã£o knowledge base**: LiÃ§Ãµes MVP + Fase 2A agora juntas no router central
- **DecisÃµes mais rÃ¡pidas**: Consultar liÃ§Ãµes inline (nÃ£o precisa abrir arquivo separado)
- **Contexto preservado**: Agente tem consciÃªncia de 9 liÃ§Ãµes validadas sempre aplicadas
- **AntipadrÃµes visÃ­veis**: Top 5 armadilhas catalogadas para consulta rÃ¡pida

**PrÃ³ximo:**

- ğŸ”œ **Integrar liÃ§Ã£o test-debugging** em derived-cursor-rules.mdc
- ğŸ”œ **Fase 2B** - Self-RAG + CRAG (aplicar 9 liÃ§Ãµes aprendidas)

---

**Ãšltima AtualizaÃ§Ã£o:** 2025-10-15 (v1.3)
**Status:** âœ… TIER 1+2+3 COMPLETO | âœ… LIÃ‡Ã•ES FASE 2A INTEGRADAS | ğŸ¯ Pronto para Fase 2B
