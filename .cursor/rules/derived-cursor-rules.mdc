---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
version: "2.0"
last_updated: "2025-10-19"
---

## HEADERS

## TECH STACK

### Test Methodology (Validado Out/2025 - ROI: 32-60 min economizados por implementaÃ§Ã£o)

**ANTES de escrever QUALQUER teste (unitÃ¡rio ou E2E), aplicar [[memory:9969868]] - CHECKLIST OBRIGATÃ“RIO de 15 pontos**:

**PONTOS 1-8: Testes UnitÃ¡rios** (validado FASE 2.5)

1. **Ler assinatura completa**: `grep "def method_name" src/file.py -A 10` (NUNCA assumir)
2. **Verificar tipo retorno**: Objeto Pydantic ou built-in? Campos obrigatÃ³rios?
3. **Contar parÃ¢metros**: Quantos params (nÃ£o contar `self`)?
4. **ValidaÃ§Ãµes prÃ©-flight**: ValidaÃ§Ãµes no cÃ³digo + Pydantic (min_length, Literal, validators)
5. **Entender decorators**: @retry com `reraise=True` relanÃ§a exceÃ§Ã£o original (NÃƒO RetryError)
6. **Fixtures Pydantic**: NUNCA passar `None` para `default_factory`. Incluir campos obrigatÃ³rios.
7. **Dados vÃ¡lidos em mocks**: Usar MARGEM DE SEGURANÃ‡A (ex: min_length=20 â†’ usar 50+ chars)
8. **Verificar mÃ©todo correto**: Confirmar nome exato via grep (ex: `invoke()` nÃ£o `process_query()`)

**PONTOS 9-12: E2E Workflow Tests** (validado FASE 2.6) - Ver seÃ§Ã£o "E2E Workflow Tests" below for details

**Debugging pytest (Validado [[memory:9969628]] e [[memory:10012853]])**:
*   **SEMPRE** usar `--tb=long` (traceback completo, NUNCA --tb=short)
*   **NUNCA** usar filtros (`Select-Object`, `Select-String`) - oculta informaÃ§Ã£o crÃ­tica
*   Comando correto: `pytest tests/test_file.py -v --tb=long 2>&1`

**pytest em paralelo** (performance):
*   `-n <num_workers>`: Workers paralelos
*   `-v`: Verbose output
*   `--tb=long`: Traceback completo
*   `--dist=loadscope`: Distribui por fixture scope (mais seguro)
*   `--dist=loadfile`: Distribui por arquivo (fixtures function-scoped)

**LiÃ§Ãµes Detalhadas**:
- `docs/lessons/lesson-test-debugging-methodology-2025-10-15.md` (FASE 2.4, 5 erros)
- `docs/lessons/lesson-diagnostic-agent-test-methodology-2025-10-16.md` (FASE 2.5, 7 erros, 1.100+ linhas)
- `docs/lessons/lesson-onboarding-state-e2e-tests-2025-10-16.md` (FASE 2.6, 4 problemas E2E, 11.900+ linhas)
- `docs/lessons/lesson-swot-testing-methodology-2025-10-19.md` (FASE 3.1, Implementation-First Testing, APIs desconhecidas, 700+ linhas, 30-40 min economizados)
- `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
- `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys applied + PONTO 15 fixtures Pydantic)

**ROI Comprovado**: Aplicar checklist ANTES economiza 38 minutos de debugging por implementaÃ§Ã£o (validado FASE 2.5 unitÃ¡rio, FASE 2.6 E2E).

#### E2E Workflow Tests (Validado FASE 2.6 - Out/2025)

**Context**: Testes E2E de workflows LangGraph multi-turn stateless requerem 4 consideraÃ§Ãµes adicionais ao checklist base.

**4 Pontos Adicionais (Checklist expandido: 8 â†’ 12 pontos)**:

**9. Verificar property/mÃ©todo existe**:
*   **SEMPRE** usar `grep "@property\|def method_name" src/file.py` antes de usar em cÃ³digo
*   **Exemplo validado (2025-10-16)**: Assumiu `client_profile_agent` property existia â†’ `AttributeError`
*   **PrevenÃ§Ã£o**: 
    ```bash
    grep "@property" src/graph/workflow.py | grep "client_profile_agent"
    # Se nÃ£o retornar nada, property nÃ£o existe - criar ou usar alternativa
    ```
*   **ROI**: 5-8 min economizados

**10. Considerar persistÃªncia de state**:
*   **SEMPRE** perguntar: "Como state persiste entre mÃºltiplos `run()` calls?"
*   **Exemplo validado (2025-10-16)**: Workflow stateless, `onboarding_progress` perdido entre calls
*   **SoluÃ§Ã£o padrÃ£o**: In-memory sessions dict
    ```python
    class BSCWorkflow:
        def __init__(self):
            self._onboarding_sessions: dict[str, dict[str, Any]] = {}
        
        def handler(self, state):
            user_id = state.user_id
            # Load session
            session = self._onboarding_sessions.get(user_id, {})
            # Process...
            # Save session
            self._onboarding_sessions[user_id] = updated_session
            # Cleanup (se completo)
            if is_complete:
                del self._onboarding_sessions[user_id]
    ```
*   **ROI**: 20-30 min economizados (pattern reutilizÃ¡vel)

**11. Fixtures Pydantic com ID customizado**:
*   **SEMPRE** criar profile inline quando teste precisa `client_id` especÃ­fico
*   **Exemplo validado (2025-10-16)**: Fixture `client_id='fixture'`, teste esperava `'test_005'` â†’ assertion falhou
*   **Template**:
    ```python
    test_profile = ClientProfile(
        client_id="test_cliente_specific_id",  # Match user_id do teste
        company=valid_client_profile.company,  # Reutilizar outros campos
        context=valid_client_profile.context,
        # ... demais campos do fixture original
    )
    ```
*   **ROI**: 8-12 min economizados

**12. Teste de regressÃ£o crÃ­tico OBRIGATÃ“RIO**:
*   **SEMPRE** incluir 1 teste validando que funcionalidade existente NÃƒO quebrou
*   **Exemplo validado (2025-10-16)**: `test_rag_workflow_cliente_existente_nao_quebrado`
    *   Validou que cliente existente (phase=DISCOVERY) usa RAG tradicional
    *   Routing correto, workflow completo, zero breaking changes
*   **Template**:
    ```python
    def test_existing_functionality_not_broken():
        """CRÃTICO: Validar que funcionalidade X nÃ£o quebrou com nova feature Y.
        
        Este teste previne REGRESSÃƒO!
        """
        # Setup: Cliente/estado existente (nÃ£o novo)
        mock_existing_state()
        
        # Action: Executar workflow tradicional
        result = workflow.run_traditional_flow(...)
        
        # Assert: Comportamento mantido
        assert traditional_method.called
        assert not new_feature_method.called  # Nova feature NÃƒO interferiu
    ```
*   **ROI**: CrÃ­tico (previne breaking changes, economiza horas de rollback)

**13. Implementation-First Testing para APIs desconhecidas (validado FASE 3.1 - 2025-10-19)**
*   Quando implementar tool/agent NOVO com API desconhecida, SEMPRE ler implementaÃ§Ã£o ANTES de escrever testes. 
*   Pattern: (1) `grep "def " src/module/file.py` (descobre mÃ©todos disponÃ­veis), (2) `grep "def method_name" src/module/file.py -A 15` (lÃª signature completa), (3) `grep "class SchemaName" src/memory/schemas.py -A 30` (verifica schemas Pydantic usados), (4) Escrever testes alinhados com API real.
*   RAZÃƒO: TDD funciona quando vocÃª CONHECE a API. Para APIs desconhecidas, assumir estrutura causa reescrita completa de testes.
*   **ROI**: 30-40 min economizados por implementaÃ§Ã£o.
*   **QUANDO USAR**: APIs novas (tools consultivas FASE 3+), agentes novos, integraÃ§Ãµes complexas (RAG, LLM, multi-step).
*   **QUANDO NÃƒO USAR** (TDD tradicional melhor): API conhecida, lÃ³gica simples (math, pure functions), refactoring (testes jÃ¡ existem).
*   **LiÃ§Ã£o detalhada**: `docs/lessons/lesson-swot-testing-methodology-2025-10-19.md` (700+ linhas com checklist completo).

**15. LER SCHEMA PYDANTIC VIA GREP ANTES DE CRIAR FIXTURE OU ACESSAR CAMPOS (validado FASE 3.5 - 2025-10-19)**
*   PROBLEMA RESOLVIDO: Fixtures Pydantic invÃ¡lidas (5 erros recorrentes 4 sessÃµes: SWOT, Five Whys, Issue Tree, KPI, Strategic Obj), context builders acessando campos inexistentes (2 erros 3 sessÃµes).
*   ROOT CAUSE: NÃƒO ler schema Pydantic ANTES de criar fixture/acessar campos, assumir estrutura baseada em memÃ³ria.
*   **QUANDO APLICAR**: SEMPRE antes de: (1) Criar fixture Pydantic, (2) Escrever context builder, (3) Acessar campos de schema em qualquer cÃ³digo.
*   **COMO APLICAR** (5 sub-pontos detalhados em [[memory:9969868]]):
    1. `grep "class SchemaName" src/memory/schemas.py -A 50` â†’ Identificar campos obrigatÃ³rios/opcionais/Literal
    2. `grep "min_length\|@field_validator" src/memory/schemas.py` â†’ Identificar validators
    3. Validar nested schemas (schemas que agregam outros schemas)
    4. Criar fixture com campos validados (margem de seguranÃ§a em min_length)
    5. Defensive programming em context builders (hasattr/getattr antes de acessar)
*   **ROI ESPERADO**: 30-40 min economizados por sessÃ£o (fixtures corretas primeira tentativa).
*   **ROI VALIDADO**: SessÃ£o 20 teve 8 erros (1.5h debugging) que teriam sido prevenidos aplicando PONTO 15.
*   **LiÃ§Ã£o detalhada**: `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys Root Cause Analysis completa).

#### Mock Testing Patterns (Validado FASE 3.4 - Out/2025)

**Context**: Mocks para mÃºltiplas chamadas sequenciais requerem abordagem dinÃ¢mica. Mock estÃ¡tico (`return_value` fixo) causa falhas quando tool processa lista de items esperando outputs diferentes.

**Ponto 14: Mock MÃºltiplas Chamadas com itertools.cycle**:

**QUANDO USAR**: Mock serÃ¡ invocado N vezes (N > 1) esperando outputs diferentes a cada chamada.

**RAZÃƒO**: Mock estÃ¡tico retorna sempre o mesmo valor. Tools que processam lista items (ex: 4 perspectivas BSC) falham com ValidationError quando mock nÃ£o diferencia outputs.

**SOLUÃ‡ÃƒO VALIDADA**: `itertools.cycle` para retornar valores sequencialmente.

**Template Geral**:
```python
from itertools import cycle

@pytest.fixture
def mock_with_cycle():
    """Mock com itertools.cycle - mÃºltiplos valores sequenciais."""
    mock_obj = MagicMock()
    
    # Step 1: Definir valores esperados em ordem
    values_order = [value1, value2, value3, value4]
    values_cycle = cycle(values_order)
    
    # Step 2: side_effect usa next(cycle)
    def side_effect_fn(*args, **kwargs):
        """Retorna prÃ³ximo valor na sequÃªncia."""
        return next(values_cycle)
    
    mock_obj.method.side_effect = side_effect_fn
    return mock_obj
```

**Exemplo Real - KPI Definer Tool** (validado 2025-10-19):
```python
from itertools import cycle
from pydantic import BaseModel

@pytest.fixture
def mock_llm() -> MagicMock:
    """Mock LLM que retorna KPIs com perspectiva correta sequencialmente."""
    llm = MagicMock(spec=["invoke", "with_structured_output"])
    
    class KPIListOutput(BaseModel):
        kpis: list[KPIDefinition]
    
    # Define KPIs mock para cada perspectiva
    kpis_by_perspective = {
        "Financeira": [KPIDefinition(name="ARR", perspective="Financeira", ...)],
        "Clientes": [KPIDefinition(name="NPS", perspective="Clientes", ...)],
        "Processos Internos": [KPIDefinition(name="Lead Time", perspective="Processos Internos", ...)],
        "Aprendizado e Crescimento": [KPIDefinition(name="Retencao", perspective="Aprendizado e Crescimento", ...)]
    }
    
    # SOLUCAO: itertools.cycle alinhado com ordem de chamadas da tool
    # Tool chama: Financeira â†’ Clientes â†’ Processos â†’ Aprendizado (linhas 152-156)
    perspective_order = [
        "Financeira",
        "Clientes",
        "Processos Internos",
        "Aprendizado e Crescimento"
    ]
    perspective_cycle = cycle(perspective_order)
    
    def mock_invoke_side_effect(prompt: str):
        """Retorna KPIs da prÃ³xima perspectiva na sequÃªncia."""
        perspective = next(perspective_cycle)
        kpis = kpis_by_perspective[perspective]
        return KPIListOutput(kpis=kpis)
    
    mock_structured_llm = MagicMock()
    mock_structured_llm.invoke.side_effect = mock_invoke_side_effect
    llm.with_structured_output.return_value = mock_structured_llm
    return llm
```

**Pergunta-Chave ANTES de criar mock**:
> "Quantas vezes o mock serÃ¡ invocado e com quais diferenÃ§as nos outputs esperados?"

**DecisÃ£o Tree**:
- Mock invocado **1x** OU **N vezes com MESMO output**? â†’ `return_value` fixo OK
- Mock invocado **N vezes com OUTPUTS DIFERENTES**? â†’ `itertools.cycle` ou `side_effect` lista
- Mock invocado **N vezes exatas** (N == len(lista))? â†’ `side_effect = [val1, val2, val3]`
- Mock invocado **N vezes indefinido** OU **N > len(values)**? â†’ `itertools.cycle` (melhor escolha)

**Alternativas e Trade-offs**:

1. **Lista finita** (`side_effect = [val1, val2, val3]`):
   - âœ… Simples, direto
   - âŒ Erro `StopIteration` se chamadas > len(lista)
   - **Usar quando**: N chamadas exatas conhecido

2. **Callable com contador**:
   ```python
   call_count = 0
   def side_effect():
       nonlocal call_count
       call_count += 1
       return values[call_count - 1]
   ```
   - âœ… Controle explÃ­cito
   - âŒ Mais verboso, `nonlocal` nÃ£o pythÃ´nico
   - **Usar quando**: LÃ³gica complexa por chamada

3. **itertools.cycle** (RECOMENDADO):
   - âœ… PythÃ´nico, elegante
   - âœ… Nunca lanÃ§a `StopIteration` (ciclo infinito)
   - âœ… Alinha naturalmente com ordem de chamadas da tool
   - âŒ Ciclo repete (pode mascarar bugs se tool chamar mais que esperado)
   - **Usar quando**: Mock mÃºltiplas chamadas sequenciais (99% dos casos)

**Checklist AplicaÃ§Ã£o**:
- [ ] Mock serÃ¡ invocado N vezes (N > 1)?
- [ ] Outputs esperados sÃ£o diferentes a cada chamada?
- [ ] Tool processa lista de items sequencialmente?
- [ ] Ordem do cycle alinha com ordem de chamadas da tool? (verificar cÃ³digo `for item in items:`)
- [ ] `side_effect` usa `next(cycle)` corretamente?
- [ ] Teste valida perspectiva/category/type correto em cada output?

**AntipadrÃµes Evitados**:
- âŒ **Mock estÃ¡tico com return_value fixo** para mÃºltiplas chamadas
- âŒ **String matching no prompt** para detectar qual output retornar (frÃ¡gil, dependente de formato)
- âŒ **Parsing JSON/XML** para diferenciar chamadas (adiciona complexidade desnecessÃ¡ria)
- âŒ **Criar ValidationError manualmente** em Pydantic (preferir passar dados invÃ¡lidos e deixar Pydantic validar)

**ROI**: 15-20 min economizados (debugging mock estÃ¡tico vs criaÃ§Ã£o correta desde inÃ­cio)

**Aplicabilidade Futura**:
- âœ… Objetivos EstratÃ©gicos Tool (mÃºltiplos objetivos por perspectiva BSC)
- âœ… Action Plan Tool (lista de aÃ§Ãµes sequenciais)
- âœ… Benchmarking Tool (comparaÃ§Ãµes mÃºltiplas perspectivas)
- âœ… Qualquer tool que processa lista items com LLM/API calls sequenciais

**LiÃ§Ãµes Detalhadas**:
- `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
- MemÃ³ria [[memory:9969868]] ponto 14 (checklist obrigatÃ³rio 14 pontos)

 Regarding `.cursorignore` files:

*   Cursor IDE uses `.cursorignore` files to control which files Cursor can access. This is for security and performance. This is for security and performance.
*   `.env` files are ignored by default.
*   To unignore a file, place an exclamation mark `!` in front of the ignore glob pattern in the `.cursorignore` file. For example: `!.env`.
*   You can edit the global ignore list in Cursor's settings. Go to `File > Preferences > VS code settings` and search for "Global Cursor Ignore List". You can remove patterns from the list there.
*   If you don't have a `.cursorignore` file, but `.env` files are still ignored, check the global ignore list.
*   Global cursorignore negations do not override `.gitignore` rules unless specific syntax is used.

### How to Liberate .env from the Global Cursor Ignore

Based on collected information, Cursor ignores `.env` files by default through global configuration. Here are three ways to liberate the file:

#### Method 1: Modify Global Ignore List (Recommended)

1.  **Open Cursor Settings:**
    *   Press `Ctrl + ,` (Windows) or `Cmd + ,` (Mac)
    *   Or go to: `File > Preferences > Settings`
2.  **Search for "Global Cursor Ignore":**
    *   Type in the search bar: `cursor ignore`
    *   Locate: **"Cursor > General: Global Cursor Ignore List"**
3.  **Remove or Comment the `.env` Entry:**
    *   You will see a list with patterns like:

    ```
    **/node_modules/**
    **/.git/**
    **/.env
    ```
    *   **Delete** the line `**/.env` or patterns related to `.env*`
    *   Click **"OK"** or save the settings.

#### Method 2: Use `.cursorignore` to "Unignore" (Local Override)

Create or edit the `.cursorignore` file in the **root of your project**:

```bash
# .cursorignore

# Unignore .env files (overrides the global ignore)
!.env
!.env.local
!.env.development

# You can also be more specific
!**/.env
```

**Important:** The `!` (exclamation) prefix **reverses** the ignore, allowing the file.

#### Method 3: Via Cursor Interface (Quick Shortcut)

1.  **Access Indexing Settings:**
    *   Go to: `Settings > Cursor > Indexing and Docs`
    *   Or search for: `cursor indexing`
2.  **Edit .cursorignore:**
    *   Click **"Edit .cursorignore"**
    *   Add the line: `!.env`
3.  **Reload the Index:**
    *   Press `Ctrl + Shift + P` (Windows) or `Cmd + Shift + P` (Mac)
    *   Type: `Cursor: Reindex Codebase`
    *   Execute the command

**IMPORTANT: Security Considerations**

Brightdata's research revealed important community discussions:

> **"Cursor ignores env files by default. Although risky, you can remove env files from the ignore list..."**

**Why does Cursor ignore `.env` by default?**

1.  **Credential Protection:** `.env` files contain API keys, passwords, and sensitive tokens.
2.  **Leakage Prevention:** Prevents information from being inadvertently sent to AI servers.
3.  **Context Security:** Prevents information from being inadvertently sent to AI servers.
4.  **Context Security:** Prevents credentials from appearing in chat/completions context.

**If you REALLY need to liberate `.env`:**

âœ… **Best Practice:**

*   Use `.env.example` or `.env.template` with dummy values.
*   Liberate **only** these template files in the ignore.
*   Keep the real `.env` ignored.

âŒ **Avoid:**

*   Liberating `.env` in public/shared projects.
*   Exposing real credentials in the AI context.
*   Removing protection in repositories with multiple collaborators.

**Test if it Worked**

1.  **Check the File Icon:**

    *   In the Cursor Explorer, ignored files show a ðŸš« icon.
    *   After liberating, the icon should disappear.
2.  **Test in Chat:**

    *   Open Cursor Chat.
    *   Type: `@.env` (mention the file).
    *   If it works, the file has been successfully liberated.
3.  **Verify Indexing:**

    *   `Ctrl + P` â†’ Type `.env`
    *   The file should appear in the results.

**Complete Example: `.cursorignore`**

```bash
# .cursorignore - Complete Example

# Keep ignored (security)
**/.env
**/.env.local
**/.env.production

# Liberate templates (safe)
!.env.example
!.env.template

# Other useful ignores
**/node_modules/**
**/.git/**
**/dist/**
**/build/**
**/__pycache/**
**/.pytest_cache/**
**/.venv/**
**/venv/**
```

**Sources (2025)**

*   [Cursor Docs - Ignore Files](https://cursor.com/docs/context/ignore/files)
*   [Cursor Forum - You can unignore files in .cursorignore](https://forum.cursor.com/t/you-can-unignore-files-in-cursorignore/38074)
*   [GitHub Issue #3101 - .env files ignored by default](https://github.com/cursor/cursor/issues/3101)
*   [Reddit r/cursor - How to disable .cursorignore from .cursorignore](https://www.reddit.com/r/cursor/comments/1ki08ww/)

### Language Detection Improvements

When the language detector in `src/rag/query_translator.py` is unable to determine the language of a query, it defaults to PT-BR. To improve language detection accuracy:

*   The `_detect_language` function has been enhanced with a regular expression to identify Portuguese suffixes (Ã§Ã£o, Ãµes, Ã¡rio, Ã¡rios, eira, eiras, eiro, eiros).
*   Word boundaries are used in keyword searches to prevent substring matches (e.g., "financial" in "financeiros").
*   The logging message has been updated to provide more context when the language is ambiguous.
*   Expand the list of keywords to include common BSC terms to improve language detection for technical queries.
*   When no keywords are detected, the system defaults to PT-BR, assuming a Brazilian context.

#### Code Snippets:

*   **Detecting Portuguese Suffixes:**

    ```python
    has_pt_suffixes = bool(re.search(r'\b\w*(Ã§Ã£o|Ãµes|Ã¡rio|Ã¡rios|eira|eiras|eiro|eiros)\b', text_lower))
    ```

*   **Keyword Search with Word Boundaries:**

    ```python
    pt_count = sum(1 for kw in pt_keywords if re.search(r'\b' + re.escape(kw) + r'\b', text_lower))
    en_count = sum(1 for kw in pt_keywords if re.search(r'\b' + re.escape(kw) + r'\b', text_lower))
    ```

### Dependency Management

*   When updating dependencies, especially `ruff`, ensure the version specified in `requirements.txt` is compatible with the project's other dependencies. Aim to use the latest stable version, but specify a version range to allow for minor updates without breaking changes. For example: `ruff>=0.7.0,<1.0.0`.
*   After updating dependencies in `requirements.txt`, run `pip install --upgrade -r requirements.txt` to update the environment.
*   When adding `mem0ai` ensure it is added in the Utilities section

### Pydantic Version Handling

*   When working with LangChain projects, be aware of Pydantic version compatibility. LangChain v0.3+ uses Pydantic V2 internally and recommends migrating imports from `langchain_core.pydantic_v1` to `pydantic` directly.
*   Avoid using the `pydantic.v1` compatibility namespace; import directly from Pydantic V2.
*   When using `BaseSettings` from `pydantic_settings`, use `SettingsConfigDict` instead of `ConfigDict` for configuration.
*   **Pydantic Config Migration**: Replace `class Config:` with `model_config = ConfigDict(...)` in Pydantic models to avoid deprecation warnings.

### New Rule: Python Circular Imports Resolution

When encountering circular import errors in Python, especially when using type hints, adhere to the following best practices:

- **Use `TYPE_CHECKING` for Conditional Imports**: Import modules or classes conditionally using the `typing.TYPE_CHECKING` flag. This ensures that the imports are only used during type-checking and not at runtime, which prevents circular import errors.
- **Leverage Lazy Imports**: Implement lazy imports within functions or methods. This defers the import statement until the function or method is called, avoiding circular dependencies during module loading.
- **Employ Forward References**: Use forward references as string literals for type hints. This allows you to refer to classes that are not yet defined. However, this approach should be combined with `from __future__ import annotations` for cleaner code.
- **Ensure Code Layering**: Refactor the code to ensure that the circular dependencies are not a result of poorly layered architecture. Consider using abstract classes or interfaces to define dependencies.
- **Apply Best Practices**: Follow community-endorsed best practices for circular import resolution.

### Rclone OneDrive Configuration and Usage

*   When using rclone with OneDrive, be aware of potential throttling issues, especially when transferring large files. To mitigate this, use the `--tpslimit` flag to limit transactions per second and the `--bwlimit` flag to limit bandwidth usage.
*   When encountering throttling errors, reduce the transfer rate or use rclone's built-in retry mechanisms.

### Architecture Reference Docs (FASE 3 - Out/2025)

When working on consulting code (agents, workflow, diagnostic tools):

**Consult `docs/architecture/` BEFORE reading source code:**

- **DATA_FLOW_DIAGRAMS.md**: Data flows (ClientProfile lifecycle, Diagnostic workflow, State transitions) - 5 Mermaid diagrams
- **API_CONTRACTS.md**: API contracts (signatures, parameters, exceptions, schemas) - 8 agents documented

**ROI**: ~1h saved per task (consult docs 1-2 min vs reading code 10-15 min)

**When to consult**:
- Implementing feature that interacts with existing agents
- Calling methods from other agents (needs exact signature)
- Debugging data flows or state transitions
- Creating tests (needs to know return types and exceptions)

### New Rule: Testing and Regression Prevention

To maintain code quality and prevent regressions, the following testing practices are **MANDATORY**:

1. **Continuous Integration (CI)**:
   - **ALWAYS** run the entire test suite (`pytest tests/ -n 8`) after each tool implementation.
   - **REASON**: Detect regressions immediately, preventing accumulated debugging time.
   - **ROI**: Prevents 2-4 hours of debugging regression errors later.
2. **Regression Tests**:
   - **MANDATORY**: Include at least one regression test (`test_existing_functionality_not_broken`) for each new feature or tool.
   - **PURPOSE**: Validates that existing functionality remains intact after new changes.

### New Rule: Documentation of Input/Output Flows

To improve agent performance and reduce debugging time, the following documentation rule is **MANDATORY**:

1. **Input/Output Flow Diagrams**:
   - Create a section "Input/Output Flow" in each `docs/tools/[TOOL].md` file.
   - Include a Mermaid sequence diagram showing the data flow: `ClientProfile â†’ Tool â†’ Schemas â†’ DiagnosticAgent â†’ Output`
   - **PURPOSE**: Provide a clear visual representation of the data flow for each tool.
   - **ROI**: Saves 10-15 minutes of code reading by providing a 30-second overview.

### Testing and Debugging Methodology (Improvements from Session 19 - 2025-10-19)

1. **Five Whys Root Cause Analysis**:
   - **WHEN**: Encountering test failures, **ALWAYS** apply 5 Whys to identify the root cause.
   - **EXAMPLE**: Session 19 debugging mock LLM (perspectives corretas).
   - **BENEFIT**: Structured debugging saves 15-20 minutes compared to trial-and-error.
2. **itertools.cycle for Mocking**:
   - **WHEN**: Mocking sequential calls (e.g., 4 perspectives), use `itertools.cycle` for elegant, robust mocks.
   - **AVOID**: String parsing or ad-hoc logic to determine the mock return value.
3. **model_validator for Cross-Perspective Validation**:
   - **WHEN**: Working with Pydantic models that involve multiple perspectives, use `model_validator(mode='after')` to ensure consistency across perspectives.
   - **EXAMPLE**: KPIFramework validates that KPIs belong to correct perspectives using `model_validator`.

### New Rule: Updated Tool Development Workflow

1. **Refactor the workflow to include explicit steps**:

    *   Step 1: Sequential Thinking (plan)
    *   Step 2: Discovery (technique/approach)
    *   Step 3: Navigation (locate docs)
    *   Step 4: Knowledge Base (study patterns)
    *   Step 5: Implementation (Schema â†’ Prompts â†’ Tool â†’ Integration)
    *   Step 6: Validation (15+ tests, coverage 70%+, pytest -v --tb=long)
    *   Step 7: Documentation (650+ lines, 4 use cases BSC)

    to emphasize the importance of planning and to be more explicit about the steps and to make it easier to follow and validate each phase of the development.

### New Rule: Implementation-First Testing and Root Cause Analysis

To enhance testing effectiveness and prevent recurring issues, adhere to the following guidelines:

1.  **Implementation-First Testing**:
    *   **ALWAYS** apply Implementation-First Testing (checklist point 13) before writing tests, especially for unfamiliar APIs.
    *   **PURPOSE**: Prevent test rewrites by aligning tests with the actual implementation.
    *   **5 Whys Before Coding**: When tests fail, apply 5 Whys rigorously to identify the root cause.
2. **Root Cause Analysis and Checklist Adherence**:
    *   **MANDATORY**: Before implementing any fix, identify the root cause and explicitly state which checklist point was violated.
    *   **ROI**: Systematic debugging prevents recurring errors and wasted time.
3.  **Schema Validation**:
    *   **MANDATORY**: For Pydantic models, **ALWAYS** validate fixtures against the schema constraints to ensure data validity.
    *   **PURPOSE**: Reduce debugging time by preventing data-related errors.
4. **Context Builder Validation**:
    *   **ALWAYS** ensure context builders use attributes that exist in the schema.
    *   **PURPOSE**: Prevent runtime errors due to attribute access failures.

### New Rule: 5 Whys Root Cause Analysis for Test Failures (Validado FASE 3.5 - 2025-10-19)

1. **Application**:
    * **WHEN** tests fail, **ALWAYS** apply 5 Whys systematically to identify the root cause
    * **PATTERN**:
        1.  Collect facts and timeline (full traceback)
        2.  Apply 5 Whys with evidences
        3.  Base solution on root cause (not symptoms)
2. **Implementation-First Testing (Checklist Point 13)**:
    *   **MANDATORY**: Read the API implementation before writing tests to align with the actual implementation
    *   **PURPOSE**: Prevents test rewrites

### Checklist Atualizado (Com 5 Whys e Implementation-First Testing)

- [ ] Antes de escrever testes, LER ASSINATURA COMPLETA (grep, schema) e VALIDAR FIXTURES contra schema
- [ ] Ao encontrar falhas, aplicar 5 Whys para identificar ROOT CAUSE
- [ ] Antes de implementar fix, explicitar qual CHECKLIST foi violado
- [ ] SEMPRE aplicar Implementation-First Testing (ler cÃ³digo antes de testes)