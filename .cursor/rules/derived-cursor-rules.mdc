---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## HEADERS

## TECH STACK

### Test Methodology (Validado Out/2025 - ROI: 32-60 min economizados por implementa√ß√£o)

**ANTES de escrever QUALQUER teste (unit√°rio ou E2E), aplicar [[memory:9969868]] - CHECKLIST OBRIGAT√ìRIO de 12 pontos**:

**PONTOS 1-8: Testes Unit√°rios** (validado FASE 2.5)

1. **Ler assinatura completa**: `grep "def method_name" src/file.py -A 10` (NUNCA assumir)
2. **Verificar tipo retorno**: Objeto Pydantic ou built-in? Campos obrigat√≥rios?
3. **Contar par√¢metros**: Quantos params (n√£o contar `self`)?
4. **Valida√ß√µes pr√©-flight**: Valida√ß√µes no c√≥digo + Pydantic (min_length, Literal, validators)
5. **Entender decorators**: @retry com `reraise=True` relan√ßa exce√ß√£o original (N√ÉO RetryError)
6. **Fixtures Pydantic**: NUNCA passar `None` para `default_factory`. Incluir campos obrigat√≥rios.
7. **Dados v√°lidos em mocks**: Usar MARGEM DE SEGURAN√áA (ex: min_length=20 ‚Üí usar 50+ chars)
8. **Verificar m√©todo correto**: Confirmar nome exato via grep (ex: `invoke()` n√£o `process_query()`)

**PONTOS 9-12: E2E Workflow Tests** (validado FASE 2.6) - Ver se√ß√£o "E2E Workflow Tests" below for details

**Debugging pytest (Validado [[memory:9969628]])**:
*   **SEMPRE** usar `--tb=long` (traceback completo, NUNCA --tb=short)
*   **NUNCA** usar filtros (`Select-Object`, `Select-String`) - oculta informa√ß√£o cr√≠tica
*   Comando correto: `pytest tests/file.py -v --tb=long 2>&1`

**pytest em paralelo** (performance):
*   `-n <num_workers>`: Workers paralelos
*   `-v`: Verbose output
*   `--tb=long`: Traceback completo
*   `--dist=loadscope`: Distribui por fixture scope (mais seguro)
*   `--dist=loadfile`: Distribui por arquivo (fixtures function-scoped)

**Li√ß√µes Detalhadas**:
- `docs/lessons/lesson-test-debugging-methodology-2025-10-15.md` (FASE 2.4, 5 erros)
- `docs/lessons/lesson-diagnostic-agent-test-methodology-2025-10-16.md` (FASE 2.5, 7 erros, 1.100+ linhas)
- `docs/lessons/lesson-onboarding-state-e2e-tests-2025-10-16.md` (FASE 2.6, 4 problemas E2E, 11.900+ linhas)

**ROI Comprovado**: Aplicar checklist ANTES economiza 38 minutos de debugging por implementa√ß√£o (validado FASE 2.5 unit√°rio, FASE 2.6 E2E).

#### E2E Workflow Tests (Validado FASE 2.6 - Out/2025)

**Context**: Testes E2E de workflows LangGraph multi-turn stateless requerem 4 considera√ß√µes adicionais ao checklist base.

**4 Pontos Adicionais (Checklist expandido: 8 ‚Üí 12 pontos)**:

**9. Verificar property/m√©todo existe**:
*   **SEMPRE** usar `grep "@property\|def method_name" src/file.py` antes de usar em c√≥digo
*   **Exemplo validado (2025-10-16)**: Assumiu `client_profile_agent` property existia ‚Üí `AttributeError`
*   **Preven√ß√£o**: 
    ```bash
    grep "@property" src/graph/workflow.py | grep "client_profile_agent"
    # Se n√£o retornar nada, property n√£o existe - criar ou usar alternativa
    ```
*   **ROI**: 5-8 min economizados

**10. Considerar persist√™ncia de state**:
*   **SEMPRE** perguntar: "Como state persiste entre m√∫ltiplos `run()` calls?"
*   **Exemplo validado (2025-10-16)**: Workflow stateless, `onboarding_progress` perdido entre calls
*   **Solu√ß√£o padr√£o**: In-memory sessions dict
    ```python
    class BSCWorkflow:
        def __init__(self):
            self._onboarding_sessions: dict[str, dict[str, Any]] = {}
        
        def handler(self, state):
            user_id = state.user_id
            # Load session
            session = self._onboarding_sessions.get(user_id, {})
            # Process...
            # Save session
            self._onboarding_sessions[user_id] = updated_session
            # Cleanup (se completo)
            if is_complete:
                del self._onboarding_sessions[user_id]
    ```
*   **ROI**: 20-30 min economizados (pattern reutiliz√°vel)

**11. Fixtures Pydantic com ID customizado**:
*   **SEMPRE** criar profile inline quando teste precisa `client_id` espec√≠fico
*   **Exemplo validado (2025-10-16)**: Fixture `client_id='fixture'`, teste esperava `'test_005'` ‚Üí assertion falhou
*   **Template**:
    ```python
    test_profile = ClientProfile(
        client_id="test_cliente_specific_id",  # Match user_id do teste
        company=valid_client_profile.company,  # Reutilizar outros campos
        context=valid_client_profile.context,
        # ... demais campos do fixture original
    )
    ```
*   **ROI**: 8-12 min economizados

**12. Teste de regress√£o cr√≠tico OBRIGAT√ìRIO**:
*   **SEMPRE** incluir 1 teste validando que funcionalidade existente N√ÉO quebrou
*   **Exemplo validado (2025-10-16)**: `test_rag_workflow_cliente_existente_nao_quebrado`
    *   Validou que cliente existente (phase=DISCOVERY) usa RAG tradicional
    *   Routing correto, workflow completo, zero breaking changes
*   **Template**:
    ```python
    def test_existing_functionality_not_broken():
        """CR√çTICO: Validar que funcionalidade X n√£o quebrou com nova feature Y.
        
        Este teste previne REGRESS√ÉO!
        """
        # Setup: Cliente/estado existente (n√£o novo)
        mock_existing_state()
        
        # Action: Executar workflow tradicional
        result = workflow.run_traditional_flow(...)
        
        # Assert: Comportamento mantido
        assert traditional_method.called
        assert not new_feature_method.called  # Nova feature N√ÉO interferiu
    ```
*   **ROI**: Cr√≠tico (previne breaking changes, economiza horas de rollback)

**ROI Total E2E**: 32-60 min economizados por implementa√ß√£o (4 erros √ó 8-15 min/erro)

**Li√ß√£o Detalhada**: `docs/lessons/lesson-onboarding-state-e2e-tests-2025-10-16.md` (11.900+ linhas)

Regarding `.cursorignore` files:

*   Cursor IDE uses `.cursorignore` files to control which files Cursor can access. This is for security and performance.
*   `.env` files are ignored by default.
*   To unignore a file, place an exclamation mark `!` in front of the ignore glob pattern in the `.cursorignore` file. For example: `!.env`.
*   You can edit the global ignore list in Cursor's settings. Go to `File > Preferences > VS code settings` and search for "Global Cursor Ignore List". You can remove patterns from the list there.
*   If you don't have a `.cursorignore` file, but `.env` files are still ignored, check the global ignore list.
*   Global cursorignore negations do not override `.gitignore` rules unless specific syntax is used.

### How to Liberate .env from the Global Cursor Ignore

Based on collected information, Cursor ignores `.env` files by default through global configuration. Here are three ways to liberate the file:

#### Method 1: Modify Global Ignore List (Recommended)

1.  **Open Cursor Settings:**
    *   Press `Ctrl + ,` (Windows) or `Cmd + ,` (Mac)
    *   Or go to: `File > Preferences > Settings`
2.  **Search for "Global Cursor Ignore":**
    *   Type in the search bar: `cursor ignore`
    *   Locate: **"Cursor > General: Global Cursor Ignore List"**
3.  **Remove or Comment the `.env` Entry:**
    *   You will see a list with patterns like:

    ```
    **/node_modules/**
    **/.git/**
    **/.env
    ```
    *   **Delete** the line `**/.env` or patterns related to `.env*`
    *   Click **"OK"** or save the settings.

#### Method 2: Use `.cursorignore` to "Unignore" (Local Override)

Create or edit the `.cursorignore` file in the **root of your project**:

```bash
# .cursorignore

# Unignore .env files (overrides the global ignore)
!.env
!.env.local
!.env.development

# You can also be more specific
!**/.env
```

**Important:** The `!` (exclamation) prefix **reverses** the ignore, allowing the file.

#### Method 3: Via Cursor Interface (Quick Shortcut)

1.  **Access Indexing Settings:**
    *   Go to: `Settings > Cursor > Indexing and Docs`
    *   Or search for: `cursor indexing`
2.  **Edit .cursorignore:**
    *   Click **"Edit .cursorignore"**
    *   Add the line: `!.env`
3.  **Reload the Index:**
    *   Press `Ctrl + Shift + P` (Windows) or `Cmd + Shift + P` (Mac)
    *   Type: `Cursor: Reindex Codebase`
    *   Execute the command

**IMPORTANT: Security Considerations**

Brightdata's research revealed important community discussions:

> **"Cursor ignores env files by default. Although risky, you can remove env files from the ignore list..."**

**Why does Cursor ignore `.env` by default?**

1.  **Credential Protection:** `.env` files contain API keys, passwords, and sensitive tokens.
2.  **Leakage Prevention:** Prevents information from being inadvertently sent to AI servers.
3.  **Context Security:** Prevents credentials from appearing in chat/completions context.
4.  **Context Security:** Prevents credentials from appearing in chat/completions context.

**If you REALLY need to liberate `.env`:**

‚úÖ **Best Practice:**

*   Use `.env.example` or `.env.template` with dummy values.
*   Liberate **only** these template files in the ignore.
*   Keep the real `.env` ignored.

‚ùå **Avoid:**

*   Liberating `.env` in public/shared projects.
*   Exposing real credentials in the AI context.
*   Removing protection in repositories with multiple collaborators.

**Test if it Worked**

1.  **Check the File Icon:**

    *   In the Cursor Explorer, ignored files show a üö´ icon.
    *   After liberating, the icon should disappear.
2.  **Test in Chat:**

    *   Open Cursor Chat.
    *   Type: `@.env` (mention the file).
    *   If it works, the file has been successfully liberated.
3.  **Verify Indexing:**

    *   `Ctrl + P` ‚Üí Type `.env`
    *   The file should appear in the results.

**Complete Example: `.cursorignore`**

```bash
# .cursorignore - Complete Example

# Keep ignored (security)
**/.env
**/.env.local
**/.env.production

# Liberate templates (safe)
!.env.example
!.env.template

# Other useful ignores
**/node_modules/**
**/.git/**
**/dist/**
**/build/**
**/__pycache/**
**/.pytest_cache/**
**/.venv/**
**/venv/**
```

**Sources (2025)**

*   [Cursor Docs - Ignore Files](https://cursor.com/docs/context/ignore/files)
*   [Cursor Forum - You can unignore files in .cursorignore](https://forum.cursor.com/t/you-can-unignore-files-in-cursorignore/38074)
*   [GitHub Issue #3101 - .env files ignored by default](https://github.com/cursor/cursor/issues/3101)
*   [Reddit r/cursor - How to disable .env from .cursorignore](https://www.reddit.com/r/cursor/comments/1ki08ww/)

### Language Detection Improvements

When the language detector in `src/rag/query_translator.py` is unable to determine the language of a query, it defaults to PT-BR. To improve language detection accuracy:

*   The `_detect_language` function has been enhanced with a regular expression to identify Portuguese suffixes (√ß√£o, √µes, √°rio, √°rios, eira, eiras, eiro, eiros).
*   Word boundaries are used in keyword searches to prevent substring matches (e.g., "financial" in "financeiros").
*   The logging message has been updated to provide more context when the language is ambiguous.
*   Expand the list of keywords to include common BSC terms to improve language detection for technical queries.
*   When no keywords are detected, the system defaults to PT-BR, assuming a Brazilian context.

#### Code Snippets:

*   **Detecting Portuguese Suffixes:**

    ```python
    has_pt_suffixes = bool(re.search(r'\b\w*(√ß√£o|√µes|√°rio|√°rios|eira|eiras|eiro|eiros)\b', text_lower))
    ```

*   **Keyword Search with Word Boundaries:**

    ```python
    pt_count = sum(1 for kw in pt_keywords if re.search(r'\b' + re.escape(kw) + r'\b', text_lower))
    en_count = sum(1 for kw in pt_keywords if re.search(r'\b' + re.escape(kw) + r'\b', text_lower))
    ```

### Dependency Management

*   When updating dependencies, especially `ruff`, ensure the version specified in `requirements.txt` is compatible with the project's other dependencies. Aim to use the latest stable version, but specify a version range to allow for minor updates without breaking changes. For example: `ruff>=0.7.0,<1.0.0`.
*   After updating dependencies in `requirements.txt`, run `pip install --upgrade -r requirements.txt` to update the environment.
*   When adding `mem0ai` ensure it is added in the Utilities section

### Pydantic Version Handling

*   When working with LangChain projects, be aware of Pydantic version compatibility. LangChain v0.3+ uses Pydantic V2 internally and recommends migrating imports from `langchain_core.pydantic_v1` to `pydantic` directly.
*   Avoid using the `pydantic.v1` compatibility namespace; import directly from Pydantic V2.
*   When using `BaseSettings` from `pydantic_settings`, use `SettingsConfigDict` instead of `ConfigDict` for configuration.

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

When debugging, especially for E2E tests, consider running tests in parallel to speed up the process. For example, use sequential thinking to organize your actions.

### Architectural Considerations for the BSC Consulting Agent (v2.0)

The following architectural considerations should guide the development of the BSC Consulting Agent v2.0:

#### 1. Technology Stack Consolidation:

*   Prioritize integration with Mem0 for persistent memory, leveraging its LangGraph compatibility and validated performance metrics.
*   Incorporate MCPs for productivity, initially focusing on Google Calendar and Asana.

#### 2. Memory Implementation:

*   Utilize Mem0's platform API or consider a self-hosted alternative (PostgreSQL + pgvector) for storing client profiles and engagement progress.

#### 3. Workflow Orchestration:

*   Expand the LangGraph state machine to include a ConsultingEngagementState, with new states for onboarding, discovery, diagnostic, solution design, (optional) implementation, and monitoring.

#### 4. Agent Layer:

*   Augment existing BSC knowledge agents with new consulting agents: ClientProfileAgent, WorkshopFacilitatorAgent, and DiagnosticAnalystAgent.

#### 5. Tool/MCP Layer:

*   Integrate the Brightdata MCP (for external context), interactive tools (SWOT builder, strategy map designer, KPI definer, issue tree analyzer), and productivity MCPs (Google Calendar, Asana).

#### 6. Roadmap and Prioritization:

*   Follow a phased implementation, starting with a foundational MVP (Mem0 integration, onboarding, workflow, basic tools) and expanding to enterprise features (Brightdata, additional tools, productivity MCPs).

#### 7. Data Structuring for Mem0:

*   Adopt a well-defined schema for the ClientProfile in Mem0:

```json
{
  "user_id": "cliente_123",
  "company": {
    "name": "Empresa X",
    "industry": "Healthcare",
    "size": "200-500 employees",
    "maturity": "Growth stage"
  },
  "strategic_context": {
    "main_challenge": "Crescimento org√¢nico sem perder qualidade",
    "objectives": ["Aumentar EBITDA 20%", "Melhorar NPS 30pts"],
    "stakeholders": ["CEO Maria", "CFO Jo√£o", "COO Ana"]
  },
  "engagement": {
    "mode": "Advisor",  // ou "Full Consultant"
    "current_phase": "DISCOVERY",
    "completed_activities": ["onboarding", "swot_analysis"],
    "pending_deliverables": ["diagnostic_report", "strategy_map"]
  },
  "preferences": {
    "communication_style": "Direto, dados-driven",
    "meeting_frequency": "Semanal",
    "tools": ["Google Calendar", "Asana"]
  },
  "history": [
    {"date": "2025-10-15", "activity": "Onboarding", "key_insights": "..."},
    {"date": "2025-10-18", "activity": "SWOT Workshop", "outputs": "..."}
  ]
}
```

#### 8. Memory Solution Evaluation

*   Mem0 (mem0.ai) is the preferred choice, offering native LangGraph integration, a validated research track record, and an available MCP.

#### 9. Productivity MCP Prioritization

*   Integrate Google Calendar and Asana MCPs to facilitate scheduling and project tracking.

#### 10. Implementation Roadmap

*   Prioritize a phased rollout, beginning with Mem0 integration and a basic workflow, followed by the addition of productivity MCPs.

### Next Step: FASE 2.5 - DiagnosticAgent

The next step is to implement the DiagnosticAgent. The following specifications, objectives, and checklist should be considered.

#### Specification

| Item           | Detail                               |
| ----- | --- |
| **ID**         | FASE 2.5                             |
| **Task**       | DiagnosticAgent                      |
| **Estimated Duration** | 2-3h                                 |
| **File**       | `src/agents/diagnostic_agent.py`     |

#### Objectives

1.  **Implement BSC diagnostic analysis** across the 4 perspectives:
    *   Financial
    *   Customers
    *   Internal Processes
    *   Learning and Growth
2.  **Integration with ClientProfile**:
    *   Consume company context (CompanyInfo)
    *   Analyze identified challenges
    *   Consider strategic objectives
3.  **Structured Output**:
    *   Diagnosis by perspective
    *   Identified gaps
    *   Improvement opportunities
    *   BSC-aligned recommendations
4.  **Quality**:
    *   15+ unit tests
    *   Coverage > 80%
    *   Complete type hints

#### Expected Outcome

*   üìä Phase 2 Progress: 50% (5/10 tasks)
*   üéØ DiagnosticAgent ready for FASE 2.6 (ONBOARDING State integration)

#### Pre-Implementation Checklist

*   ‚úÖ ClientProfileAgent available (complete extraction)
*   ‚úÖ OnboardingAgent available (conversational workflow)
*   ‚úÖ BSCState v2.0 with consulting fields
*   ‚úÖ 4 BSC expert agents (Financial, Customer, Process, Learning)
*   ‚úÖ Functional RAG retriever (BSC literature)
*   ‚úÖ Validated system prompts (`client_profile_prompts.py` as reference)

### To Consider:
*   [https://github.com/humanlayer/12-factor-agents](https://github.com/humanlayer/12-factor-agents)
*   [https://www.anthropic.com/engineering/building-effective-agents](https://www.anthropic.com/engineering/building-effective-agents)
*   [https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf)

## TEST CONFIGURATION

*   The test `test_parallel_agent_execution` now has a threshold of 200 seconds.
*   The test `test_latency_percentiles` now has a P95 threshold of 240 seconds (4 minutes).
*   Queries for testing are now stored in `tests/benchmark_queries.json`, which contains 50 varied BSC queries.
*   Consider using the RAGAS framework for automated evaluation. Key metrics include context relevancy (now context precision), answer relevancy, and faithfulness.
*   Ground truth data can be generated automatically by pre-trained models and validated manually by experts, focusing on a subset of the data (10-15%).
*   To ensure pytest discovers tests and handles imports correctly, add the following to `pyproject.toml` under `[tool.pytest.ini_options]`:

```toml
pythonpath = ["."]
# Usar importlib para resolver conflitos com __init__.py (Issue #11960 pytest-dev)
addopts = [
    "--import-mode=importlib",
]
```

### Test Debugging Methodology (Validated 2025-10-15)

**CRITICAL: When debugging failing tests, ALWAYS use this methodology:**

#### 1. Traceback Completo (OBRIGAT√ìRIO)

```bash
# ‚úÖ CORRETO: Traceback completo SEM filtro
pytest tests/test_file.py -v --tb=long 2>&1

# ‚ùå NUNCA fazer: oculta informa√ß√µes cr√≠ticas
pytest tests/test_file.py -v --tb=short 2>&1
pytest tests/test_file.py -v --tb=long 2>&1 | Select-Object -Last 50
pytest tests/test_file.py -v --tb=long 2>&1 | Select-String "ERROR"
```

**Por qu√™:** Filtros (Select-Object, Select-String, grep) ocultam linhas cr√≠ticas do traceback que revelam a causa raiz do erro.

**Mem√≥ria:** [[memory:9969628]] - Validado: economiza 8 min/erro evitado

#### 2. Sequential Thinking ANTES de Corrigir

Use sequential thinking para analisar erro ANTES de implementar corre√ß√£o:

1. **Thought 1**: Qual o erro exato? (ler traceback completo)
2. **Thought 2**: Qual a linha que falha? (identificar no c√≥digo)
3. **Thought 3**: Qual a causa raiz? (n√£o apenas o sintoma)
4. **Thought 4**: Qual a corre√ß√£o necess√°ria? (m√≠nima e cir√∫rgica)
5. **Thought 5**: Como validar a corre√ß√£o? (executar teste individual)

#### 3. Resolver UM Erro por Vez

```bash
# ‚úÖ CORRETO: Focar em um teste falhando
pytest tests/test_file.py::test_specific_failing_test -v --tb=long 2>&1

# ‚ùå ERRADO: Tentar corrigir m√∫ltiplos erros simultaneamente
# (gera confus√£o e corre√ß√µes incorretas)
```

#### 4. Checklist ANTES de Escrever Testes

**Mem√≥ria:** [[memory:9969868]] - Previne 80% dos erros comuns

- [ ] 1. **Ler assinatura completa** do m√©todo: `grep "def method_name" src/file.py -A 10`
- [ ] 2. **Verificar tipo de retorno** exato (list vs Pydantic object)
- [ ] 3. **Contar par√¢metros** que o m√©todo aceita (n√£o assumir)
- [ ] 4. **Verificar valida√ß√µes pr√©-flight** (if checks no in√≠cio)
- [ ] 5. **Entender decorators** (@retry lan√ßa RetryError ap√≥s N tentativas)
- [ ] 6. **Fixtures Pydantic**: NUNCA passar None para campos com default_factory
- [ ] 7. **Dados v√°lidos em mocks**: Criar objetos Pydantic apenas com dados v√°lidos

**ROI:** 8 min economizados por erro evitado

**Fonte:** `docs/lessons/lesson-test-debugging-methodology-2025-10-15.md` (428 linhas)

## BENCHMARKING

*   To run a full Fase 2A benchmark, use the command: `python tests/benchmark_fase2a/run_benchmark.py`.
*   To run a pilot benchmark with 5 queries, use the command: `python tests/benchmark_fase2a/run_benchmark.py --pilot`.
*   The benchmark compares BASELINE (without optimizations) against FASE 2A (with Query Decomposition, Adaptive Re-ranking, Router).
*   The benchmark should be executed in the background and its progress monitored.
*   Upon completion, a comparative report with all RAGAS metrics should be generated.
*   Estimated runtime: 2.5-3 hours.
*   Estimated cost: ~$1.60 USD.
*   The test `test_parallel_agent_execution` now has a threshold of 200 seconds.
*   The test `test_latency_percentiles` now has a P95 threshold of 240 seconds (4 minutes).
*   Queries for testing are now stored in `tests/benchmark_queries.json`, which contains 50 varied BSC queries.
*   Consider using the RAGAS framework for automated evaluation. Key metrics include context relevancy (now context precision), answer relevancy, and faithfulness.
*   Ground truth data can be generated automatically by pre-trained models and validated manually by experts, focusing on a subset of the data (10-15%).

## GIT WORKFLOW & SECURITY

### Handling Secrets and GitHub Push Protection

When working with Git, especially on projects involving API keys or sensitive information, it's crucial to follow secure practices to prevent accidental exposure of secrets. GitHub provides a feature called Push Protection that scans commits for known secrets and blocks the push if any are found. Here's how to handle such situations:

**1. Ensure `.env` and similar files are in `.gitignore`:**

*   Always include `.env`, `.env.local`, and similar environment configuration files in your `.gitignore` to prevent them from being accidentally committed to the repository.

**2. GitHub Push Protection Bypass (Use with Caution):**

*   If you accidentally commit secrets and GitHub blocks the push, it provides URLs to "Allow secret." This option should be used with extreme caution and only in private repositories where you trust all collaborators.

**3. Disabling Push Protection (Not Recommended):**

*   It is possible to disable Push Protection in your repository settings (`Code security > Secret scanning > Push protection`). However, this is generally **not recommended** as it reduces the security of your repository. Only disable it if you fully understand the risks.

**4. Best Practice: Removing Secrets from Git History**

*   The most secure approach is to remove the secrets from the Git history. This can be done using tools like `git filter-branch` or `BFG Repo Cleaner`. This process rewrites the commit history, so it should be done carefully and coordinated with all collaborators.

**5. .env Templates**

*   Use `.env.example` or `.env.template` files with placeholder values instead of committing the actual `.env` file.

**Example .gitignore:**

```
# Environment variables
.env
.env.local
.env.*.local
```

**Steps to recover from accidentally committing secrets:**

1.  **Immediately add `.env` to `.gitignore`**.
2.  **Remove the `.env` file from the repository:**

    ```bash
    git rm --cached .env
    git commit -m "Remove .env file"
    ```
3.  **Rewrite Git history (Advanced):** Use `git filter-branch` or BFG Repo Cleaner` to remove the file from all past commits. This is the most secure option but requires caution.
4.  **Inform Collaborators:** If others have pulled the commit, they need to rebase their work after the history is rewritten.

### Handling Accidental Commits of Secrets

If you accidentally commit secrets (like API keys) to a Git repository, especially a public one, follow these steps immediately to mitigate the risk:

1.  **Immediately add the secrets file (e.g., `.env`) to `.gitignore`**. This prevents future accidental commits.
2.  **Remove the secrets file from the repository's history.** This is crucial to prevent unauthorized access to the secrets. Use tools like `git filter-branch` or `BFG Repo Cleaner` for this purpose.

    ```bash
    # Example using git filter-branch (use with caution!)
    git filter-branch --force --index-filter \
    'git rm --cached --ignore-unmatch path/to/your/secrets_file' \
    --prune-empty --tag-name-filter cat -- --all
    git push origin --force --all
    git push origin --force --tags
    ```

    *   **Important:** Rewriting Git history can be complex and affect collaborators. Communicate with your team before performing these steps.
3.  **Reset the compromised secrets.** Generate new API keys, passwords, or tokens to invalidate the old ones.
4.  **Inform collaborators.** If others have pulled the commit, they need to rebase their work after the history is rewritten.
5.  **Enable GitHub Push Protection.** This feature helps prevent accidental commits of known secrets. Find it in your repository settings under `Code security > Secret scanning > Push protection`.
6.  **Consider using `.env.example` or `.env.template`:** Include these files with placeholder values instead of committing the actual `.env` file.

**GitHub Push Protection Bypass (Use with Caution):**

*   If you accidentally commit secrets and GitHub blocks the push, it provides URLs to "Allow secret." This option should be used with extreme caution and only in private repositories where you trust all collaborators.

**Disabling Push Protection (Not Recommended):**

*   It is possible to disable Push Protection in your repository settings (`Code security > Secret scanning > Push protection`). However, this is generally **not recommended** as it reduces the security of your repository. Only disable it if you fully understand the risks.

**Recovering from Committing Secrets (Comprehensive Guide):**

If secrets are accidentally committed, the following steps must be taken to fully resolve the problem:

1.  **Add the secrets file to `.gitignore`**: Ensure that `.env`, `.env.local`, and similar files are included in `.gitignore` to prevent future commits.
2.  **Remove the secrets file from the repository:**

    ```bash
    git rm --cached .env
    git commit -m "Remove .env file"
    ```
3.  **Rewrite Git history (Crucial):** Use `git filter-branch` or, preferably, `git filter-repo` (if available) to remove the file from all past commits. This is the most secure option:

    ```bash
    git filter-branch --force --index-filter "git rm --cached --ignore-unmatch .env" --prune-empty --tag-name-filter cat -- --all
    ```

    Or, using `git filter-repo` (requires installation):

   First, install git-filter-repo:

    ```bash
    pip install git-filter-repo
    ```

   Then, run the filter:

    ```bash
    git filter-repo --invert-blob-contents --path .env
    ```

    *   **Important:** Rewriting Git history can be complex and may affect collaborators. Communicate with your team before performing these steps.
4.  **Force Push:** After rewriting history, force push the changes:

    ```bash
    git push origin master --force
    ```
5.  **Stash Changes (If Necessary):** If there are unstaged changes, stash them before rebasing or filtering:

    ```bash
    git stash
    ```

    And pop them after the history rewrite:

    ```bash
    git stash pop
    ```
6.  **Verify `.env` is ignored:**

    ```bash
    git check-ignore -v .env
    ```

    This command should confirm that `.env` is now ignored by Git.
7.  **Inform Collaborators:** If others have pulled the commit, they need to rebase their work after the history is rewritten.

### .gitignore Rules
Always add the following to `.gitignore`:
*   `.cursor/progress/` to prevent accidental commits of temporary Cursor files that may contain secrets.

## DEBUGGING

When debugging, especially for E2E tests, consider running tests in parallel to speed up the process. For example, use sequential thinking to organize your actions.

When running pytest in parallel, use the following flags:

*   `-n <num_workers>`: Specifies the number of workers for parallel execution.
*   `-v`: Enables verbose output.
*   `--tb=long`: Uses a **LONG** traceback format (NEVER use --tb=short for debugging)
*   `--dist=loadscope`: Distributes tests by fixture scope (safer than `loadscope`).
*   `--dist=loadfile`: Distributes tests by file (use if fixtures are function-scoped).

When the test `test_parallel_agent_execution` fails, the problem may be the threshold.
*   Check the tests and see if the tests and see if the threshold is too restrictive.
*   Consider the fact that the workflow includes routing, agents, synthesis and judge, and the threshold must account for those steps.

### Diagnosing Windows Clipboard Issues

If `Ctrl+C` and `Ctrl+V` are not working correctly on a Windows machine, follow these steps to diagnose and resolve the problem:

1.  **Test Globally:**
    *   Check if the issue is specific to Cursor IDE or a system-wide problem. Test in Notepad and a web browser.

2.  **Cursor-Specific Solutions:**
    *   **Verify Keyboard Shortcuts:**
        *   In Cursor, press `Ctrl + Shift + P` to open the Command Palette.
        *   Type "Preferences: Open Keyboard Shortcuts".
        *   Search for "copy" and "paste" to ensure they are mapped to `Ctrl+C` and `Ctrl+