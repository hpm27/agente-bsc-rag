---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
version: "2.7"
last_updated: "2025-10-20"
---

## HEADERS

## TECH STACK

### Test Methodology (Validado Out/2025 - ROI: 32-60 min economizados por implementação)

**ANTES de escrever QUALQUER teste (unitário ou E2E), aplicar [[memory:9969868]] - CHECKLIST OBRIGATÓRIO de 15 pontos + SUB-PONTO 15.6**:

**PONTOS 1-8: Testes Unitários** (validado FASE 2.5)

1. **Ler assinatura completa**: `grep "def method_name" src/file.py -A 10` (NUNCA assumir)
2. **Verificar tipo retorno**: Objeto Pydantic ou built-in? Campos obrigatórios?
3. **Contar parâmetros**: Quantos params (não contar `self`)?
4. **Validações pré-flight**: Validações no código + Pydantic (min_length, Literal, validators)
5. **Entender decorators**: @retry com `reraise=True` relança exceção original (NÃO RetryError)
6. **Fixtures Pydantic**: NUNCA passar `None` para `default_factory`. Incluir campos obrigatórios.
7. **Dados válidos em mocks**: Usar MARGEM DE SEGURANÇA (ex: min_length=20 → usar 50+ chars)
8. **Verificar método correto**: Confirmar nome exato via grep (ex: `invoke()` não `process_query()`)

**PONTOS 9-12: E2E Workflow Tests** (validado FASE 2.6) - Ver seção "E2E Workflow Tests" below for details

**Debugging pytest (Validado [[memory:9969628]] e [[memory:10012853]])**:
*   **SEMPRE** usar `--tb=long` (traceback completo, NUNCA --tb=short)
*   **NUNCA** usar filtros (`Select-Object`, `Select-String`) - oculta informação crítica
*   Comando correto: `pytest tests/test_file.py -v --tb=long 2>&1`

**pytest em paralelo** (performance):
*   `-n <num_workers>`: Workers paralelos
*   `-v`: Verbose output
*   `--tb=long`: Traceback completo
*   `--dist=loadscope`: Distribui por fixture scope (mais seguro)
*   `--dist=loadfile`: Distribui por arquivo (fixtures function-scoped)

**Lições Detalhadas**:
- `docs/lessons/lesson-test-debugging-methodology-2025-10-15.md` (FASE 2.4, 5 erros)
- `docs/lessons/lesson-diagnostic-agent-test-methodology-2025-10-16.md` (FASE 2.5, 7 erros, 1.100+ linhas)
- `docs/lessons/lesson-onboarding-state-e2e-tests-2025-10-16.md` (FASE 2.6, 4 problemas E2E, 11.900+ linhas)
- `docs/lessons/lesson-swot-testing-methodology-2025-10-19.md` (FASE 3.1, Implementation-First Testing, APIs desconhecidas, 700+ linhas, 30-40 min economizados)
- `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
- `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys applied + PONTO 15 fixtures Pydantic)
- `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md` (950+ linhas, 5 Whys + Hypothesis Property-Based Testing discovery, SUB-PONTO 15.6)
- `docs/lessons/lesson-testes-reais-do-projeto.md`

**ROI Comprovado**: Aplicar checklist ANTES economiza 38 minutos de debugging por implementação (validado FASE 2.5 unitário, FASE 2.6 E2E).

---

### Property-Based Testing com Hypothesis para Pydantic (Descoberta Out/2025)

**Context**: Mainstream solution 2024-2025 para validação automática de fixtures Pydantic. Descoberto em research Brightdata durante Sessão 21 (Benchmarking Tool) após identificar problema recorrente de fixtures inválidas (6 sessões consecutivas).

**QUANDO USAR**: Validators Pydantic complexos (min_length, ranges, field_validator customizados, Literal com múltiplos valores)

**FERRAMENTA**: Hypothesis (https://hypothesis.readthedocs.io) - Property-based testing framework oficial Pydantic

**Pattern Validado** (Pydantic AI Docs 2024):
```python
from hypothesis import given
from hypothesis.strategies import from_type
from pydantic import BaseModel

class BenchmarkComparison(BaseModel):
    perspective: str
    gap: float = Field(ge=-100, le=200)
    gap_type: Literal["positive", "negative", "neutral"]
    benchmark_source: str = Field(min_length=20)
    insight: str = Field(min_length=50)

# Hypothesis gera 100+ fixtures válidas AUTOMATICAMENTE
@given(comparison=from_type(BenchmarkComparison))
def test_benchmark_comparison_properties(comparison):
    """Property-based test: QUALQUER BenchmarkComparison válido deve satisfazer propriedades."""
    # Testa propriedades em CENTENAS de casos gerados
    assert len(comparison.insight) >= 50  # Hypothesis respeita min_length!
    assert comparison.gap_type in ["positive", "negative", "neutral"]
    assert -100 <= comparison.gap <= 200
```

**Benefícios** (validado Pydantic AI docs + CodiLime 2024):
- ✅ **Gera centenas de fixtures válidas automaticamente** (100+ exemplos por default)
- ✅ **Respeita validators Pydantic** (min_length, Literal, Field constraints, @field_validator)
- ✅ **Encontra edge cases** que testes manuais não cobrem (valores extremos: gap=199.9, gap=-99.9)
- ✅ **Reduz manutenção** - fixtures atualizam automaticamente quando schema Pydantic muda
- ✅ **Detecta bugs em validators** - se Hypothesis gera fixture inválida que passa, validator está bugado!

**Limitações**:
- ❌ Requer aprendizado de `strategies` API (curva aprendizado ~2h)
- ❌ Testes mais lentos (gera 100+ exemplos vs 1 fixture manual)
- ❌ Não substitui testes de negócio específicos (complementa)
- ❌ Fixtures geradas são aleatórias (não reproduzíveis sem seed)

**Quando NÃO usar** (usar fixtures manuais):
- ❌ Casos de negócio BSC específicos (ex: 4 perspectivas balanceadas, KPIs setoriais específicos)
- ❌ Testes rápidos CI/CD (Hypothesis adiciona ~5-10s por teste)
- ❌ Testes de regressão (precisa reproduzir cenário exato)

**Decision Matrix**:

| Cenário | Hypothesis | Fixtures Manuais | Justificativa |
|---------|-----------|------------------|---------------|
| **Validators complexos** | ✅ | ❌ | Testa 100+ combinações automaticamente |
| **Casos BSC específicos** | ❌ | ✅ | Controle exato do cenário de negócio |
| **Schemas em mudança** | ✅ | ❌ | Fixtures atualizam automaticamente |
| **CI/CD rápido** | ❌ | ✅ | Hypothesis +5-10s por teste |
| **Edge cases desconhecidos** | ✅ | ❌ | Descobre bugs não pensados |
| **Testes regressão** | ❌ | ✅ | Reproduz cenário exato |

**Recomendação: Abordagem Híbrida** (validada CodiLime + Semaphore 2024):
- ✅ Hypothesis para validators Pydantic (min_length, ranges, Literal, field_validator)
- ✅ Fixtures manuais para casos de negócio BSC (4 perspectivas, 10-15 KPIs balanceados)

**Exemplo Concreto Benchmarking Tool**:
```python
# Hypothesis para testar VALIDATOR (gap_type alinhado com gap)
@given(
    gap=st.floats(min_value=-100, max_value=200),
    gap_type=st.sampled_from(["positive", "negative", "neutral"])
)
def test_gap_type_validator_properties(gap, gap_type):
    """Property: gap_type deve alinhar com gap (validator customizado)."""
    try:
        comparison = BenchmarkComparison(gap=gap, gap_type=gap_type, ...)
        # Se passou, validar alinhamento
        if gap_type == "negative":
            assert gap >= 5  # Validator customizado threshold
    except ValidationError:
        # Se falhou, verificar se falha é correta
        if gap_type == "negative" and gap < 5:
            pass  # Falha esperada ✅
        else:
            pytest.fail("ValidationError inesperado!")

# Fixture manual para caso BSC específico (4 perspectivas balanceadas)
@pytest.fixture
def valid_benchmark_report_bsc():
    """Caso BSC específico: 2 comparações por perspectiva (balanceado)."""
    return BenchmarkReport(
        comparisons=[
            # 2 Financeira (setor específico)
            BenchmarkComparison(perspective="Financeira", metric_name="Margem EBITDA", ...),
            BenchmarkComparison(perspective="Financeira", metric_name="ROI", ...),
            # 2 Clientes (B2B SaaS)
            BenchmarkComparison(perspective="Clientes", metric_name="NPS", ...),
            BenchmarkComparison(perspective="Clientes", metric_name="Churn", ...),
            # 2 Processos
            BenchmarkComparison(perspective="Processos Internos", metric_name="Lead Time", ...),
            BenchmarkComparison(perspective="Processos Internos", metric_name="Cycle Time", ...),
            # 2 Aprendizado
            BenchmarkComparison(perspective="Aprendizado e Crescimento", metric_name="Retenção", ...),
            BenchmarkComparison(perspective="Aprendizado e Crescimento", metric_name="Treinamento", ...)
        ]
    )
```

**ROI Esperado**:
- **Hypothesis**: Economiza 15-20 min testando validators (100+ casos vs 3-5 manuais)
- **Fixtures manuais**: Economiza 10-15 min em casos BSC (controle exato)
- **Híbrido**: Melhor dos dois mundos (validators automáticos + negócio controlado)

**Instalação**:
```bash
pip install hypothesis pytest-hypothesis
```

**Fontes Validadas** (Brightdata research Out/2025):
1. Pydantic AI Docs - Hypothesis Integration (https://ai.pydantic.dev/testing/)
2. CodiLime - Testing APIs with Pytest (Oct 2024)
3. Semaphore - Property-Based Testing Python Hypothesis (Jan 2023)
4. Hypothesis Documentation (https://hypothesis.readthedocs.io/)

**Lição Completa**: `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md` (950+ linhas, 5 Whys Root Cause Analysis, Hypothesis discovery, SUB-PONTO 15.6 validado)

---

#### E2E Workflow Tests (Validado FASE 2.6 - Out/2025)

**Context**: Testes E2E de workflows LangGraph multi-turn stateless requerem 4 considerações adicionais ao checklist base.

**4 Pontos Adicionais (Checklist expandido: 8 → 12 pontos)**:

**9. Verificar property/método existe**:
*   **SEMPRE** usar `grep "@property\|def method_name" src/file.py` antes de usar em código
*   **Exemplo validado (2025-10-16)**: Assumiu `client_profile_agent` property existia → `AttributeError`
*   **Prevenção**: 
    ```bash
    grep "@property" src/graph/workflow.py | grep "client_profile_agent"
    # Se não retornar nada, property não existe - criar ou usar alternativa
    ```
*   **ROI**: 5-8 min economizados

**10. Considerar persistência de state**:
*   **SEMPRE** perguntar: "Como state persiste entre múltiplos `run()` calls?"
*   **Exemplo validado (2025-10-16)**: Workflow stateless, `onboarding_progress` perdido entre calls
*   **Solução padrão**: In-memory sessions dict
    ```python
    class BSCWorkflow:
        def __init__(self):
            self._onboarding_sessions: dict[str, dict[str, Any]] = {}
        
        def handler(self, state):
            user_id = state.user_id
            # Load session
            session = self._onboarding_sessions.get(user_id, {})
            # Process...
            # Save session
            self._onboarding_sessions[user_id] = updated_session
            # Cleanup (se completo)
            if is_complete:
                del self._onboarding_sessions[user_id]
    ```
*   **ROI**: 20-30 min economizados (pattern reutilizável)

**11. Fixtures Pydantic com ID customizado**:
*   **SEMPRE** criar profile inline quando teste precisa `client_id` específico
*   **Exemplo validado (2025-10-16)**: Fixture `client_id='fixture'`, teste esperava `'test_005'` → assertion falhou
*   **Template**:
    ```python
    test_profile = ClientProfile(
        client_id="test_cliente_specific_id",  # Match user_id do teste
        company=valid_client_profile.company,  # Reutilizar outros campos
        context=valid_client_profile.context,
        # ... demais campos do fixture original
    )
    ```
*   **ROI**: 8-12 min economizados

**12. Teste de regressão crítico OBRIGATÓRIO**:
*   **SEMPRE** incluir 1 teste validando que funcionalidade existente NÃO quebrou
*   **Exemplo validado (2025-10-16)**: `test_rag_workflow_cliente_existente_nao_quebrado`
    *   Validou que cliente existente (phase=DISCOVERY) usa RAG tradicional
    *   Routing correto, workflow completo, zero breaking changes
*   **Template**:
    ```python
    def test_existing_functionality_not_broken():
        """CRÍTICO: Validar que funcionalidade X não quebrou com nova feature Y.
        
        Este teste previne REGRESSÃO!
        """
        # Setup: Cliente/estado existente (não novo)
        mock_existing_state()
        
        # Action: Executar workflow tradicional
        result = workflow.run_traditional_flow(...)
        
        # Assert: Comportamento mantido
        assert traditional_method.called
        assert not new_feature_method.called  # Nova feature NÃO interferiu
    ```
*   **ROI**: Crítico (previne breaking changes, economiza horas de rollback)

**13. Implementation-First Testing para APIs desconhecidas (validado FASE 3.1 - 2025-10-19)**
*   Quando implementar tool/agent NOVO com API desconhecida, SEMPRE ler implementação ANTES de escrever testes. 
*   Pattern: (1) `grep "def " src/module/file.py` (descobre métodos disponíveis), (2) `grep "def method_name" src/module/file.py -A 15` (lê signature completa), (3) `grep "class SchemaName" src/memory/schemas.py -A 30` (verifica schemas Pydantic usados), (4) Escrever testes alinhados com API real.
*   RAZÃO: TDD funciona quando você CONHECE a API. Para APIs desconhecidas, assumir estrutura causa reescrita completa de testes.
*   **ROI**: 30-40 min economizados por implementação.
*   **QUANDO USAR**: APIs novas (tools consultivas FASE 3+), agentes novos, integrações complexas (RAG, LLM, multi-step).
*   **QUANDO NÃO USAR** (TDD tradicional melhor): API conhecida, lógica simples (math, pure functions), refactoring (testes já existem).
*   **Lição detalhada**: `docs/lessons/lesson-swot-testing-methodology-2025-10-19.md` (700+ linhas com checklist completo).

**14. Mock Múltiplas Chamadas com itertools.cycle**:
*   **Mock Múltiplas Chamadas com itertools.cycle**: Mock para múltiplas chamadas sequenciais requer abordagem dinâmica. Mock estático (`return_value` fixo) causa falhas quando tool processa lista de items esperando outputs diferentes.
*   **QUANDO USAR**: Mock será invocado N vezes (N > 1) esperando outputs diferentes a cada chamada.
*   **RAZÃO**: Mock estático retorna sempre o mesmo valor. Tools que processam lista items (ex: 4 perspectivas BSC) falham com ValidationError quando mock não diferencia outputs.
*   **SOLUÇÃO VALIDADA**: `itertools.cycle` para retornar valores sequencialmente.
*   **Template Geral**:
    ```python
    from itertools import cycle

    @pytest.fixture
    def mock_with_cycle():
        """Mock com itertools.cycle - múltiplos valores sequenciais."""
        mock_obj = MagicMock()
        
        # Step 1: Definir valores esperados em ordem
        values_order = [value1, value2, value3, value4]
        values_cycle = cycle(values_order)
        
        # Step 2: side_effect usa next(cycle)
        def side_effect_fn(*args, **kwargs):
            """Retorna próximo valor na sequência."""
            return next(values_cycle)
        
        mock_obj.method.side_effect = side_effect_fn
        return mock_obj
    ```
*   **Exemplo Real - KPI Definer Tool** (validado 2025-10-19):
    ```python
    from itertools import cycle
    from pydantic import BaseModel

    @pytest.fixture
    def mock_llm() -> MagicMock:
        """Mock LLM que retorna KPIs com perspectiva correta sequencialmente."""
        llm = MagicMock(spec=["invoke", "with_structured_output"])
        
        class KPIListOutput(BaseModel):
            kpis: list[KPIDefinition]
    
        # Define KPIs mock para cada perspectiva
        kpis_by_perspective = {
            "Financeira": [KPIDefinition(name="ARR", perspective="Financeira", ...)],
            "Clientes": [KPIDefinition(name="NPS", perspective="Clientes", ...)],
            "Processos Internos": [KPIDefinition(name="Lead Time", perspective="Processos Internos", ...)],
            "Aprendizado e Crescimento": [KPIDefinition(name="Retencao", perspective="Aprendizado e Crescimento", ...)]
        }
        
        # SOLUCAO: itertools.cycle alinhado com ordem de chamadas da tool
        # Tool chama: Financeira → Clientes → Processos → Aprendizado (linhas 152-156)
        perspective_order = [
            "Financeira",
            "Financeira",
            "Clientes",
            "Processos Internos",
            "Aprendizado e Crescimento"
        ]
        perspective_cycle = cycle(perspective_order)
        
        def mock_invoke_side_effect(prompt: str):
            """Retorna KPIs da próxima perspectiva na sequência."""
            perspective = next(perspective_cycle)
            kpis = kpis_by_perspective[perspective]
            return KPIListOutput(kpis=kpis)
        
        mock_structured_llm = MagicMock()
        mock_structured_llm.invoke.side_effect = mock_invoke_side_effect
        llm.with_structured_output.return_value = mock_structured_llm
        return llm
    ```
*   **Pergunta-Chave ANTES de criar mock**:
    > "Quantas vezes o mock será invocado e com quais diferenças nos outputs esperados?"
*   **Decisão Tree**:
    - Mock invocado **1x** OU **N vezes com MESMO output**? → `return_value` fixo OK
    - Mock invocado **N vezes com OUTPUTS DIFERENTES**? → `itertools.cycle` ou `side_effect` lista
    - Mock invocado **N vezes exatas** (N == len(lista))? → `side_effect = [val1, val2, val3]`
    - Mock invocado **N vezes indefinido** OU **N > len(values)**? → `itertools.cycle` (melhor escolha)
*   **Alternativas e Trade-offs**:
    1. **Lista finita** (`side_effect = [val1, val2, val3]`):
       - ✅ Simples, direto
       - ❌ Erro `StopIteration` se chamadas > len(lista)
       - **Usar quando**: N chamadas exatas conhecido
    2. **Callable com contador**:
       ```python
       call_count = 0
       def side_effect():
           nonlocal call_count
           call_count += 1
           return values[call_count - 1]
       ```
       - ✅ Controle explícito
       - ❌ Mais verboso, `nonlocal` não pythônico
       - **Usar quando**: Lógica complexa por chamada
    3. **itertools.cycle** (RECOMENDADO):
       - ✅ Pythônico, elegante
       - ✅ Nunca lança `StopIteration` (ciclo infinito)
       - ✅ Alinha naturalmente com ordem de chamadas da tool
       - ❌ Ciclo repete (pode mascarar bugs se tool chamar mais que esperado)
       - **Usar quando**: Mock múltiplas chamadas sequenciais (99% dos casos)
*   **Checklist Aplicação**:
    - [ ] Mock será invocado N vezes (N > 1)?
    - [ ] Outputs esperados são diferentes a cada chamada?
    - [ ] Tool processa lista de items sequencialmente?
    - [ ] Ordem do cycle alinha com ordem de chamadas da tool? (verificar código `for item in items:`)
    - [ ] `side_effect` usa `next(cycle)` corretamente?
    - [ ] Teste valida perspectiva/category/type correto em cada output?
*   **Antipadrões Evitados**:
    - ❌ **Mock estático com return_value fixo** para múltiplas chamadas
    - ❌ **String matching no prompt** para detectar qual output retornar (frágil, dependente de formato)
    - ❌ **Parsing JSON/XML** para diferenciar chamadas (adiciona complexidade desnecessária)
    - ❌ **Criar ValidationError manualmente** em Pydantic (preferir passar dados inválidos e deixar Pydantic validar)
*   **ROI**: 15-20 min economizados (debugging mock estático vs criação correta desde início)
*   **Aplicabilidade Futura**:
    - ✅ Objetivos Estratégicos Tool (múltiplos objetivos por perspectiva BSC)
    - ✅ Action Plan Tool (lista de ações sequenciais)
    - ✅ Benchmarking Tool (comparações múltiplas perspectivas)
    - ✅ Qualquer tool que processa lista items com LLM/API calls sequenciais
*   **Lições Detalhadas**:
    - `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
    - Memória [[memory:9969868]] ponto 14 (checklist obrigatório 14 pontos)

**15. LER SCHEMA PYDANTIC VIA GREP ANTES DE CRIAR FIXTURE OU ACESSAR CAMPOS (validado FASE 3.5 - 2025-10-19)**
*   PROBLEMA RESOLVIDO: Fixtures Pydantic inválidas (5 erros recorrentes 4 sessões: SWOT, Five Whys, Issue Tree, KPI, Strategic Obj), context builders acessando campos inexistentes (2 erros 3 sessões).
*   ROOT CAUSE: NÃO ler schema Pydantic ANTES de criar fixture/acessar campos, assumir estrutura baseada em memória.
*   **QUANDO APLICAR**: SEMPRE antes de: (1) Criar fixture Pydantic, (2) Escrever context builder, (3) Acessar campos de schema em qualquer código.
*   **COMO APLICAR** (5 sub-pontos detalhados em [[memory:9969868]]):
    1.  `grep "class SchemaName" src/memory/schemas.py -A 50` → Identificar campos obrigatórios/opcionais/Literal
    2.  `grep "min_length\|@field_validator" src/memory/schemas.py` → Identificar validators
    3. Validar nested schemas (schemas que agregam outros schemas)
    4. Criar fixture com campos validados (margem de segurança em min_length)
    5. Defensive programming em context builders (hasattr/getattr antes de acessar)
*   **ROI ESPERADO**: 30-40 min economizados por sessão (fixtures corretas primeira tentativa).
*   **ROI VALIDADO**: Sessão 20 teve 8 erros (1.5h debugging) que teriam sido prevenidos aplicando PONTO 15.
*   **Lição detalhada**: `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys Root Cause Analysis completa).

**15.6 IDENTIFICAR TODOS SCHEMAS PYDANTIC USADOS NO TESTE (Validado FASE 3.6 - 2025-10-19)**
*   PROBLEMA: PONTO 15 aplicado PARCIALMENTE (apenas em 1 de 4 schemas), fixtures inválidas passaram despercebidas.
*   ROOT CAUSE: Checklist não especifica "identificar TODOS schemas via grep imports do teste".
*   **QUANDO APLICAR**: SEMPRE antes de criar fixtures Pydantic OU escrever testes com múltiplos schemas
*   **COMO APLICAR** (5 sub-passos):
    1.  Grep imports do teste → listar todos schemas Pydantic usados
    2.  Grep CADA schema identificado (fields + constraints)
    3.  Grep validators de CADA schema (@field_validator, @model_validator)
    4.  Criar fixtures com margem +20%
    5.  Dry-run validation (try/except ValidationError em fixture)
*   **ROI ESPERADO**: 30-60 min economizados por sessão (fixtures corretas primeira tentativa).
*   **Lição detalhada**: `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md`

**16. Sequential Thinking + 5 Whys Root Cause Analysis para Debugging Persistente (Validado FASE 3.7 - 2025-10-20)**
*   **Context**: Erros complexos em workflows LangGraph (ex: loops infinitos, routing incorreto) requerem metodologia sistemática para identificar CAUSA RAIZ.
*   **QUANDO USAR**: Erro persiste mesmo após múltiplas tentativas de correção, logs confusos, dependências complexas (ex: Pydantic, LangGraph, LLMs, APIs).
*   **FERRAMENTAS**:
    1.  Sequential Thinking (MCP): Analisar problema passo a passo, gerar hipóteses, validar com evidências.
    2.  5 Whys Root Cause Analysis: Encontrar causa raiz perguntando "Por que?" 5 vezes (ou mais).
*   **COMO APLICAR (5 Whys Adaptado)**:
    1.  WHY 1: Por que o sistema falha? (Descrever o erro exato - ex: KeyError, TypeError, loop infinito)
    2.  WHY 2: Por que esse erro ocorre? (Identificar a função/linha de código exata)
    3.  WHY 3: Por que essa função/linha está sendo executada? (Rastrear o fluxo de execução)
    4.  WHY 4: Qual a entrada dessa função/linha? (Examinar o state Pydantic)
    5.  WHY 5: Por que essa entrada é inválida? (Causa raiz - ex: schema incorreto, valor não inicializado)
*   **Checklist Aplicação**:
    *   [ ] Descrever erro exato (TypeError, KeyError, loop infinito)
    *   [ ] Rastrear função/linha causadora do erro
    *   [ ] Entender fluxo de execução até o erro
    *   [ ] Examinar state Pydantic no ponto da falha
    *   [ ] Identificar causa raiz da entrada inválida
*   **Lições Detalhadas**:
    *   `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
*   **Exemplo Concreto (resolução loop infinito FASE 3.7)**:
    *   WHY 1: Loop infinito de refinamento (iteração 1/2 sempre)
    *   WHY 2: decide_next_step() chamado repetidamente
    *   WHY 3: Mutações em edge NÃO persistem no LangGraph
    *   WHY 4: state.refinement_iteration nunca incrementa
    *   WHY 5: judge_evaluation() (nó) NÃO retorna "refinement_iteration" no dict

**17. Checklist para evitar erros de incompatibilidade de parâmetros em chamadas de função/método:**
*   **QUANDO APLICAR**: SEMPRE que houver alteração na assinatura de uma função/método (adição, remoção ou alteração de parâmetros).
*   **COMO APLICAR**:
    1.  Listar TODOS os locais onde a função/método é chamado (usar `grep "method_name(" src/` para encontrar todas as chamadas).
    2.  Verificar se os parâmetros passados na chamada correspondem à assinatura atualizada da função/método.
    3.  Se a assinatura foi simplificada (parâmetros removidos), remover os parâmetros desnecessários na chamada.
    4.  Se a assinatura foi expandida (novos parâmetros), fornecer valores para os novos parâmetros na chamada (usar valores default si apropriado).
*   **ROI**: Economiza tempo de debugging e previne erros de `TypeError: unexpected keyword argument`.
*   **Lições Detalhadas**:
    *   `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging).
    *   `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys applied).
    *   `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md`
    *   `docs/lessons/lesson-testes-reais-do-projeto.md`

**18. Para garantir a correta inicialização e persistência do estado em workflows LangGraph, especialmente durante o processo de onboarding, siga este guia:**

*   **QUANDO APLICAR**: Principalmente em workflows com múltiplos passos (multi-turn), como o onboarding, onde a informação precisa ser coletada e persistida ao longo de várias interações com o usuário.
*   **COMO APLICAR:**
    1.  Certifique-se de que todos os dados necessários para a execução do workflow (incluindo o estado inicial) estejam corretamente inicializados no objeto de estado (BSCState).
    2.  Verifique se todos os nós do workflow que modificam o estado (funções que retornam um dicionário com as atualizações) estão retornando TODAS as informações necessárias, e não apenas as modificações. Isso garante que o estado completo seja persistido.
    3.  Nos casos em que o estado é carregado de uma fonte externa (ex: Mem0), verifique se todos os campos necessários são carregados corretamente. Se o estado não existir, inicialize-o com valores default.
    4.  Ao transitar entre diferentes fases do workflow (ex: ONBOARDING -> DISCOVERY), certifique-se de que todos os dados relevantes sejam transferidos para o novo estado.
*   **PROBLEMAS COMUNS**:
    *   `KeyError` devido a campos não inicializados no estado.
    *   Perda de estado entre interações.
    *   Comportamento inesperado devido a valores default incorretos.
*   **ROI**: Evita erros de tempo de execução, garante a persistência correta do estado e facilita a depuração.
*   **Lições Detalhadas**:
    *   `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging).
    *   `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys applied).
    *   `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md`
    *   `docs/lessons/lesson-testes-reais-do-projeto.md`
*   **Exemplo Concreto (resolução dos bugs #5, #7 e #9)**:
    * Antes da correção, o sistema não inicializava corretamente o `client_profile`, resultando em erros de `NoneType` ao acessar atributos. A correção envolveu inicializar o `client_profile` com os valores padrão antes de qualquer acesso, e garantir que todos os nós retornassem o estado completo.

**19. Sempre que encontrar erros de `TypeError: unexpected keyword argument` em chamadas de função, siga este checklist:**
*   **Contexto**: Este erro indica que uma função ou método está sendo chamado com um argumento de palavra-chave