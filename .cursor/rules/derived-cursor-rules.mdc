---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
version: "3.38"
last_updated: "2025-10-27"
---

## HEADERS

## TECH STACK

### Test Methodology (Validado Out/2025 - ROI: 32-60 min economizados por implementação)

**ANTES de escrever QUALQUER teste (unitário ou E2E), aplicar [[memory:9969868]] - CHECKLIST OBRIGATÓRIO de 15 pontos + SUB-PONTO 15.6**:

**PONTOS 1-8: Testes Unitários** (validado FASE 2.5)

1. **Ler assinatura completa**: `grep "def method_name" src/file.py -A 10` (NUNCA assumir)
2. **Verificar tipo retorno**: Objeto Pydantic ou built-in? Campos obrigatórios?
3. **Contar parâmetros**: Quantos params (não contar `self`)?
4. **Validações pré-flight**: Validações no código + Pydantic (min_length, Literal, validators)
5. **Entender decorators**: @retry com `reraise=True` relança exceção original (NÃO RetryError)
6. **Fixtures Pydantic**: NUNCA passar `None` para `default_factory`. Incluir campos obrigatórios.
7. **Dados válidos em mocks**: Usar MARGEM DE SEGURANÇA (ex: min_length=20 → usar 50+ chars)
8. **Verificar método correto**: Confirmar nome exato via grep (ex: `invoke()` não `process_query()`)

**PONTOS 9-12: E2E Workflow Tests** (validado FASE 2.6) - Ver seção "E2E Workflow Tests" below for details

**Debugging pytest (Validado [[memory:9969628]] e [[memory:10012853]])**:
*   **SEMPRE** usar `--tb=long` (traceback completo, NUNCA --tb=short)
*   **NUNCA** usar filtros (`Select-Object`, `Select-String`) - oculta informação crítica
*   Comando correto: `pytest tests/test_file.py -v --tb=long 2>&1`

**pytest em paralelo** (performance):
*   `-n <num_workers>`: Workers paralelos
*   `-v`: Verbose output
*   `--tb=long`: Traceback completo
*   `--dist=loadscope`: Distribui por fixture scope (mais seguro)
*   `--dist=loadfile`: Distribui por arquivo (fixtures function-scoped)

**Lições Detalhadas**:
- `docs/lessons/lesson-test-debugging-methodology-2025-10-15.md` (FASE 2.4, 5 erros)
- `docs/lessons/lesson-diagnostic-agent-test-methodology-2025-10-16.md` (FASE 2.5, 7 erros, 1.100+ linhas)
- `docs/lessons/lesson-onboarding-state-e2e-tests-2025-10-16.md` (FASE 2.6, 4 problemas E2E, 11.900+ linhas)
- `docs/lessons/lesson-swot-testing-methodology-2025-10-19.md` (FASE 3.1, Implementation-First Testing, APIs desconhecidas, 700+ linhas, 30-40 min economizados)
- `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
- `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys applied + PONTO 15 fixtures Pydantic)
- `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md` (950+ linhas, 5 Whys + Hypothesis Property-Based Testing discovery, SUB-PONTO 15.6)
- `docs/lessons/lesson-testes-reais-do-projeto.md`
- `docs/lessons/lesson-async-parallelization-langgraph-2025-10-20.md` - Async/Await & LangGraph State (FASE 1 Onboarding, 4 problemas críticos, GIL+threads, nested loops, state mutation, Mem0 v2, 950+ linhas, ROI 35-85% speedup, checklists completos)
- `docs/lessons/lesson-asyncio-streamlit-resource-warnings-2025-10-20.md` - ResourceWarning, nest_asyncio, loop.create_task()
- `docs/lessons/lesson-streamlit-module-cache-2025-10-21.md` - Cache de módulos Streamlit (RESOLVIDO, 317+ linhas, Pydantic nested, 5 Whys, Brightdata research, 75 min economizados)
- `docs/lessons/lesson-streamlit-ui-debugging-2025-10-22.md` - Streamlit UI & Debugging Complexo (FASE 2.7, 7 problemas, Sequential Thinking, Brightdata research, prompt schema alignment, UI best practices, 800+ linhas, ROI 50-67% debugging)
- `docs/lessons/lesson-e2e-testing-debugging-methodology-2025-10-27.md` - Metodologia de Debugging E2E com APIs Externas (FASE 3.10, Sequential Thinking + Brightdata research + debugging estruturado, issue #3284 Mem0, parsing defensivo, ROI 50-70% economia tempo)

**ROI Comprovado**: Aplicar checklist ANTES economiza 38 minutos de debugging por implementação (validado FASE 2.5 unitário, FASE 2.6 E2E).

---

### Property-Based Testing com Hypothesis para Pydantic (Descoberta Out/2025)

**Context**: Mainstream solution 2024-2025 para validação automática de fixtures Pydantic. Descoberto em research Brightdata durante Sessão 21 (Benchmarking Tool) após identificar problema recorrente de fixtures inválidas (6 sessões consecutivas).

**QUANDO USAR**: Validators Pydantic complexos (min_length, ranges, field_validator customizados, Literal com múltiplos valores)

**FERRAMENTA**: Hypothesis (https://hypothesis.readthedocs.io) - Property-based testing framework oficial Pydantic

**Pattern Validado** (Pydantic AI Docs 2024):
```python
from hypothesis import given
from hypothesis.strategies import from_type
from pydantic import BaseModel

class BenchmarkComparison(BaseModel):
    perspective: str
    gap: float = Field(ge=-100, le=200)
    gap_type: Literal["positive", "negative", "neutral"]
    benchmark_source: str = Field(min_length=20)
    insight: str = Field(min_length=50)

# Hypothesis gera 100+ fixtures válidas AUTOMATICAMENTE
@given(comparison=from_type(BenchmarkComparison))
def test_benchmark_comparison_properties(comparison):
    """Property-based test: QUALQUER BenchmarkComparison válido deve satisfazer propriedades."""
    # Testa propriedades em CENTENAS de casos gerados
    assert len(comparison.insight) >= 50  # Hypothesis respeita min_length!
    assert comparison.gap_type in ["positive", "negative", "neutral"]
    assert -100 <= comparison.gap <= 200
```

**Benefícios** (validado Pydantic AI docs + CodiLime 2024):
- ✅ **Gera centenas de fixtures válidas automaticamente** (100+ exemplos por default)
- ✅ **Respeita validators Pydantic** (min_length, Literal, Field constraints, @field_validator)
- ✅ **Encontra edge cases** que testes manuais não cobrem (valores extremos: gap=199.9, gap=-99.9)
- ✅ **Reduz manutenção** - fixtures atualizam automaticamente quando schema Pydantic muda
- ✅ **Detecta bugs em validators** - se Hypothesis gera fixture inválida que passa, validator está bugado!

**Limitações**:
- ❌ Requer aprendizado de `strategies` API (curva aprendizado ~2h)
- ❌ Testes mais lentos (gera 100+ exemplos vs 1 fixture manual)
- ❌ Não substitui testes de negócio específicos (complementa)
- ❌ Fixtures geradas são aleatórias (não reproduzíveis sem seed)

**Quando NÃO usar** (usar fixtures manuais):
- ❌ Casos de negócio BSC específicos (ex: ex: 4 perspectivas balanceadas, KPIs setoriais específicos)
- ❌ Testes rápidos CI/CD (Hypothesis adiciona ~5-10s por teste)
- ❌ Não substitui testes de regressão (precisa reproduzir cenário exato)

**Decision Matrix**:

| Cenário | Hypothesis | Fixtures Manuais | Justificativa |
|---------|-----------|------------------|---------------|
| **Validators complexos** | ✅ | ❌ | Testa 100+ combinações automaticamente |
| **Casos BSC específicos** | ❌ | ✅ | Controle exato do cenário de negócio |
| **Schemas em mudança** | ✅ | ❌ | Fixtures atualizam automaticamente |
| **CI/CD rápido** | ❌ | ✅ | Hypothesis +5-10s por teste |
| **Edge cases desconhecidos** | ✅ | ❌ | Descobre bugs não pensados |
| **Testes regressão** | ❌ | ✅ | Reproduz cenário exato |

**Recomendação: Abordagem Híbrida** (validada CodiLime + Semaphore 2024):
- ✅ Hypothesis para validators Pydantic (min_length, ranges, field_validator)
- ✅ Fixtures manuais para casos de negócio BSC (4 perspectivas, 10-15 KPIs balanceados)

**Exemplo Concreto Benchmarking Tool**:
```python
# Hypothesis para testar VALIDATOR (gap_type alinhado com gap)
@given(
    gap=st.floats(min_value=-100, max_value=200),
    gap_type=st.sampled_from(["positive", "negative", "neutral"])
)
def test_gap_type_validator_properties(gap, gap_type):
    """Property: gap_type deve alinhar com gap (validator customizado)."""
    try:
        comparison = BenchmarkComparison(gap=gap, gap_type=gap_type, ...)
        # Se passou, validar alinhamento
        if gap_type == "negative":
            assert gap >= 5  # Validator customizado threshold
    except ValidationError:
        # Se falhou, verificar se falha é correta
        if gap_type == "negative" and gap < 5:
            pass  # Falha esperada ✅
        else:
            pytest.fail("ValidationError inesperado!")

# Fixture manual para caso BSC específico (4 perspectivas balanceadas)
@pytest.fixture
def valid_benchmark_report_bsc():
    """Caso BSC específico: 2 comparações por perspectiva (balanceado)."""
    return BenchmarkReport(
        comparisons=[
            # 2 Financeira (setor específico)
            BenchmarkComparison(perspective="Financeira", metric_name="Margem EBITDA", ...),
            BenchmarkComparison(perspective="Financeira", metric_name="ROI", ...),
            # 2 Clientes (B2B SaaS)
            BenchmarkComparison(perspective="Clientes", metric_name="NPS", ...),
            BenchmarkComparison(perspective="Clientes", metric_name="Churn", ...),
            # 2 Processos
            BenchmarkComparison(perspective="Processos Internos", metric_name="Lead Time", ...),
            BenchmarkComparison(perspective="Processos Internos", metric_name="Cycle Time", ...),
            # 2 Aprendizado
            BenchmarkComparison(perspective="Aprendizado e Crescimento", metric_name="Retenção", ...),
            BenchmarkComparison(perspective="Aprendizado e Crescimento", metric_name="Treinamento", ...)
        ]
    )
```

**ROI Esperado**:
- **Hypothesis**: Economiza 15-20 min testando validators (100+ casos vs 3-5 manuais)
- **Fixtures manuais**: Economiza 10-15 min em casos BSC (controle exato)
- **Híbrido**: Melhor dos dois mundos (validators automáticos + negócio controlado)

**Instalação**:
```bash
pip install hypothesis pytest-hypothesis
```

**Fontes Validadas** (Brightdata research Out/2025):
1. Pydantic AI Docs - Hypothesis Integration (https://ai.pydantic.dev/testing/)
2. CodiLime - Testing APIs with Pytest (Oct 2024)
3. Semaphore - Property-Based Testing Python Hypothesis (Jan 2023)
4. Hypothesis Documentation (https://hypothesis.readthedocs.io/)

**Lição Completa**: `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md` (950+ linhas, 5 Whys Root Cause Analysis, Hypothesis discovery, SUB-PONTO 15.6 validado)

---

### Metodologia de Debugging E2E com APIs Externas (Validado Out/2025 - ROI: 50-70% economia tempo)

**CONTEXTO**: Problema recorrente em 80% dos projetos com integrações - testes E2E falhando silenciosamente com APIs externas. Metodologia validada na FASE 3.10 (Mem0 Platform integration).

**QUANDO USAR**: Testes E2E com APIs externas, integrações terceiros, debugging de problemas de comunicação entre sistemas.

**METODOLOGIA VALIDADA** (3 componentes):

**1. Sequential Thinking para Planejamento Estruturado**
```
Thought 1: Identificar problema específico (ex: get_tool_output retorna None)
Thought 2: Listar possíveis causas (timing, estrutura dados, filtros API)
Thought 3: Pesquisar soluções da comunidade (GitHub issues, Brightdata)
Thought 4: Implementar solução baseada em evidências (workaround validado)
Thought 5: Validar e documentar (logs, testes, lição aprendida)
```
**ROI**: Planejamento estruturado vs debugging aleatório = 60% economia tempo.

**2. Brightdata Research para Soluções da Comunidade**
- **GitHub Issues**: Fonte valiosa de problemas conhecidos (ex: Mem0 Issue #3284)
- **Best Practices 2025**: E2E testing com APIs externas, debugging estruturado
- **Workarounds Validados**: Filtros manuais, parsing defensivo, retry logic

**ROI**: Soluções baseadas em evidências vs tentativa-e-erro = 70% economia tempo.

**3. Debugging Estruturado com Logs Explícitos**
```python
# Logs defensivos para APIs externas
print(f"[DEBUG] API Response Type: {type(response)}")
print(f"[DEBUG] API Response Content: {response}")
print(f"[DEBUG] Parsed Data: {parsed_data}")

# Parsing defensivo para estruturas imprevisíveis
if isinstance(response, dict) and 'results' in response:
    data_list = response['results']
elif isinstance(response, list):
    data_list = response
else:
    data_list = [response] if response else []
```
**ROI**: Debugging direcionado vs investigação cega = 50% economia tempo.

**PROBLEMAS RECORRENTES IDENTIFICADOS**:

**1. APIs Externas com Documentação Incompleta (80% frequência)**
- **Sintoma**: Estrutura de resposta diferente do esperado
- **Solução**: Sempre testar estrutura antes de assumir formato
- **Implementação**: Parsing defensivo + logs explícitos

**2. Filtros/Metadata de APIs Não Funcionando (60% frequência)**
- **Sintoma**: Filtros retornam resultados vazios sem erro
- **Solução**: Workaround com filtro manual após busca ampla
- **Implementação**: Fallback para métodos alternativos

**3. Testes E2E com APIs Externas Frágeis (90% frequência)**
- **Sintoma**: Testes falham intermitentemente, timing issues
- **Solução**: Sandbox environments + retry logic + dados únicos
- **Implementação**: Testes integrados (salvar + recuperar no mesmo teste)

**CHECKLIST OBRIGATÓRIO** (baseado em Best Practices 2025):

**Pré-Teste**:
- [ ] Pesquisar issues conhecidos da API no GitHub
- [ ] Verificar documentação oficial da API
- [ ] Implementar parsing defensivo para estrutura de resposta
- [ ] Configurar logs de debug explícitos

**Durante o Teste**:
- [ ] Usar Sequential Thinking para planejamento
- [ ] Executar testes com `--tb=long` para debugging completo
- [ ] Implementar retry logic para falhas temporárias
- [ ] Usar client_ids únicos para evitar conflitos

**Pós-Teste**:
- [ ] Documentar problemas encontrados e soluções
- [ ] Criar workarounds baseados em evidências
- [ ] Atualizar testes para serem mais resilientes
- [ ] Compartilhar lições aprendidas com a equipe

**EXEMPLO CONCRETO** (Mem0 Platform Issue #3284):
```python
# ANTES (falhava silenciosamente)
filters = {"AND": [{"user_id": client_id}]}
memories = self.client.get_all(filters=filters)

# DEPOIS (funciona com workaround)
filters = {"AND": [{"user_id": client_id}]}
memories = self.client.get_all(filters=filters)

# Parsing defensivo da estrutura
if isinstance(memories, dict) and 'results' in memories:
    memories_list = memories['results']
elif isinstance(memories, list):
    memories_list = memories
else:
    memories_list = [memories] if memories else []
```

**MÉTRICAS DE SUCESSO VALIDADAS**:
- **Tempo de Resolução**: 6-8h estimado → 3h real (50-60% economia)
- **Taxa de Sucesso**: 100% problemas identificados e resolvidos
- **Qualidade**: Soluções baseadas em evidências da comunidade

**FONTES**:
- `docs/lessons/lesson-e2e-testing-debugging-methodology-2025-10-27.md`
- Mem0 Issue #3284 (GitHub)
- Bunnyshell E2E Testing Best Practices 2025
- Refold API Integration Testing Guide 2025

**ROI TOTAL**: 50-70% economia tempo debugging, metodologia replicável para futuras sessões E2E.

---

### LangChain Structured Output - Prompt Schema Alignment (Validado Out/2025 - ROI: 1-2h economizadas por sessão)

**PROBLEMA RECORRENTE**: ValidationError em campos obrigatórios Pydantic mesmo usando `with_structured_output()` (3+ sessões FASE 2).

**ROOT CAUSE DESCOBERTA (Brightdata Nov 2024 - leocon.dev):**
> "LLM segue EXEMPLO do prompt PRIMEIRO. O schema Pydantic fornece validação, mas se o exemplo do prompt omite um campo obrigatório, o LLM provavelmente omitirá também."

**EXEMPLO CONCRETO (Sessão Out 22, 2025):**
```python
# Schema Pydantic:
class Recommendation(BaseModel):
    impact: Literal["HIGH", "MEDIUM", "LOW"] = Field(description="Impacto esperado")

# Prompt original (ERRADO):
"""
RETORNE JSON:
{
    "title": "...",
    "expected_impact": "HIGH"  # ❌ Campo ERRADO! Schema usa 'expected_impact'
}
"""

# Resultado: LLM retorna JSON sem campo 'impact' → ValidationError após 3 retries (~9 min)
```

**BEST PRACTICE VALIDADA (comunidade 2024-2025):**

**1. Field() com Examples:**
```python
class Recommendation(BaseModel):
    impact: Literal["HIGH", "MEDIUM", "LOW"] = Field(
        description="Impacto esperado da recomendação",
        examples=["HIGH", "MEDIUM", "LOW"]  # ✅ Ajuda LLM entender opções
    )
    timeframe: str = Field(
        description="Prazo estimado",
        examples=["3-6 meses", "quick win (1-3 meses)", "6-12 meses"]
    )
```

**2. json_schema_extra com Exemplo Completo:**
```python
class Recommendation(BaseModel):
    title: str
    description: str
    impact: Literal["HIGH", "MEDIUM", "LOW"]
    effort: Literal["HIGH", "MEDIUM", "LOW"]
    priority: Literal["HIGH", "MEDIUM", "LOW"]
    timeframe: str
    next_steps: list[str]

    class Config:
        json_schema_extra = {
            "example": {  # ✅ Objeto completo válido
                "title": "Implementar Dashboard Financeiro",
                "description": "Criar dashboard consolidando top 10 KPIs...",
                "impact": "HIGH",  # ✅ Campo PRESENTE no exemplo!
                "effort": "LOW",
                "priority": "HIGH",
                "timeframe": "3-6 meses",
                "next_steps": ["Definir KPIs com CFO", "Prototipar em Power BI"]
            }
        }
```

**3. Prompt Alinhado com Schema:**
```python
# Prompt DEVE mencionar TODOS campos obrigatórios
GENERATE_RECOMMENDATIONS_PROMPT = """
Retorne JSON com:
{
    "title": "string (10-150 caracteres)",
    "description": "string (mínimo 50 caracteres)",
    "impact": "HIGH" | "MEDIUM" | "LOW",  # ✅ CORRETO (alinhado com schema)
    "effort": "HIGH" | "MEDIUM" | "LOW",
    "priority": "HIGH" | "MEDIUM" | "LOW",
    "timeframe": "string (ex: '3-6 meses', 'quick win')",
    "next_steps": ["lista de strings"]
}
```

**CHECKLIST PRÉ-PROMPT OBRIGATÓRIO:**

ANTES de criar QUALQUER prompt com structured output:
- [ ] Grep schema Pydantic: `grep "class SchemaName" src/memory/schemas.py -A 50`
- [ ] Listar TODOS campos obrigatórios (sem `= None` ou default)
- [ ] Verificar Literal values (se aplicável)
- [ ] Adicionar `examples=[...]` em Field() para campos complexos
- [ ] Criar `json_schema_extra` com objeto completo válido
- [ ] Prompt menciona TODOS campos obrigatórios explicitamente
- [ ] Testar com 3+ queries variadas antes de commit

**PATTERN COMPLETO VALIDADO:**
```python
# 1. Schema Pydantic com examples
class MyOutput(BaseModel):
    field1: str = Field(description="...", examples=["exemplo1", "exemplo2"])
    field2: Literal["A", "B", "C"] = Field(description="...", examples=["A"])

    class Config:
        json_schema_extra = {
            "example": {
                "field1": "exemplo1",
                "field2": "A"
            }
        }

# 2. Prompt alinhado
PROMPT = """
Retorne JSON com:
- field1: string (exemplos: "exemplo1", "exemplo2")
- field2: "A" | "B" | "C"
"""

# 3. Structured output com fallback
structured_llm = llm.with_structured_output(MyOutput, method="function_calling")
try:
    result = await asyncio.wait_for(structured_llm.ainvoke(messages), timeout=300)
except Exception as e:
    logger.warning(f"function_calling falhou: {e}. Tentando json_mode...")
    structured_llm = llm.with_structured_output(MyOutput, method="json_mode")
    result = await asyncio.wait_for(structured_llm.ainvoke(messages), timeout=240)
```

**QUANDO NÃO APLICAR:**
- ❌ Schema Pydantic simples (1-3 campos, todos strings)
- ❌ Free-form text (chat, conversação)
- ❌ Prototipagem rápida (validar conceito)

**ROI VALIDADO:** Previne ValidationError recorrente (3+ sessões FASE 2), economiza 1-2h debugging por sessão.

**FONTES:**
- leocon.dev/blog/2024/11/from-chaos-to-control-mastering-llm-outputs-with-langchain-and-pydantic/ (Nov 2024)
- LangChain Official Docs - How to return structured data (2024-2025)
- `docs/lessons/lesson-streamlit-ui-debugging-2025-10-22.md` (linhas 200-400, validação empírica)
- Memória [[memory:10230048]] (criada Out 22, 2025)

---

#### E2E Workflow Tests (Validado FASE 2.6 - Out/2025)

**Context**: Testes E2E de workflows LangGraph multi-turn stateless requerem 4 considerações adicionais ao checklist base.

**4 Pontos Adicionais (Checklist expandido: 8 → 12 pontos)**:

**9. Verificar property/método existe**:
*   **SEMPRE** usar `grep "@property\|def method_name" src/file.py` before de usar em código
*   **Exemplo validado (2025-10-16)**: Assumiu `client_profile_agent` property existia → `AttributeError`
*   **Prevenção**:
    ```bash
    grep "@property" src/graph/workflow.py | grep "client_profile_agent"
    # Se não retornar nada, property não existe - criar ou usar alternativa
    ```
*   **ROI**: 5-8 min economizados

**10. Considerar persistência de state**:
*   **SEMPRE** perguntar: "Como state persiste entre múltiplos `run()` calls?"
*   **Exemplo validado (2025-10-16)**: Workflow stateless, `onboarding_progress` perdido entre calls
*   **Solução padrão**: In-memory sessions dict
    ```python
    class BSCWorkflow:
        def __init__(self):
            self._onboarding_sessions: dict[str, dict[str, Any]] = {}

        def handler(self, state):
            user_id = state.user_id
            # Load session
            session = self._onboarding_sessions.get(user_id, {})
            # Process...
            # Save session
            self._onboarding_sessions[user_id] = updated_session
            # Cleanup (se completo)
            if is_complete:
                del self._onboarding_sessions[user_id]
    ```
*   **ROI**: 20-30 min economizados (pattern reutilizável)

**11. Fixtures Pydantic com ID customizado**:
*   **SEMPRE** criar profile inline quando teste precisa `client_id` específico
*   **Exemplo validado (2025-10-16)**: Fixture `client_id='fixture'`, teste esperava `'test_005'` → assertion falhou
*   **Template**:
    ```python
    test_profile = ClientProfile(
        client_id="test_cliente_specific_id",  # Match user_id do teste
        company=valid_client_profile.company,  # Reutilizar outros campos
        context=valid_client_profile.context,
        # ... demais campos do fixture original
    )
    ```
*   **ROI**: 8-12 min economizados

**12. Teste de regressão crítico OBRIGATÓRIO**:
*   **SEMPRE** incluir 1 teste validando que funcionalidade existente NÃO quebrou
*   **Exemplo validado (2025-10-16)**: `test_rag_workflow_cliente_existente_nao_quebrado`
    *   Validou que cliente existente (phase=DISCOVERY) usa RAG tradicional
    *   Routing correto, workflow completo, zero breaking changes
*   **Template**:
    ```python
    def test_existing_functionality_not_broken():
        """CRÍTICO: Validar que funcionalidade X não quebrou com nova feature Y.

        Este teste previne REGRESSÃO!
        """
        # Setup: Cliente/estado existente (não novo)
        mock_existing_state()

        # Action: Executar workflow tradicional
        result = workflow.run_traditional_flow(...)

        # Assert: Comportamento mantido
        assert traditional_method.called
        assert not new_feature_method.called  # Nova feature NÃO interferiu
    ```
*   **ROI**: Crítico (previne breaking changes, economiza horas de rollback)

**13. Implementation-First Testing para APIs desconhecidas (validado FASE 3.1 - 2025-10-19)**
*   Quando implementar tool/agent NOVO com API desconhecida, SEMPRE ler implementação ANTES de escrever testes.
*   Pattern: (1) `grep "def " src/module/file.py` (descobre métodos disponíveis), (2) `grep "def method_name" src/module/file.py -A 15` (lê signature completa), (3) `grep "class SchemaName" src/memory/schemas.py -A 30` (verifica schemas Pydantic usados), (4) Escrever testes alinhados com API real.
*   RAZÃO: TDD funciona quando você CONHECE a API. Para APIs desconhecidas, assumir estrutura causa reescrita completa de testes.
*   **ROI**: 30-40 min economizados por implementação.
*   **QUANDO USAR**: APIs novas (tools consultivas FASE 3+), agentes novos, integrações complexas (RAG, LLM, multi-step).
*   **QUANDO NÃO USAR** (TDD tradicional melhor): API conhecida, lógica simples (math, pure functions), refactoring (testes já existem).
*   **Lição detalhada**: `docs/lessons/lesson-swot-testing-methodology-2025-10-19.md` (700+ linhas com checklist completo).

**14. Mock Múltiplas Chamadas com itertools.cycle**:
*   **Mock Múltiplas Chamadas com itertools.cycle**: Mock para múltiplas chamadas sequenciais requer abordagem dinâmica. Mock estático (`return_value` fixo) causa falhas quando tool processa lista de items esperando outputs diferentes.
*   **QUANDO USAR**: Mock será invocado N vezes (N > 1) esperando outputs diferentes a cada chamada.
*   **RAZÃO**: Mock estático retorna sempre o mesmo valor. Tools que processam lista items (ex: 4 perspectivas BSC) falham com ValidationError quando mock não diferencia outputs.
*   **SOLUÇÃO VALIDADA**: `itertools.cycle` para retornar valores sequencialmente.
*   **Template Geral**:
    ```python
    from itertools import cycle

    @pytest.fixture
    def mock_with_cycle():
        """Mock com itertools.cycle - múltiplos valores sequenciais."""
        mock_obj = MagicMock()

        # Step 1: Definir valores esperados em ordem
        values_order = [value1, value2, value3, value4]
        values_cycle = cycle(values_order)

        # Step 2: side_effect usa next(cycle)
        def side_effect_fn(*args, **kwargs):
            """Retorna próximo valor na sequência."""
            return next(values_cycle)

        mock_obj.method.side_effect = side_effect_fn
        return mock_obj
    ```
*   **Exemplo Real - KPI Definer Tool** (validado 2025-10-19):
    ```python
    from itertools import cycle
    from pydantic import BaseModel

    @pytest.fixture
    def mock_llm() -> MagicMock:
        """Mock LLM que retorna KPIs com perspectiva correta sequencialmente."""
        llm = MagicMock(spec=["invoke", "with_structured_output"])

        class KPIListOutput(BaseModel):
            kpis: list[KPIDefinition]

        # Define KPIs mock para cada perspectiva
        kpis_by_perspective = {
            "Financeira": [KPIDefinition(name="ARR", perspective="Financeira", ...)],
            "Clientes": [KPIDefinition(name="NPS", perspective="Clientes", ...)],
            "Processos Internos": [KPIDefinition(name="Lead Time", perspective="Processos Internos", ...)],
            "Aprendizado e Crescimento": [KPIDefinition(name="Retencao", perspective="Aprendizado e Crescimento", ...)]
        }

        # SOLUCAO: itertools.cycle alinhado com ordem de chamadas da tool
        # Tool chama: Financeira → Clientes → Processos → Aprendizado (linhas 152-156)
        perspective_order = [
            "Financeira",
            "Financeira",
            "Clientes",
            "Processos Internos",
            "Aprendizado e Crescimento"
        ]
        perspective_cycle = cycle(perspective_order)

        def mock_invoke_side_effect(prompt: str):
            """Retorna KPIs da próxima perspectiva na sequência."""
            perspective = next(perspective_cycle)
            kpis = kpis_by_perspective[perspective]
            return KPIListOutput(kpis=kpis)

        mock_structured_llm = MagicMock()
        mock_structured_llm.invoke.side_effect = mock_invoke_side_effect
        llm.with_structured_output.return_value = mock_structured_llm
        return llm
    ```
*   **Pergunta-Chave ANTES de criar mock**:
    > "Quantas vezes o mock será invocado e com quais diferenças nos outputs esperados?"
*   **Decisão Tree**:
    - Mock invocado **1x** OU **N vezes com MESMO output**? → `return_value` fixo OK
    - Mock invocado **N vezes com OUTPUTS DIFERENTES**? → `itertools.cycle` ou `side_effect` lista
    - Mock invocado **N vezes exatas** (N == len(lista))? → `side_effect = [val1, val2, val3]`
    - Mock invocado **N vezes indefinido** OU **N > len(values)**? → `itertools.cycle` (melhor escolha)
*   **Alternativas e Trade-offs**:
    1. **Lista finita** (`side_effect = [val1, val2, val3]`):
       - ✅ Simples, direto
       - ❌ Erro `StopIteration` se chamadas > len(lista)
       - **Usar quando**: N chamadas exatas conhecido
    2. **Callable com contador**:
       ```python
       call_count = 0
       def side_effect():
           nonlocal call_count
           call_count += 1
           return values[call_count - 1]
       ```
       - ✅ Controle explícito
       - ❌ Mais verboso, `nonlocal` não pythônico
       - **Usar quando**: Lógica complexa por chamada
    3. **itertools.cycle** (RECOMENDADO):
       - ✅ Pythônico, elegante
       - ✅ Nunca lança `StopIteration` (ciclo infinito)
       - ✅ Alinha naturalmente com ordem de chamadas da tool
       - ❌ Ciclo repete (pode mascarar bugs se tool chamar mais que o esperado)
       - **Usar quando**: Mock múltiplas chamadas sequenciais (99% dos casos)
*   **Checklist Aplicação**:
    - [ ] Mock será invocado N vezes (N > 1)?
    - [ ] Outputs esperados são diferentes a cada chamada?
    - [ ] Tool processa lista de items sequencialmente?
    - [ ] Ordem do cycle alinha com ordem de chamadas da tool? (verificar código `for item in items:`)
    - [ ] `side_effect` usa `next(cycle)` corretamente?
    - [ ] Teste valida perspectiva/category/type correto em cada output?
*   **Antipadrões Evitados**:
    - ❌ **Mock estático com return_value fixo** para múltiplas chamadas
    - ❌ **String matching no prompt** para detectar qual output retornar (frágil, dependente de formato)
    - ❌ **Parsing JSON/XML** para diferenciar chamadas (adiciona complexidade desnecessária)
    - ❌ **Criar ValidationError manualmente** em Pydantic (preferir passar dados inválidos e deixar Pydantic validar)
*   **ROI**: 15-20 min economizados (debugging mock estático vs criação correta desde início)
*   **Aplicabilidade Futura**:
    - ✅ Objetivos Estratégicos Tool (múltiplos objetivos por perspectiva BSC)
    - ✅ Action Plan Tool (lista de ações sequenciais)
    - ✅ Benchmarking Tool (comparações múltiplas perspectivas)
    - ✅ Qualquer tool que processa lista items com LLM/API calls sequenciais
*   **Lições Detalhadas**:
    - `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
    - Memória [[memory:9969868]] ponto 14 (checklist obrigatório 14 pontos)

**15. LER SCHEMA PYDANTIC VIA GREP ANTES DE CRIAR FIXTURE OU ACESSAR CAMPOS (validado FASE 3.5 - 2025-10-19)**
*   PROBLEMA RESOLVIDO: Fixtures Pydantic inválidas (5 erros recorrentes 4 sessões: SWOT, Five Whys, Issue Tree, KPI, Strategic Obj), context builders acessando campos inexistentes (2 erros 3 sessões).
*   ROOT CAUSE: NÃO ler schema Pydantic ANTES de criar fixture/acessar campos, assumir estrutura baseada em memória.
*   **QUANDO APLICAR**: SEMPRE antes de: (1) Criar fixture Pydantic, (2) Escrever context builder, (3) Acessar campos de schema em qualquer código.
*   **COMO APLICAR** (5 sub-pontos detalhados em [[memory:9969868]]):
    1.  `grep "class SchemaName" src/memory/schemas.py -A 50` → Identificar campos obrigatórios/opcionais/Literal
    2.  `grep "min_length\|@field_validator" src/memory/schemas.py` → Identificar validators
    3. Validar nested schemas (schemas que agregam outros schemas)
    4. Criar fixture com campos validados (margem de segurança em min_length)
    5. Defensive programming em context builders (hasattr/getattr antes de acessar)
*   **ROI ESPERADO**: 30-40 min economizados por sessão (fixtures corretas primeira tentativa).
*   **ROI VALIDADO**: Sessão 20 teve 8 erros (1.5h debugging) que teriam sido prevenidos aplicando PONTO 15.
*   **Lição detalhada**: `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys Root Cause Analysis completa).

**15.6 IDENTIFICAR TODOS SCHEMAS PYDANTIC USADOS NO TESTE (Validado FASE 3.6 - 2025-10-19)**
*   PROBLEMA: PONTO 15 aplicado PARCIALMENTE (apenas em 1 de 4 schemas), fixtures inválidas passaram despercebidas.
*   ROOT CAUSE: Checklist não especifica "identificar TODOS schemas via grep imports do teste".
*   **QUANDO APLICAR**: SEMPRE antes de aplicar PONTO 15 (leitura via grep), identificar TODOS schemas Pydantic

---

### UI Defensive Programming (Validado Nov/2025 - ROI: 4-6x economia tempo)

**CONTEXTO**: Problema recorrente 4+ sessões (Out-Nov/2025) - AttributeError em UI Streamlit acessando campos/métodos Pydantic sem validar schema. Impacto: 40 min debugging por sessão. Root cause sistêmico: UI implementada ANTES de schemas estarem completos → código assume estrutura sem validação defensiva.

**OCORRÊNCIAS DOCUMENTADAS**:
1. Sessão 29: AttributeError '.current_challenges' (loop infinito)
2. Sessão 40: AttributeError '.cause_effect_links' (Strategy Map)
3. Sessão 41: AttributeError '.create_details_table()' (BSCNetworkGraph)
4. Sessão 41: Filter priority "ALTA" vs "Alta" (Literal case mismatch)

**CHECKLIST PRE-UI OBRIGATORIO (6 pontos - aplicar SEMPRE antes de implementar página Streamlit)**:

1. **Grep TODOS schemas usados**: `grep "from src.memory.schemas import" pages/X.py -A 5`
2. **Para CADA schema identificado**: `grep "class SchemaName" src/memory/schemas.py -A 80`
3. **Listar campos obrigatórios vs opcionais**: SEM `| None` = obrigatório, COM `| None` = opcional
4. **Identificar Literal values**: `grep "field.*Literal" src/memory/schemas.py` (case-sensitive!)
5. **Validar métodos disponíveis** (se UI chama): `[m for m in dir(Class) if not m.startswith('_')]`
6. **Aplicar defensive programming**: hasattr antes de acessar, getattr com default

**PATTERN DEFENSIVO VALIDADO**:
```python
# Campos - SEMPRE usar getattr com default
value = getattr(objective, 'field_name', default_value)

# Métodos - hasattr check
if hasattr(graph_instance, 'method_name'):
    result = graph_instance.method_name()
else:
    result = create_alternative_manually(data)  # Fallback

# Literal - case exato (grep confirma!)
alta_prio = len([o for o in objs if o.priority == "Alta"])  # [OK] Case correto!
```

**QUANDO APLICAR**: SEMPRE antes de implementar/modificar QUALQUER página Streamlit que acessa Pydantic models.

**ROI VALIDADO**: 10 min checklist previne 40-60 min debugging AttributeError runtime = 4-6x economia. Aplicável: 100% páginas UI (20+ páginas Streamlit).

**FONTES**: Memória [[memory:10178686]] atualizada (Nov 2025), lesson-sessao-41-ui-schema-evolution-2025-11-22.md (1.180+ linhas), Brightdata research Medium Vik Y. "Defensive Programming Python" (2024).

---

### LangGraph State Schema Evolution (Validado Nov/2025 - ROI: 6-12x economia tempo)

**CONTEXTO**: Problema crítico descoberto Sessão 41 (Nov 2025) - LangGraph StateGraph IGNORA SILENCIOSAMENTE campos retornados por handlers que não estão definidos no schema Pydantic. Sem erro, sem warning, sem log = silent failure (30-60 min debugging).

**SINTOMA**: Handler retorna `{"new_field": value}` mas `state.new_field` fica None/inexistente → condicionais `if state.new_field:` retornam False → funcionalidade não executa (save, UI display).

**ROOT CAUSE VALIDADO (Brightdata Nov 2025)**:
> "LangGraph design deliberado descarta campos não definidos no schema para manter type safety. Fields returned by nodes that aren't in schema are **silently dropped**."
> Fonte: GitHub Issue langchain-ai/langgraphjs#536 (Sep 2024, 9 upvotes)

**OCORRÊNCIA SESSÃO 41**: implementation_handler retornou `{"action_plan": dict}` mas campo action_plan AUSENTE no BSCState → LangGraph descartou silent → state.action_plan nunca populado → save SQLite condicional if state.action_plan: retornou False → Action Plan NUNCA salvo!

**CHECKLIST PRE-SCHEMA-CHANGE OBRIGATORIO (5 pontos - aplicar SEMPRE que handler LangGraph retornar campo novo)**:

1. **Handler retorna campo novo?** Identificar em código `return {"new_field": value, ...}`
2. **Grep schema**: `grep "new_field" src/graph/states.py` (se vazio = ADICIONAR!)
3. **Adicionar campo ao BSCState ANTES de testar handler**:
   ```python
   # Campo Novo (SPRINT X - FASE Y) - FEATURE/BUG FIX SESSAO Z (data)
   # Comentário 1 linha explicando por que adicionado
   new_field: dict[str, Any] | None = None
   ```
4. **Validar campo presente**: `python -c "from src.graph.states import BSCState; print('new_field' in BSCState.model_fields)"`
5. **Comentário inline obrigatório**: Sprint/Fase + bug fix/feature + sessão + data

**PATTERN CORRETO VALIDADO** (Sessão 41):
```python
# src/graph/states.py - BSCState
# Action Plan (SPRINT 3 - FASE 6) - BUG FIX SESSAO 41 (2025-11-22)
# Campo ausente causava action_plan não ser salvo no state (LangGraph ignora campos não definidos)
action_plan: dict[str, Any] | None = None
```

**QUANDO APLICAR**: SEMPRE que handler/node retornar campo novo no dict de update. Especialmente crítico em: novos sprints/fases, refatorações de state, integrações que adicionam dados workflow.

**ROI VALIDADO**: 5 min checklist previne 30-60 min debugging silent failure = 6-12x economia. Aplicável: 15+ handlers futuros em 6 sprints (FASE 5-6).

**FONTES**: GitHub Issue langchain-ai/langgraphjs#536 (Sep 2024), Blog swarnendu.de "LangGraph Best Practices" (Sep 2025, Section 1.1 "Keep state boring and typed"), Memória [[memory:11467544]] (criada Nov 2025), lesson-sessao-41-ui-schema-evolution-2025-11-22.md (1.180+ linhas).

**LIÇÃO-CHAVE**: LangGraph NÃO avisa campos ausentes - responsabilidade do desenvolvedor garantir alinhamento handler-schema. SEMPRE atualizar BSCState schema ANTES de handler retornar campo novo.
