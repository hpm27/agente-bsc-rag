---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
version: "2.4"
last_updated: "2025-10-19"
---

## HEADERS

## TECH STACK

### Test Methodology (Validado Out/2025 - ROI: 32-60 min economizados por implementa√ß√£o)

**ANTES de escrever QUALQUER teste (unit√°rio ou E2E), aplicar [[memory:9969868]] - CHECKLIST OBRIGAT√ìRIO de 15 pontos + SUB-PONTO 15.6**:

**PONTOS 1-8: Testes Unit√°rios** (validado FASE 2.5)

1. **Ler assinatura completa**: `grep "def method_name" src/file.py -A 10` (NUNCA assumir)
2. **Verificar tipo retorno**: Objeto Pydantic ou built-in? Campos obrigat√≥rios?
3. **Contar par√¢metros**: Quantos params (n√£o contar `self`)?
4. **Valida√ß√µes pr√©-flight**: Valida√ß√µes no c√≥digo + Pydantic (min_length, Literal, validators)
5. **Entender decorators**: @retry com `reraise=True` relan√ßa exce√ß√£o original (N√ÉO RetryError)
6. **Fixtures Pydantic**: NUNCA passar `None` para `default_factory`. Incluir campos obrigat√≥rios.
7. **Dados v√°lidos em mocks**: Usar MARGEM DE SEGURAN√áA (ex: min_length=20 ‚Üí usar 50+ chars)
8. **Verificar m√©todo correto**: Confirmar nome exato via grep (ex: `invoke()` n√£o `process_query()`)

**PONTOS 9-12: E2E Workflow Tests** (validado FASE 2.6) - Ver se√ß√£o "E2E Workflow Tests" below for details

**Debugging pytest (Validado [[memory:9969628]] e [[memory:10012853]])**:
*   **SEMPRE** usar `--tb=long` (traceback completo, NUNCA --tb=short)
*   **NUNCA** usar filtros (`Select-Object`, `Select-String`) - oculta informa√ß√£o cr√≠tica
*   Comando correto: `pytest tests/test_file.py -v --tb=long 2>&1`

**pytest em paralelo** (performance):
*   `-n <num_workers>`: Workers paralelos
*   `-v`: Verbose output
*   `--tb=long`: Traceback completo
*   `--dist=loadscope`: Distribui por fixture scope (mais seguro)
*   `--dist=loadfile`: Distribui por arquivo (fixtures function-scoped)

**Li√ß√µes Detalhadas**:
- `docs/lessons/lesson-test-debugging-methodology-2025-10-15.md` (FASE 2.4, 5 erros)
- `docs/lessons/lesson-diagnostic-agent-test-methodology-2025-10-16.md` (FASE 2.5, 7 erros, 1.100+ linhas)
- `docs/lessons/lesson-onboarding-state-e2e-tests-2025-10-16.md` (FASE 2.6, 4 problemas E2E, 11.900+ linhas)
- `docs/lessons/lesson-swot-testing-methodology-2025-10-19.md` (FASE 3.1, Implementation-First Testing, APIs desconhecidas, 700+ linhas, 30-40 min economizados)
- `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
- `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys applied + PONTO 15 fixtures Pydantic)
- `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md` (950+ linhas, 5 Whys + Hypothesis Property-Based Testing discovery, SUB-PONTO 15.6)

**ROI Comprovado**: Aplicar checklist ANTES economiza 38 minutos de debugging por implementa√ß√£o (validado FASE 2.5 unit√°rio, FASE 2.6 E2E).

---

### Property-Based Testing com Hypothesis para Pydantic (Descoberta Out/2025)

**Context**: Mainstream solution 2024-2025 para valida√ß√£o autom√°tica de fixtures Pydantic. Descoberto em research Brightdata durante Sess√£o 21 (Benchmarking Tool) ap√≥s identificar problema recorrente de fixtures inv√°lidas (6 sess√µes consecutivas).

**QUANDO USAR**: Validators Pydantic complexos (min_length, ranges, field_validator customizados, Literal com m√∫ltiplos valores)

**FERRAMENTA**: Hypothesis (https://hypothesis.readthedocs.io) - Property-based testing framework oficial Pydantic

**Pattern Validado** (Pydantic AI Docs 2024):
```python
from hypothesis import given
from hypothesis.strategies import from_type
from pydantic import BaseModel

class BenchmarkComparison(BaseModel):
    perspective: str
    gap: float = Field(ge=-100, le=200)
    gap_type: Literal["positive", "negative", "neutral"]
    benchmark_source: str = Field(min_length=20)
    insight: str = Field(min_length=50)

# Hypothesis gera 100+ fixtures v√°lidas AUTOMATICAMENTE
@given(comparison=from_type(BenchmarkComparison))
def test_benchmark_comparison_properties(comparison):
    """Property-based test: QUALQUER BenchmarkComparison v√°lido deve satisfazer propriedades."""
    # Testa propriedades em CENTENAS de casos gerados
    assert len(comparison.insight) >= 50  # Hypothesis respeita min_length!
    assert comparison.gap_type in ["positive", "negative", "neutral"]
    assert -100 <= comparison.gap <= 200
```

**Benef√≠cios** (validado Pydantic AI docs + CodiLime 2024):
- ‚úÖ **Gera centenas de fixtures v√°lidas automaticamente** (100+ exemplos por default)
- ‚úÖ **Respeita validators Pydantic** (min_length, Literal, Field constraints, @field_validator)
- ‚úÖ **Encontra edge cases** que testes manuais n√£o cobrem (valores extremos: gap=199.9, gap=-99.9)
- ‚úÖ **Reduz manuten√ß√£o** - fixtures atualizam automaticamente quando schema Pydantic muda
- ‚úÖ **Detecta bugs em validators** - se Hypothesis gera fixture inv√°lida que passa, validator est√° bugado!

**Limita√ß√µes**:
- ‚ùå Requer aprendizado de `strategies` API (curva aprendizado ~2h)
- ‚ùå Testes mais lentos (gera 100+ exemplos vs 1 fixture manual)
- ‚ùå N√£o substitui testes de neg√≥cio espec√≠ficos (complementa)
- ‚ùå Fixtures geradas s√£o aleat√≥rias (n√£o reproduz√≠veis sem seed)

**Quando N√ÉO usar** (usar fixtures manuais):
- ‚ùå Casos de neg√≥cio BSC espec√≠ficos (ex: 4 perspectivas balanceadas, KPIs setoriais espec√≠ficos)
- ‚ùå Testes r√°pidos CI/CD (Hypothesis adiciona ~5-10s por teste)
- ‚ùå Testes de regress√£o (precisa reproduzir cen√°rio exato)

**Decision Matrix**:

| Cen√°rio | Hypothesis | Fixtures Manuais | Justificativa |
|---------|-----------|------------------|---------------|
| **Validators complexos** | ‚úÖ | ‚ùå | Testa 100+ combina√ß√µes automaticamente |
| **Casos BSC espec√≠ficos** | ‚ùå | ‚úÖ | Controle exato do cen√°rio de neg√≥cio |
| **Schemas em mudan√ßa** | ‚úÖ | ‚ùå | Fixtures atualizam automaticamente |
| **CI/CD r√°pido** | ‚ùå | ‚úÖ | Hypothesis +5-10s por teste |
| **Edge cases desconhecidos** | ‚úÖ | ‚ùå | Descobre bugs n√£o pensados |
| **Testes regress√£o** | ‚ùå | ‚úÖ | Reproduz cen√°rio exato |

**Recomenda√ß√£o: Abordagem H√≠brida** (validada CodiLime + Semaphore 2024):
- ‚úÖ Hypothesis para validators Pydantic (min_length, ranges, Literal, field_validator)
- ‚úÖ Fixtures manuais para casos de neg√≥cio BSC (4 perspectivas, 10-15 KPIs balanceados)

**Exemplo Concreto Benchmarking Tool**:
```python
# Hypothesis para testar VALIDATOR (gap_type alinhado com gap)
@given(
    gap=st.floats(min_value=-100, max_value=200),
    gap_type=st.sampled_from(["positive", "negative", "neutral"])
)
def test_gap_type_validator_properties(gap, gap_type):
    """Property: gap_type deve alinhar com gap (validator customizado)."""
    try:
        comparison = BenchmarkComparison(gap=gap, gap_type=gap_type, ...)
        # Se passou, validar alinhamento
        if gap_type == "negative":
            assert gap >= 5  # Validator customizado threshold
    except ValidationError:
        # Se falhou, verificar se falha √© correta
        if gap_type == "negative" and gap < 5:
            pass  # Falha esperada ‚úÖ
        else:
            pytest.fail("ValidationError inesperado!")

# Fixture manual para caso BSC espec√≠fico (4 perspectivas balanceadas)
@pytest.fixture
def valid_benchmark_report_bsc():
    """Caso BSC espec√≠fico: 2 compara√ß√µes por perspectiva (balanceado)."""
    return BenchmarkReport(
        comparisons=[
            # 2 Financeira (setor espec√≠fico)
            BenchmarkComparison(perspective="Financeira", metric_name="Margem EBITDA", ...),
            BenchmarkComparison(perspective="Financeira", metric_name="ROI", ...),
            # 2 Clientes (B2B SaaS)
            BenchmarkComparison(perspective="Clientes", metric_name="NPS", ...),
            BenchmarkComparison(perspective="Clientes", metric_name="Churn", ...),
            # 2 Processos
            BenchmarkComparison(perspective="Processos Internos", metric_name="Lead Time", ...),
            BenchmarkComparison(perspective="Processos Internos", metric_name="Cycle Time", ...),
            # 2 Aprendizado
            BenchmarkComparison(perspective="Aprendizado e Crescimento", metric_name="Reten√ß√£o", ...),
            BenchmarkComparison(perspective="Aprendizado e Crescimento", metric_name="Treinamento", ...)
        ]
    )
```

**ROI Esperado**:
- **Hypothesis**: Economiza 15-20 min testando validators (100+ casos vs 3-5 manuais)
- **Fixtures manuais**: Economiza 10-15 min em casos BSC (controle exato)
- **H√≠brido**: Melhor dos dois mundos (validators autom√°ticos + neg√≥cio controlado)

**Instala√ß√£o**:
```bash
pip install hypothesis pytest-hypothesis
```

**Fontes Validadas** (Brightdata research Out/2025):
1. Pydantic AI Docs - Hypothesis Integration (https://ai.pydantic.dev/testing/)
2. CodiLime - Testing APIs with Pytest (Oct 2024)
3. Semaphore - Property-Based Testing Python Hypothesis (Jan 2023)
4. Hypothesis Documentation (https://hypothesis.readthedocs.io/)

**Li√ß√£o Completa**: `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md` (950+ linhas, 5 Whys Root Cause Analysis, Hypothesis discovery, SUB-PONTO 15.6 validado)

---

#### E2E Workflow Tests (Validado FASE 2.6 - Out/2025)

**Context**: Testes E2E de workflows LangGraph multi-turn stateless requerem 4 considera√ß√µes adicionais ao checklist base.

**4 Pontos Adicionais (Checklist expandido: 8 ‚Üí 12 pontos)**:

**9. Verificar property/m√©todo existe**:
*   **SEMPRE** usar `grep "@property\|def method_name" src/file.py` antes de usar em c√≥digo
*   **Exemplo validado (2025-10-16)**: Assumiu `client_profile_agent` property existia ‚Üí `AttributeError`
*   **Preven√ß√£o**: 
    ```bash
    grep "@property" src/graph/workflow.py | grep "client_profile_agent"
    # Se n√£o retornar nada, property n√£o existe - criar ou usar alternativa
    ```
*   **ROI**: 5-8 min economizados

**10. Considerar persist√™ncia de state**:
*   **SEMPRE** perguntar: "Como state persiste entre m√∫ltiplos `run()` calls?"
*   **Exemplo validado (2025-10-16)**: Workflow stateless, `onboarding_progress` perdido entre calls
*   **Solu√ß√£o padr√£o**: In-memory sessions dict
    ```python
    class BSCWorkflow:
        def __init__(self):
            self._onboarding_sessions: dict[str, dict[str, Any]] = {}
        
        def handler(self, state):
            user_id = state.user_id
            # Load session
            session = self._onboarding_sessions.get(user_id, {})
            # Process...
            # Save session
            self._onboarding_sessions[user_id] = updated_session
            # Cleanup (se completo)
            if is_complete:
                del self._onboarding_sessions[user_id]
    ```
*   **ROI**: 20-30 min economizados (pattern reutiliz√°vel)

**11. Fixtures Pydantic com ID customizado**:
*   **SEMPRE** criar profile inline quando teste precisa `client_id` espec√≠fico
*   **Exemplo validado (2025-10-16)**: Fixture `client_id='fixture'`, teste esperava `'test_005'` ‚Üí assertion falhou
*   **Template**:
    ```python
    test_profile = ClientProfile(
        client_id="test_cliente_specific_id",  # Match user_id do teste
        company=valid_client_profile.company,  # Reutilizar outros campos
        context=valid_client_profile.context,
        # ... demais campos do fixture original
    )
    ```
*   **ROI**: 8-12 min economizados

**12. Teste de regress√£o cr√≠tico OBRIGAT√ìRIO**:
*   **SEMPRE** incluir 1 teste validando que funcionalidade existente N√ÉO quebrou
*   **Exemplo validado (2025-10-16)**: `test_rag_workflow_cliente_existente_nao_quebrado`
    *   Validou que cliente existente (phase=DISCOVERY) usa RAG tradicional
    *   Routing correto, workflow completo, zero breaking changes
*   **Template**:
    ```python
    def test_existing_functionality_not_broken():
        """CR√çTICO: Validar que funcionalidade X n√£o quebrou com nova feature Y.
        
        Este teste previne REGRESS√ÉO!
        """
        # Setup: Cliente/estado existente (n√£o novo)
        mock_existing_state()
        
        # Action: Executar workflow tradicional
        result = workflow.run_traditional_flow(...)
        
        # Assert: Comportamento mantido
        assert traditional_method.called
        assert not new_feature_method.called  # Nova feature N√ÉO interferiu
    ```
*   **ROI**: Cr√≠tico (previne breaking changes, economiza horas de rollback)

**13. Implementation-First Testing para APIs desconhecidas (validado FASE 3.1 - 2025-10-19)**
*   Quando implementar tool/agent NOVO com API desconhecida, SEMPRE ler implementa√ß√£o ANTES de escrever testes. 
*   Pattern: (1) `grep "def " src/module/file.py` (descobre m√©todos dispon√≠veis), (2) `grep "def method_name" src/module/file.py -A 15` (l√™ signature completa), (3) `grep "class SchemaName" src/memory/schemas.py -A 30` (verifica schemas Pydantic usados), (4) Escrever testes alinhados com API real.
*   RAZ√ÉO: TDD funciona quando voc√™ CONHECE a API. Para APIs desconhecidas, assumir estrutura causa reescrita completa de testes.
*   **ROI**: 30-40 min economizados por implementa√ß√£o.
*   **QUANDO USAR**: APIs novas (tools consultivas FASE 3+), agentes novos, integra√ß√µes complexas (RAG, LLM, multi-step).
*   **QUANDO N√ÉO USAR** (TDD tradicional melhor): API conhecida, l√≥gica simples (math, pure functions), refactoring (testes j√° existem).
*   **Li√ß√£o detalhada**: `docs/lessons/lesson-swot-testing-methodology-2025-10-19.md` (700+ linhas com checklist completo).

**15. LER SCHEMA PYDANTIC VIA GREP ANTES DE CRIAR FIXTURE OU ACESSAR CAMPOS (validado FASE 3.5 - 2025-10-19)**
*   PROBLEMA RESOLVIDO: Fixtures Pydantic inv√°lidas (5 erros recorrentes 4 sess√µes: SWOT, Five Whys, Issue Tree, KPI, Strategic Obj), context builders acessando campos inexistentes (2 erros 3 sess√µes).
*   ROOT CAUSE: N√ÉO ler schema Pydantic ANTES de criar fixture/acessar campos, assumir estrutura baseada em mem√≥ria.
*   **QUANDO APLICAR**: SEMPRE antes de: (1) Criar fixture Pydantic, (2) Escrever context builder, (3) Acessar campos de schema em qualquer c√≥digo.
*   **COMO APLICAR** (5 sub-pontos detalhados em [[memory:9969868]]):
    1. `grep "class SchemaName" src/memory/schemas.py -A 50` ‚Üí Identificar campos obrigat√≥rios/opcionais/Literal
    2. `grep "min_length\|@field_validator" src/memory/schemas.py` ‚Üí Identificar validators
    3. Validar nested schemas (schemas que agregam outros schemas)
    4. Criar fixture com campos validados (margem de seguran√ßa em min_length)
    5. Defensive programming em context builders (hasattr/getattr antes de acessar)
*   **ROI ESPERADO**: 30-40 min economizados por sess√£o (fixtures corretas primeira tentativa).
*   **ROI VALIDADO**: Sess√£o 20 teve 8 erros (1.5h debugging) que teriam sido prevenidos aplicando PONTO 15.
*   **Li√ß√£o detalhada**: `docs/lessons/lesson-strategic-objectives-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys Root Cause Analysis completa).

**15.6 IDENTIFICAR TODOS SCHEMAS PYDANTIC USADOS NO TESTE (Validado FASE 3.6 - 2025-10-19)**
*   PROBLEMA: PONTO 15 aplicado PARCIALMENTE (apenas em 1 de 4 schemas), fixtures inv√°lidas passaram despercebidas.
*   ROOT CAUSE: Checklist n√£o especifica "identificar TODOS schemas via grep imports do teste".
*   **QUANDO APLICAR**: SEMPRE antes de criar fixtures Pydantic OU escrever testes com m√∫ltiplos schemas
*   **COMO APLICAR** (5 sub-passos):
    1.  Grep imports do teste ‚Üí listar todos schemas Pydantic usados
    2.  Grep CADA schema identificado (fields + constraints)
    3.  Grep validators de CADA schema (@field_validator, @model_validator)
    4.  Criar fixtures com margem +20%
    5.  Dry-run validation (try/except ValidationError em fixture)
*   **ROI ESPERADO**: 30-60 min economizados por sess√£o (fixtures corretas primeira tentativa).
*   **Li√ß√£o detalhada**: `docs/lessons/lesson-benchmarking-testing-methodology-2025-10-19.md`

#### Mock Testing Patterns (Validado FASE 3.4 - Out/2025)

**Context**: Mocks para m√∫ltiplas chamadas sequenciais requerem abordagem din√¢mica. Mock est√°tico (`return_value` fixo) causa falhas quando tool processa lista de items esperando outputs diferentes.

**Ponto 14: Mock M√∫ltiplas Chamadas com itertools.cycle**:

**QUANDO USAR**: Mock ser√° invocado N vezes (N > 1) esperando outputs diferentes a cada chamada.

**RAZ√ÉO**: Mock est√°tico retorna sempre o mesmo valor. Tools que processam lista items (ex: 4 perspectivas BSC) falham com ValidationError quando mock n√£o diferencia outputs.

**SOLU√á√ÉO VALIDADA**: `itertools.cycle` para retornar valores sequencialmente.

**Template Geral**:
```python
from itertools import cycle

@pytest.fixture
def mock_with_cycle():
    """Mock com itertools.cycle - m√∫ltiplos valores sequenciais."""
    mock_obj = MagicMock()
    
    # Step 1: Definir valores esperados em ordem
    values_order = [value1, value2, value3, value4]
    values_cycle = cycle(values_order)
    
    # Step 2: side_effect usa next(cycle)
    def side_effect_fn(*args, **kwargs):
        """Retorna pr√≥ximo valor na sequ√™ncia."""
        return next(values_cycle)
    
    mock_obj.method.side_effect = side_effect_fn
    return mock_obj
```

**Exemplo Real - KPI Definer Tool** (validado 2025-10-19):
```python
from itertools import cycle
from pydantic import BaseModel

@pytest.fixture
def mock_llm() -> MagicMock:
    """Mock LLM que retorna KPIs com perspectiva correta sequencialmente."""
    llm = MagicMock(spec=["invoke", "with_structured_output"])
    
    class KPIListOutput(BaseModel):
        kpis: list[KPIDefinition]
    
    # Define KPIs mock para cada perspectiva
    kpis_by_perspective = {
        "Financeira": [KPIDefinition(name="ARR", perspective="Financeira", ...)],
        "Clientes": [KPIDefinition(name="NPS", perspective="Clientes", ...)],
        "Processos Internos": [KPIDefinition(name="Lead Time", perspective="Processos Internos", ...)],
        "Aprendizado e Crescimento": [KPIDefinition(name="Retencao", perspective="Aprendizado e Crescimento", ...)]
    }
    
    # SOLUCAO: itertools.cycle alinhado com ordem de chamadas da tool
    # Tool chama: Financeira ‚Üí Clientes ‚Üí Processos ‚Üí Aprendizado (linhas 152-156)
    perspective_order = [
        "Financeira",
        "Clientes",
        "Processos Internos",
        "Aprendizado e Crescimento"
    ]
    perspective_cycle = cycle(perspective_order)
    
    def mock_invoke_side_effect(prompt: str):
        """Retorna KPIs da pr√≥xima perspectiva na sequ√™ncia."""
        perspective = next(perspective_cycle)
        kpis = kpis_by_perspective[perspective]
        return KPIListOutput(kpis=kpis)
    
    mock_structured_llm = MagicMock()
    mock_structured_llm.invoke.side_effect = mock_invoke_side_effect
    llm.with_structured_output.return_value = mock_structured_llm
    return llm
```

**Pergunta-Chave ANTES de criar mock**:
> "Quantas vezes o mock ser√° invocado e com quais diferen√ßas nos outputs esperados?"

**Decis√£o Tree**:
- Mock invocado **1x** OU **N vezes com MESMO output**? ‚Üí `return_value` fixo OK
- Mock invocado **N vezes com OUTPUTS DIFERENTES**? ‚Üí `itertools.cycle` ou `side_effect` lista
- Mock invocado **N vezes exatas** (N == len(lista))? ‚Üí `side_effect = [val1, val2, val3]`
- Mock invocado **N vezes indefinido** OU **N > len(values)**? ‚Üí `itertools.cycle` (melhor escolha)

**Alternativas e Trade-offs**:

1. **Lista finita** (`side_effect = [val1, val2, val3]`):
   - ‚úÖ Simples, direto
   - ‚ùå Erro `StopIteration` se chamadas > len(lista)
   - **Usar quando**: N chamadas exatas conhecido

2. **Callable com contador**:
   ```python
   call_count = 0
   def side_effect():
       nonlocal call_count
       call_count += 1
       return values[call_count - 1]
   ```
   - ‚úÖ Controle expl√≠cito
   - ‚ùå Mais verboso, `nonlocal` n√£o pyth√¥nico
   - **Usar quando**: L√≥gica complexa por chamada

3. **itertools.cycle** (RECOMENDADO):
   - ‚úÖ Pyth√¥nico, elegante
   - ‚úÖ Nunca lan√ßa `StopIteration` (ciclo infinito)
   - ‚úÖ Alinha naturalmente com ordem de chamadas da tool
   - ‚ùå Ciclo repete (pode mascarar bugs se tool chamar mais que esperado)
   - **Usar quando**: Mock m√∫ltiplas chamadas sequenciais (99% dos casos)

**Checklist Aplica√ß√£o**:
- [ ] Mock ser√° invocado N vezes (N > 1)?
- [ ] Outputs esperados s√£o diferentes a cada chamada?
- [ ] Tool processa lista de items sequencialmente?
- [ ] Ordem do cycle alinha com ordem de chamadas da tool? (verificar c√≥digo `for item in items:`)
- [ ] `side_effect` usa `next(cycle)` corretamente?
- [ ] Teste valida perspectiva/category/type correto em cada output?

**Antipadr√µes Evitados**:
- ‚ùå **Mock est√°tico com return_value fixo** para m√∫ltiplas chamadas
- ‚ùå **String matching no prompt** para detectar qual output retornar (fr√°gil, dependente de formato)
- ‚ùå **Parsing JSON/XML** para diferenciar chamadas (adiciona complexidade desnecess√°ria)
- ‚ùå **Criar ValidationError manualmente** em Pydantic (preferir passar dados inv√°lidos e deixar Pydantic validar)

**ROI**: 15-20 min economizados (debugging mock est√°tico vs cria√ß√£o correta desde in√≠cio)

**Aplicabilidade Futura**:
- ‚úÖ Objetivos Estrat√©gicos Tool (m√∫ltiplos objetivos por perspectiva BSC)
- ‚úÖ Action Plan Tool (lista de a√ß√µes sequenciais)
- ‚úÖ Benchmarking Tool (compara√ß√µes m√∫ltiplas perspectivas)
- ‚úÖ Qualquer tool que processa lista items com LLM/API calls sequenciais

**Li√ß√µes Detalhadas**:
- `docs/lessons/lesson-kpi-testing-5whys-methodology-2025-10-19.md` (950+ linhas, 5 Whys root cause debugging)
- Mem√≥ria [[memory:9969868]] ponto 14 (checklist obrigat√≥rio 14 pontos)

 Regarding `.cursorignore` files:

*   Cursor IDE uses `.cursorignore` files to control which files Cursor can access. This is for security and performance. This is for security and performance.
*   `.env` files are ignored by default.
*   To unignore a file, place an exclamation mark `!` in front of the ignore glob pattern in the `.cursorignore` file. For example: `!.env`.
*   You can edit the global ignore list in Cursor's settings. Go to `File > Preferences > VS code settings` and search for "Global Cursor Ignore List". You can remove patterns from the list there.
*   If you don't have a `.cursorignore` file, but `.env` files are still ignored, check the global ignore list.
*   Global cursorignore negations do not override `.gitignore` rules unless specific syntax is used.

### How to Liberate .env from the Global Cursor Ignore

Based on collected information, Cursor ignores `.env` files by default through global configuration. Here are three ways to liberate the file:

#### Method 1: Modify Global Ignore List (Recommended)

1.  **Open Cursor Settings:**
    *   Press `Ctrl + ,` (Windows) or `Cmd + ,` (Mac)
    *   Or go to: `File > Preferences > Settings`
2.  **Search for "Global Cursor Ignore":**
    *   Type in the search bar: `cursor ignore`
    *   Locate: **"Cursor > General: Global Cursor Ignore List"**
3.  **Remove or Comment the `.env` Entry:**
    *   You will see a list with patterns like:

    ```
    **/node_modules/**
    **/.git/**
    **/.env
    ```
    *   **Delete** the line `**/.env` or patterns related to `.env*`
    *   Click **"OK"** or save the settings.

#### Method 2: Use `.cursorignore` to "Unignore" (Local Override)

Create or edit the `.cursorignore` file in the **root of your project**:

```bash
# .cursorignore

# Unignore .env files (overrides the global ignore)
!.env
!.env.local
!.env.development

# You can also be more specific
!**/.env
```

**Important:** The `!` (exclamation) prefix **reverses** the ignore, allowing the file.

#### Method 3: Via Cursor Interface (Quick Shortcut)

1.  **Access Indexing Settings:**
    *   Go to: `Settings > Cursor > Indexing and Docs`
    *   Or search for: `cursor indexing`
2.  **Edit .cursorignore:**
    *   Click **"Edit .cursorignore"**
    *   Add the line: `!.env`
3.  **Reload the Index:**
    *   Press `Ctrl + Shift + P` (Windows) or `Cmd + Shift + P` (Mac)
    *   Type: `Cursor: Reindex Codebase`
    *   Execute the command

**IMPORTANT: Security Considerations**

Brightdata's research revealed important community discussions:

> **"Cursor ignores env files by default. Although risky, you can remove env files from the ignore list..."**

**Why does Cursor ignore `.env` by default?**

1.  **Credential Protection:** `.env` files contain API keys, passwords, and sensitive tokens.
2.  **Leakage Prevention:** Prevents information from being inadvertently sent to AI servers.
3.  **Context Security:** Prevents information from being inadvertently sent to AI servers.
4.  **Context Security:** Prevents information from being inadvertently sent to AI servers.
5.  **Context Security:** Prevents information from being inadvertently sent to AI servers.
6.  **Context Security:** Prevents credentials from appearing in chat/completions context.

**If you REALLY need to liberate `.env`:**

‚úÖ **Best Practice:**

*   Use `.env.example` or `.env.template` with dummy values.
*   Liberate **only** these template files in the ignore.
*   Keep the real `.env` ignored.

‚ùå **Avoid:**

*   Liberating `.env` in public/shared projects.
*   Exposing real credentials in the AI context.
*   Removing protection in repositories with multiple collaborators.

**Test if it Worked**

1.  **Check the File Icon:**

    *   In the Cursor Explorer, ignored files show a üö´ icon.
    *   After liberating, the icon should disappear.
2.  **Test in Chat:**

    *   Open Cursor Chat.
    *   Type: `@.env` (mention the file).
    *   If it works, the file has been successfully liberated.
3.  **Verify Indexing:**

    *   `Ctrl + P` ‚Üí Type `.env`
    *   The file should appear in the results.

**Complete Example: `.cursorignore`**

```bash
# .cursorignore - Complete Example

# Keep ignored (security)
**/.env
**/.env.local
**/.env.production

# Liberate templates (safe)
!.env.example
!.env.template

# Other useful ignores
**/node_modules/**
**/.git/**
**/dist/**
**/build/**
**/__pycache/**
**/.pytest_cache/**
**/.venv/**
**/venv/**
```

**Sources (2025)**

*   [Cursor Docs - Ignore Files](https://cursor.com/docs/context/ignore/files)
*   [Cursor Forum - You can unignore files in .cursorignore](https://forum.cursor.com/t/you-can-unignore-files-in-cursorignore/38074)
*   [GitHub Issue #3101 - .env files ignored by default](https://github.com/cursor/cursor/issues/3101)
*   [Reddit r/cursor - How to disable .cursorignore from .cursorignore](https://www.reddit.com/r/cursor/comments/1ki08ww/)

### Language Detection Improvements

When the language detector in `src/rag/query_translator.py` is unable to determine the language of a query, it defaults to PT-BR. To improve language detection accuracy:

*   The `_detect_language` function has been enhanced with a regular expression to identify Portuguese suffixes (√ß√£o, √µes, √°rio, √°rios, eira, eiras, eiro, eiros).
*   Word boundaries are used in keyword searches to prevent substring matches (e.g., "financial" in "financeiros").
*   The logging message has been updated to provide more context when the language is ambiguous.
*   Expand the list of keywords to include common BSC terms to improve language detection for technical queries.
*   When no keywords are detected, the system defaults to PT-BR, assuming a Brazilian context.

#### Code Snippets:

*   **Detecting Portuguese Suffixes:**

    ```python
    has_pt_suffixes = bool(re.search(r'\b\w*(√ß√£o|√µes|√°rio|√°rios|eira|eiras|eiro|eiros)\b', text_lower))
    ```

*   **Keyword Search with Word Boundaries:**

    ```python
    pt_count = sum(1 for kw in pt_keywords if re.search(r'\b' + re.escape(kw) + r'\b', text_lower))
    en_count = sum(1 for kw in pt_keywords if re.search(r'\b' + re.escape(kw) + r'\b', text_lower))
    ```

### Dependency Management

*   When updating dependencies, especially `ruff`, ensure the version specified in `requirements.txt` is compatible with the project's other dependencies. Aim to use the latest stable version, but specify a version range to allow for minor updates without breaking