# Fase 2B - RAG AvanÃ§ado: Self-RAG + CRAG

**Status:** ğŸ“‹ PLANEJADO  
**InÃ­cio Previsto:** ApÃ³s validaÃ§Ã£o Benchmark Fase 2A  
**DuraÃ§Ã£o Estimada:** 2-3 semanas (8-10 dias Ãºteis)  
**Ãšltima AtualizaÃ§Ã£o:** 2025-10-14

---

## ğŸ¯ OBJETIVOS FASE 2B

Implementar tÃ©cnicas RAG avanÃ§adas focadas em:

1. **Self-RAG** - Reduzir alucinaÃ§Ãµes via self-reflection (-40-50%)
2. **CRAG** - Corrigir retrieval ruim via query reformulation (+23% quality)

**PrÃ©-requisito:** Benchmark Fase 2A validar necessidade (faithfulness, precision)

---

## ğŸ“Š DECISÃƒO CONDICIONAL (Baseada em Benchmark Fase 2A)

### CenÃ¡rio 1: Implementar Self-RAG

**SE:**
- Faithfulness < 0.85 (alucinaÃ§Ãµes detectadas)
- Judge approval rate < 85%
- Queries complexas com respostas incompletas

**ENTÃƒO:** Implementar Self-RAG (Prioridade ALTA)

---

### CenÃ¡rio 2: Implementar CRAG

**SE:**
- Context Precision < 0.70 (retrieval ruim)
- Muitas queries com docs irrelevantes
- Retrieval falha em queries ambÃ­guas

**ENTÃƒO:** Implementar CRAG (Prioridade ALTA)

---

### CenÃ¡rio 3: Pular Fase 2B

**SE:**
- Faithfulness > 0.90 (alucinaÃ§Ãµes mÃ­nimas)
- Context Precision > 0.80 (retrieval excelente)
- Judge approval > 90%

**ENTÃƒO:** Considerar Fase 2B **OPCIONAL** (sistema jÃ¡ muito bom)

---

## ğŸ—ºï¸ ROADMAP FASE 2B

### Fase 2B.1 - Self-RAG (Semana 1)

**DuraÃ§Ã£o:** 3-4 dias Ãºteis (13-16h)  
**Complexidade:** â­â­â­â­ MÃ©dia-Alta  
**ROI Esperado:** â­â­â­â­ Alto

#### Etapas

1. **Research & Discovery** (1h)
   - [ ] Ler tutorial LangGraph Self-RAG oficial
   - [ ] Estudar paper "Self-RAG: Learning to Retrieve, Generate, and Critique" (2023)
   - [ ] Analisar implementaÃ§Ãµes 2025 (Analytics Vidhya, DataCamp)
   - [ ] Estudar reflection tokens ([Retrieve], [Relevant], [Supported], [Useful])

2. **Design & Architecture** (1h)
   - [ ] Desenhar workflow LangGraph com 5 nÃ³s:
     * `retrieve_decision` - Decide SE precisa retrieval
     * `retrieve` - Busca documentos
     * `grade_documents` - Avalia relevÃ¢ncia (keep/discard)
     * `generate` - Gera resposta
     * `grade_answer` - Verifica suporte nos docs
   - [ ] Definir prompts para cada grading step
   - [ ] Planejar integraÃ§Ã£o com workflow BSC atual

3. **Implementation** (6-8h)
   - [ ] Criar `src/rag/self_rag.py` com classe `SelfRAG`
   - [ ] Implementar `RetrievalGrader` (julga relevÃ¢ncia de docs)
   - [ ] Implementar `AnswerGrader` (verifica suporte/alucinaÃ§Ã£o)
   - [ ] Integrar com `BSCWorkflow` via feature flag
   - [ ] Adicionar `ENABLE_SELF_RAG=true/false` em `.env`
   - [ ] Criar prompts em `src/prompts/self_rag_prompts.py`

4. **Testing** (3h)
   - [ ] Criar `tests/test_self_rag.py` (15+ testes unitÃ¡rios)
   - [ ] Criar benchmark especÃ­fico (20 queries propensas a alucinaÃ§Ã£o)
   - [ ] Medir hallucination rate (baseline vs self-rag)
   - [ ] Validar mÃ©tricas: Faithfulness >0.90, LatÃªncia +20-30%

5. **Documentation** (2h)
   - [ ] Criar `docs/techniques/SELF_RAG.md` (300+ linhas)
   - [ ] Documentar liÃ§Ã£o aprendida em `docs/lessons/`
   - [ ] Atualizar `README.md` e plano

**MÃ©tricas de Sucesso:**
- âœ… Hallucination rate < 5% (vs 10-15% baseline)
- âœ… Faithfulness > 0.90 (vs 0.80-0.85 baseline)
- âœ… Judge approval > 90%
- âš ï¸ LatÃªncia +20-30% (trade-off aceitÃ¡vel)

---

### Fase 2B.2 - CRAG (Semana 2)

**DuraÃ§Ã£o:** 4-5 dias Ãºteis (18-21h)  
**Complexidade:** â­â­â­â­â­ Alta  
**ROI Esperado:** â­â­â­â­ Alto (SE retrieval ruim)

#### Etapas

1. **Research & Discovery** (1.5h)
   - [ ] Ler tutorial Meilisearch CRAG (Sep 2025)
   - [ ] Estudar tutorial DataCamp CRAG (Sep 2024)
   - [ ] Analisar cÃ³digo GitHub HuskyInSalt/CRAG
   - [ ] Entender grading algorithm (Correct/Incorrect/Ambiguous)
   - [ ] Estudar knowledge refinement (decomposiÃ§Ã£o de docs)

2. **Design & Architecture** (1.5h)
   - [ ] Desenhar workflow com 4 decisÃµes:
     * Grade retrieval (Correct/Incorrect/Ambiguous)
     * SE Correct â†’ usar docs
     * SE Incorrect â†’ web search fallback
     * SE Ambiguous â†’ combinar docs + web
   - [ ] Planejar query rewriter (reformulaÃ§Ã£o automÃ¡tica)
   - [ ] Definir knowledge refinement strategy

3. **Web Search Integration** (2h)
   - [ ] Avaliar opÃ§Ãµes: Tavily API, Bing API, ou Brightdata MCP
   - [ ] Integrar web search como fallback
   - [ ] Criar filtro de resultados web (relevÃ¢ncia BSC)
   - [ ] Combinar docs internos + web results (RRF)

4. **Implementation** (8-10h)
   - [ ] Criar `src/rag/corrective_rag.py` com classe `CorrectiveRAG`
   - [ ] Implementar `RetrievalGrader` (Correct/Incorrect/Ambiguous)
   - [ ] Implementar `QueryRewriter` (reformulaÃ§Ã£o de queries)
   - [ ] Implementar `KnowledgeRefiner` (knowledge strips)
   - [ ] Integrar web search fallback
   - [ ] Adicionar `ENABLE_CRAG=true/false` em `.env`
   - [ ] Criar prompts em `src/prompts/crag_prompts.py`

5. **Testing** (3h)
   - [ ] Criar `tests/test_crag.py` (15+ testes unitÃ¡rios)
   - [ ] Criar benchmark especÃ­fico (20 queries ambÃ­guas)
   - [ ] Medir retrieval quality (baseline vs CRAG)
   - [ ] Validar mÃ©tricas: Precision 0.65 â†’ 0.80, Correction rate 10-15%

6. **Documentation** (2h)
   - [ ] Criar `docs/techniques/CRAG.md` (300+ linhas)
   - [ ] Documentar liÃ§Ã£o aprendida
   - [ ] Atualizar documentaÃ§Ã£o geral

**MÃ©tricas de Sucesso:**
- âœ… Context Precision: 0.65 â†’ 0.80 (+23%)
- âœ… Correction triggered: 10-15% queries
- âœ… Accuracy em queries corrigidas: +15%
- âš ï¸ LatÃªncia +30-40% (mais que Self-RAG)

---

### Fase 2B.3 - IntegraÃ§Ã£o & ValidaÃ§Ã£o (Semana 3)

**DuraÃ§Ã£o:** 2-3 dias (8-10h)

1. **IntegraÃ§Ã£o Completa** (3h)
   - [ ] Integrar Self-RAG + CRAG ao workflow
   - [ ] Resolver conflitos (ambos ativos simultaneamente)
   - [ ] Otimizar trade-off latÃªncia vs qualidade
   - [ ] Testar combinaÃ§Ãµes de features

2. **ValidaÃ§Ã£o E2E** (3h)
   - [ ] Executar suite E2E completa (22 testes)
   - [ ] Validar sem regressÃµes
   - [ ] Ajustar thresholds se necessÃ¡rio

3. **Benchmark Fase 2B** (4h)
   - [ ] Executar benchmark com 50 queries
   - [ ] Comparar: Baseline â†’ Fase 2A â†’ Fase 2B
   - [ ] Validar ROI incremental
   - [ ] Gerar relatÃ³rio comparativo

**MÃ©tricas de Sucesso:**
- âœ… E2E tests 100% passando
- âœ… Nenhuma regressÃ£o vs Fase 2A
- âœ… Melhoria incremental em qualidade

---

### Fase 2B.4 - DocumentaÃ§Ã£o & LiÃ§Ãµes (Semana 3-4)

**DuraÃ§Ã£o:** 1-2 dias (5-8h)

1. **DocumentaÃ§Ã£o TÃ©cnica**
   - [ ] Finalizar `docs/techniques/SELF_RAG.md`
   - [ ] Finalizar `docs/techniques/CRAG.md`
   - [ ] Criar comparaÃ§Ã£o tÃ©cnica (quando usar cada uma)

2. **LiÃ§Ãµes Aprendidas**
   - [ ] `docs/lessons/lesson-self-rag-2025-10-XX.md`
   - [ ] `docs/lessons/lesson-crag-2025-10-XX.md`
   - [ ] Documentar trade-offs observados

3. **AtualizaÃ§Ã£o Geral**
   - [ ] Atualizar `README.md` (Fase 2B completa)
   - [ ] Atualizar `.cursor/rules/rag-bsc-core.mdc`
   - [ ] Criar `docs/history/FASE_2B_COMPLETA.md`

---

## ğŸ“š REFERÃŠNCIAS TÃ‰CNICAS

### Self-RAG

**Papers:**
- [Self-RAG: Learning to Retrieve, Generate, and Critique](https://arxiv.org/abs/2310.11511) (2023)

**Tutorials (2025):**
- [LangGraph Self-RAG Official Tutorial](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/)
- [Analytics Vidhya - Self-RAG Guide](https://www.analyticsvidhya.com/blog/2025/01/self-rag/)
- [Medium - Self-Reflective RAG with LangGraph](https://nayakpplaban.medium.com/build-a-reflective-agentic-rag-workflow-using-langgraph)

**GitHub:**
- [AkariAsai/self-rag](https://github.com/AkariAsai/self-rag) - Paper original implementation

---

### CRAG (Corrective RAG)

**Papers:**
- [Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884) (2024)

**Tutorials (2025):**
- [Meilisearch - CRAG Guide](https://www.meilisearch.com/blog/corrective-rag) (Sep 2025)
- [DataCamp - CRAG Implementation](https://www.datacamp.com/tutorial/corrective-rag-crag) (Sep 2024)
- [GeeksforGeeks - CRAG Tutorial](https://www.geeksforgeeks.org/artificial-intelligence/corrective-retrieval-augmented-generation-crag/) (Oct 2025)

**GitHub:**
- [HuskyInSalt/CRAG](https://github.com/HuskyInSalt/CRAG) - Official implementation
- [LangGraph CRAG Tutorial](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/)

---

## ğŸ¯ CRITÃ‰RIOS DE DECISÃƒO

### Quando Implementar Self-RAG?

âœ… **SIM, implementar SE:**
- Hallucination rate > 10%
- Faithfulness < 0.85
- Judge rejeita >15% respostas
- Queries complexas geram informaÃ§Ã£o nÃ£o suportada

âŒ **NÃƒO implementar SE:**
- Faithfulness jÃ¡ > 0.90
- Judge approval > 90%
- AlucinaÃ§Ãµes raras (<5%)

---

### Quando Implementar CRAG?

âœ… **SIM, implementar SE:**
- Context Precision < 0.70
- Retrieval falha em >20% queries
- Queries ambÃ­guas com docs irrelevantes
- Dataset incompleto para domÃ­nio

âŒ **NÃƒO implementar SE:**
- Context Precision > 0.80
- Retrieval jÃ¡ muito bom
- Dataset completo e bem curado

---

## âš™ï¸ ARQUITETURA PLANEJADA

### Self-RAG Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SELF-RAG WORKFLOW                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Query Input
   â†“
2. Retrieve Decision (LLM)
   â”œâ”€ [Retrieve] â†’ Continuar
   â””â”€ [No Retrieve] â†’ Generate direto
   â†“
3. Retrieve Documents (k=50)
   â†“
4. Grade Documents (LLM)
   â”œâ”€ [Relevant] â†’ Keep
   â””â”€ [Irrelevant] â†’ Discard
   â†“
5. Generate Answer (LLM com docs relevantes)
   â†“
6. Grade Answer (LLM)
   â”œâ”€ [Supported] â†’ Retornar resposta
   â”œâ”€ [Partially Supported] â†’ Re-retrieve (iteraÃ§Ã£o)
   â””â”€ [Not Supported] â†’ Re-generate
   â†“
7. Final Answer
```

**Reflection Tokens:**
- `[Retrieve]` / `[No Retrieve]` - DecisÃ£o de retrieval
- `[Relevant]` / `[Irrelevant]` - Grading de docs
- `[Supported]` / `[Partially]` / `[Not Supported]` - Grading de resposta
- `[Useful]` / `[Not Useful]` - Utilidade final

---

### CRAG Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CRAG WORKFLOW                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Query Input
   â†“
2. Retrieve Documents (k=50, hybrid search)
   â†“
3. Grade Retrieval Quality (LLM)
   â”œâ”€ [Correct] â†’ Usar docs (confidence > 0.7)
   â”œâ”€ [Incorrect] â†’ Web search fallback
   â””â”€ [Ambiguous] â†’ Combinar docs + web
   â†“
4a. SE Correct:
    â†’ Knowledge Refinement (decompor docs longos)
    â†’ Generate com docs refinados
    â†“
4b. SE Incorrect:
    â†’ Query Rewrite (reformular query)
    â†’ Web Search (Tavily/Bing)
    â†’ Generate com web results
    â†“
4c. SE Ambiguous:
    â†’ Knowledge Refinement
    â†’ Web Search
    â†’ Combinar (RRF) docs + web
    â†’ Generate com combinaÃ§Ã£o
    â†“
5. Final Answer
```

**Componentes-Chave:**
- `RetrievalGrader` - Avalia qualidade (score 0-1)
- `QueryRewriter` - Reformula queries ruins
- `KnowledgeRefiner` - Extrai knowledge strips
- `WebSearchTool` - Fallback externo

---

## ğŸ“‹ MICRO-TAREFAS DETALHADAS

### FASE 2B.1 - Self-RAG (13-16h)

#### TASK 1: Research & Discovery (1h)

**Artefatos:**
- [ ] Notas de estudo do paper Self-RAG
- [ ] AnÃ¡lise de cÃ³digo LangGraph tutorial
- [ ] ComparaÃ§Ã£o implementaÃ§Ãµes 2025

---

#### TASK 2: Design (1h)

**Artefatos:**
- [ ] Diagrama de workflow (draw.io ou Mermaid)
- [ ] EspecificaÃ§Ã£o de prompts (5 prompts)
- [ ] DefiniÃ§Ã£o de reflection tokens

**Prompts NecessÃ¡rios:**
1. `retrieve_decision_prompt` - "Esta query precisa de retrieval?"
2. `document_grading_prompt` - "Este documento Ã© relevante?"
3. `answer_grading_prompt` - "Resposta estÃ¡ suportada pelos docs?"
4. `hallucination_check_prompt` - "Resposta contÃ©m informaÃ§Ã£o inventada?"
5. `usefulness_prompt` - "Resposta Ã© Ãºtil para query?"

---

#### TASK 3: Implementation (6-8h)

**Arquivos a Criar:**

```python
# src/rag/self_rag.py
class SelfRAG:
    """Self-Reflective RAG com grading de docs e respostas."""
    
    def __init__(self, retriever, llm):
        self.retriever = retriever
        self.llm = llm
        self.retrieval_grader = RetrievalGrader(llm)
        self.answer_grader = AnswerGrader(llm)
    
    def should_retrieve(self, query: str) -> bool:
        """Decide se query precisa de retrieval."""
        pass
    
    def grade_documents(self, query: str, docs: List[Document]) -> List[Document]:
        """Filtra docs irrelevantes."""
        pass
    
    def grade_answer(self, query: str, answer: str, docs: List[Document]) -> Dict:
        """Verifica se resposta estÃ¡ suportada."""
        pass

# src/prompts/self_rag_prompts.py
RETRIEVE_DECISION_PROMPT = """..."""
DOCUMENT_GRADING_PROMPT = """..."""
ANSWER_GRADING_PROMPT = """..."""
```

**IntegraÃ§Ã£o com BSCWorkflow:**

```python
# src/graph/workflow.py - novo nÃ³
def grade_and_refine(state: BSCState) -> BSCState:
    """NÃ³ Self-RAG para grading de resposta."""
    if not settings.ENABLE_SELF_RAG:
        return state
    
    self_rag = SelfRAG(retriever, llm)
    grading = self_rag.grade_answer(
        state["query"],
        state["final_response"],
        state["agent_responses"]
    )
    
    if not grading["supported"]:
        # Re-retrieve ou re-generate
        pass
    
    return state
```

---

#### TASK 4: Testing (3h)

**Testes UnitÃ¡rios:**

```python
# tests/test_self_rag.py

def test_should_retrieve_complex_query():
    """Query complexa deve requerer retrieval."""
    self_rag = SelfRAG(retriever, llm)
    assert self_rag.should_retrieve("Como implementar BSC?") == True

def test_should_not_retrieve_simple_query():
    """Query muito simples pode nÃ£o requerer retrieval."""
    self_rag = SelfRAG(retriever, llm)
    assert self_rag.should_retrieve("O que Ã© BSC?") == False

def test_grade_documents_filters_irrelevant():
    """Grading deve filtrar docs irrelevantes."""
    # ... 13+ testes adicionais
```

**Benchmark EspecÃ­fico:**
- 20 queries propensas a alucinaÃ§Ã£o
- Manual evaluation de hallucination rate
- ComparaÃ§Ã£o baseline vs self-rag

---

#### TASK 5: Documentation (2h)

**Template `docs/techniques/SELF_RAG.md`:**

```markdown
# Self-RAG - Self-Reflective Retrieval Augmented Generation

## VisÃ£o Geral
[DescriÃ§Ã£o de 1-2 parÃ¡grafos]

## Problema que Resolve
- AlucinaÃ§Ãµes em respostas RAG
- LLM inventando informaÃ§Ã£o nÃ£o presente nos docs
- Baixa fidelidade factual

## Como Funciona
[Diagrama + explicaÃ§Ã£o detalhada]

## Componentes
### 1. Retrieval Decision
### 2. Document Grading  
### 3. Answer Grading
### 4. Reflection Tokens

## ImplementaÃ§Ã£o
[CÃ³digo completo com comentÃ¡rios]

## MÃ©tricas
| MÃ©trica | Baseline | Self-RAG | Melhoria |
|---------|----------|----------|----------|
| Faithfulness | 0.82 | 0.93 | +13% |
| Hallucination Rate | 12% | 4% | -67% |
| LatÃªncia | 95s | 118s | +24% |

## Quando Usar
âœ… Alta necessidade de acurÃ¡cia factual
âœ… Hallucination rate > 10%
âœ… Queries complexas multi-parte

## Quando NÃƒO Usar
âŒ LatÃªncia crÃ­tica (<10s)
âŒ Faithfulness jÃ¡ > 0.90
âŒ Custo muito limitado

## LiÃ§Ãµes Aprendidas
[O que funcionou, trade-offs, insights]

## ReferÃªncias
[Papers, tutorials, cÃ³digo]
```

---

### FASE 2B.2 - CRAG (18-21h)

#### TASK 1: Research (1.5h)

**Artefatos:**
- [ ] AnÃ¡lise de paper CRAG (2024)
- [ ] ComparaÃ§Ã£o 3 implementaÃ§Ãµes (Meilisearch, DataCamp, GitHub)
- [ ] Notas sobre grading algorithm

---

#### TASK 2: Design (1.5h)

**Artefatos:**
- [ ] Diagrama workflow CRAG
- [ ] EspecificaÃ§Ã£o de grading (score thresholds)
- [ ] EstratÃ©gia de query rewrite

**Grading Thresholds:**
- **Correct**: confidence > 0.7 â†’ Usar docs
- **Ambiguous**: 0.3 < confidence â‰¤ 0.7 â†’ Combinar docs + web
- **Incorrect**: confidence â‰¤ 0.3 â†’ Web search only

---

#### TASK 3: Web Search Integration (2h)

**OpÃ§Ãµes Avaliadas:**

| OpÃ§Ã£o | Custo | LatÃªncia | Qualidade | DecisÃ£o |
|-------|-------|----------|-----------|---------|
| Tavily API | $$ | ~2s | Alta | âœ… Preferida |
| Bing API | $ | ~1s | MÃ©dia | Alternativa |
| Brightdata MCP | GrÃ¡tis | ~3s | Alta | âœ… ViÃ¡vel |

**ImplementaÃ§Ã£o:**

```python
# src/rag/web_search.py
class WebSearchTool:
    """Fallback web search para CRAG."""
    
    def __init__(self, provider="tavily"):
        self.provider = provider
    
    def search(self, query: str, k: int = 5) -> List[Document]:
        """Busca web com query reformulada."""
        if self.provider == "tavily":
            # Tavily API
            pass
        elif self.provider == "brightdata":
            # Brightdata MCP
            pass
```

---

#### TASK 4: Implementation (8-10h)

**Arquivos a Criar:**

```
src/rag/
â”œâ”€â”€ corrective_rag.py        # Classe principal CRAG
â”œâ”€â”€ retrieval_grader.py      # Avalia qualidade do retrieval
â”œâ”€â”€ query_rewriter.py        # Reformula queries ruins (JÃ EXISTE - adaptar)
â”œâ”€â”€ knowledge_refiner.py     # Extrai knowledge strips
â””â”€â”€ web_search.py            # Web search fallback

src/prompts/
â””â”€â”€ crag_prompts.py          # Prompts de grading e rewrite
```

**CÃ³digo Principal:**

```python
# src/rag/corrective_rag.py
from enum import Enum

class RetrievalQuality(Enum):
    CORRECT = "correct"
    AMBIGUOUS = "ambiguous"
    INCORRECT = "incorrect"

class CorrectiveRAG:
    """Corrective RAG com query reformulation e web search fallback."""
    
    def __init__(self, retriever, llm, web_search):
        self.retriever = retriever
        self.llm = llm
        self.web_search = web_search
        self.grader = RetrievalGrader(llm)
        self.rewriter = QueryRewriter(llm)
        self.refiner = KnowledgeRefiner(llm)
    
    def grade_retrieval(self, query: str, docs: List[Document]) -> RetrievalQuality:
        """Avalia qualidade do retrieval."""
        score = self.grader.grade(query, docs)
        
        if score > 0.7:
            return RetrievalQuality.CORRECT
        elif score > 0.3:
            return RetrievalQuality.AMBIGUOUS
        else:
            return RetrievalQuality.INCORRECT
    
    def execute(self, query: str) -> Dict[str, Any]:
        """Executa CRAG completo."""
        # 1. Retrieval inicial
        docs = self.retriever.retrieve(query, k=50)
        
        # 2. Grade retrieval
        quality = self.grade_retrieval(query, docs)
        
        # 3. DecisÃ£o baseada em grading
        if quality == RetrievalQuality.CORRECT:
            # Refinar e usar
            refined_docs = self.refiner.refine(docs)
            return {"docs": refined_docs, "source": "internal"}
        
        elif quality == RetrievalQuality.INCORRECT:
            # Rewrite + Web search
            rewritten_query = self.rewriter.rewrite(query)
            web_docs = self.web_search.search(rewritten_query)
            return {"docs": web_docs, "source": "web"}
        
        else:  # AMBIGUOUS
            # Combinar internal + web
            refined_docs = self.refiner.refine(docs)
            rewritten_query = self.rewriter.rewrite(query)
            web_docs = self.web_search.search(rewritten_query)
            combined = self._combine_docs(refined_docs, web_docs)
            return {"docs": combined, "source": "hybrid"}
```

---

#### TASK 5: Testing (3h)

**Benchmark EspecÃ­fico:**

```json
{
  "queries_ambiguas": [
    "DiferenÃ§as BSC manufatura vs serviÃ§os",
    "BSC setor pÃºblico adaptaÃ§Ãµes",
    "KPIs inovaÃ§Ã£o perspectiva aprendizado",
    ...
  ],
  "expected_corrections": 15-20% das queries
}
```

**Testes:**

```python
def test_crag_detects_poor_retrieval():
    """CRAG deve detectar retrieval ruim."""
    crag = CorrectiveRAG(retriever, llm, web_search)
    
    # Query ambÃ­gua com docs ruins
    quality = crag.grade_retrieval(
        "diferenÃ§as BSC manufatura serviÃ§os",
        low_quality_docs
    )
    
    assert quality == RetrievalQuality.INCORRECT

def test_crag_triggers_rewrite():
    """CRAG deve reescrever query quando retrieval falha."""
    # ... 13+ testes adicionais
```

---

## ğŸ“Š MÃ‰TRICAS DE VALIDAÃ‡ÃƒO

### Self-RAG

| MÃ©trica | Baseline | Target Self-RAG | Como Medir |
|---------|----------|-----------------|------------|
| Hallucination Rate | 10-15% | <5% | Manual evaluation 50 queries |
| Faithfulness (RAGAS) | 0.82 | >0.90 | RAGAS metric |
| Judge Approval | 82% | >90% | Judge agent |
| LatÃªncia P50 | 90s | <120s | Benchmark |
| Custo por query | $0.05 | <$0.07 | Token counting |

### CRAG

| MÃ©trica | Baseline | Target CRAG | Como Medir |
|---------|----------|-------------|------------|
| Context Precision | 0.65 | >0.80 | RAGAS metric |
| Correction Rate | N/A | 10-15% | Count triggers |
| Accuracy (corrigidas) | N/A | +15% | Manual eval subset |
| LatÃªncia P50 | 90s | <135s | Benchmark |
| Web search triggers | N/A | 5-10% | Logging |

---

## ğŸ”§ FEATURE FLAGS

```bash
# .env - Fase 2B flags

# Self-RAG
ENABLE_SELF_RAG=true
SELF_RAG_MIN_CONFIDENCE=0.5  # Threshold para retrieval decision
SELF_RAG_MAX_ITERATIONS=2    # Max re-retrieve iterations

# CRAG
ENABLE_CRAG=true
CRAG_CORRECT_THRESHOLD=0.7   # Score > 0.7 = Correct
CRAG_AMBIGUOUS_THRESHOLD=0.3 # Score 0.3-0.7 = Ambiguous
CRAG_WEB_SEARCH_PROVIDER=tavily  # tavily, bing, brightdata
CRAG_MAX_REWRITES=2          # Max query reformulations
```

---

## ğŸ“… CRONOGRAMA ESTIMADO

### Semana 1 - Self-RAG

| Dia | Atividade | Horas | Status |
|-----|-----------|-------|--------|
| 1 | Research + Design | 2h | â¸ï¸ |
| 2 | Implementation Part 1 | 4h | â¸ï¸ |
| 3 | Implementation Part 2 | 4h | â¸ï¸ |
| 4 | Testing + Ajustes | 3h | â¸ï¸ |
| 5 | Documentation | 2h | â¸ï¸ |

**Total:** 15h (3-4 dias)

---

### Semana 2 - CRAG

| Dia | Atividade | Horas | Status |
|-----|-----------|-------|--------|
| 1 | Research + Design | 3h | â¸ï¸ |
| 2 | Web Search Integration | 2h | â¸ï¸ |
| 3 | Implementation Part 1 | 5h | â¸ï¸ |
| 4 | Implementation Part 2 | 5h | â¸ï¸ |
| 5 | Testing | 3h | â¸ï¸ |
| 6 | Documentation | 2h | â¸ï¸ |

**Total:** 20h (4-5 dias)

---

### Semana 3 - IntegraÃ§Ã£o & ValidaÃ§Ã£o

| Dia | Atividade | Horas | Status |
|-----|-----------|-------|--------|
| 1 | IntegraÃ§Ã£o Self-RAG + CRAG | 3h | â¸ï¸ |
| 2 | Testes E2E | 3h | â¸ï¸ |
| 3 | Benchmark Fase 2B (50 queries) | 4h | â¸ï¸ |
| 4 | AnÃ¡lise + Docs finais | 3h | â¸ï¸ |

**Total:** 13h (2-3 dias)

---

## ğŸ¯ CRITÃ‰RIOS DE SUCESSO FASE 2B

### Funcional

- âœ… Self-RAG reduz hallucination rate em 50%+
- âœ… CRAG melhora precision em 20%+ quando trigado
- âœ… Ambas tÃ©cnicas tÃªm feature flags funcionais
- âœ… E2E tests 100% passando

### Performance

- âœ… LatÃªncia Self-RAG: baseline +20-30% (aceitÃ¡vel)
- âœ… LatÃªncia CRAG: baseline +30-40% (aceitÃ¡vel)
- âœ… Custo: +30-50% (justificÃ¡vel por qualidade)

### Qualidade

- âœ… RAGAS Faithfulness > 0.90
- âœ… RAGAS Context Precision > 0.80
- âœ… Judge Approval > 90%

---

## ğŸ”— DEPENDÃŠNCIAS

### Bibliotecas Python

```txt
# JÃ¡ instaladas
langchain>=0.1.0
langchain-openai>=0.0.2
langgraph>=0.0.20

# A adicionar
tavily-python>=0.3.0  # Web search (SE usar Tavily)
```

### APIs Externas

- âœ… OpenAI (jÃ¡ configurada) - Para LLM grading
- â¸ï¸ Tavily (opcional) - Web search fallback CRAG
- â¸ï¸ Bing Search (alternativa) - Web search fallback

---

## âš ï¸ RISCOS & MITIGAÃ‡Ã•ES

### Risco 1: LatÃªncia Muito Alta

**Risco:** Self-RAG + CRAG podem adicionar +50-70% latÃªncia  
**MitigaÃ§Ã£o:** 
- Feature flags para ativar seletivamente
- Router pode decidir quando usar cada tÃ©cnica
- Cache agressivo de grading results

### Risco 2: Custo Elevado

**Risco:** +3-4 LLM calls por query = +60-80% custo  
**MitigaÃ§Ã£o:**
- Usar GPT-4o-mini para grading (barato)
- Batch grading quando possÃ­vel
- Cache de decisions

### Risco 3: Over-Engineering

**Risco:** Fase 2A jÃ¡ pode ser suficiente  
**MitigaÃ§Ã£o:**
- **AGUARDAR benchmark Fase 2A**
- Implementar APENAS se mÃ©tricas justificarem
- Validar ROI incremental

---

## ğŸ“– LIÃ‡Ã•ES DO PLANEJAMENTO

### 1. Pesquisa PrÃ©via Economiza Tempo

- Brightdata identificou implementaÃ§Ãµes modernas (2025)
- LangGraph tutorials oficiais disponÃ­veis
- Evita reinventar a roda

### 2. DecisÃ£o Baseada em Dados

- NÃ£o implementar cegamente
- Benchmark Fase 2A valida necessidade
- ROI incremental deve justificar esforÃ§o

### 3. Modularidade Ã© Chave

- Feature flags permitem A/B testing
- Self-RAG e CRAG independentes
- Podem ser usadas separadamente ou em conjunto

---

## ğŸš€ PRÃ“XIMOS PASSOS IMEDIATOS

### AGORA (Enquanto Benchmark Fase 2A Roda)

1. âœ… Plano Fase 2B completo
2. â³ Aguardar benchmark terminar
3. ğŸ“Š Analisar resultados Fase 2A
4. ğŸ¯ **DECIDIR**: Implementar Fase 2B SIM/NÃƒO?

### SE SIM - Iniciar Fase 2B.1 (Self-RAG)

1. Ler tutoriais LangGraph (1h)
2. Criar workflow design (1h)
3. Implementar SelfRAG class (6-8h)
4. Testar + validar (3h)
5. Documentar (2h)

---

## ğŸ“ CHANGELOG

### v1.0 - 2025-10-14 (Planejamento Inicial)

**Criado:**
- âœ… Plano completo Fase 2B (Self-RAG + CRAG)
- âœ… Pesquisa Brightdata (implementaÃ§Ãµes 2025)
- âœ… Micro-tarefas detalhadas (31-37h total)
- âœ… Cronograma (2-3 semanas)
- âœ… CritÃ©rios de decisÃ£o condicional
- âœ… MÃ©tricas de sucesso
- âœ… ReferÃªncias tÃ©cnicas

**PrÃ³ximo:**
- â³ Aguardar benchmark Fase 2A
- ğŸ¯ Decidir iniciar Fase 2B baseado em mÃ©tricas

---

**Ãšltima AtualizaÃ§Ã£o:** 2025-10-14  
**Status:** âœ… PLANEJAMENTO COMPLETO - Aguardando validaÃ§Ã£o Fase 2A

